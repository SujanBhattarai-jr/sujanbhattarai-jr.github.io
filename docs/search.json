[
  {
    "objectID": "featured_projects.html",
    "href": "featured_projects.html",
    "title": "Data science projects",
    "section": "",
    "text": "Using AI for Good: Helping non profits in predicting carbon\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nDid Smoking Mothers Have Lowered Weight Children?\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\nIdentifying the most common word in articles on climate science\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nSoccer players attributes comparison: Conclusion from Data visualization\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nSujan Bhattarai\n\n\n18 min\n\n\n\n\n\n\n  \n\n\n\n\nComparing competing Machine Learning models in classifying Spotify songs\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nSujan Bhattarai\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Data science for worldâ€™s most pressing issue",
    "section": "",
    "text": "Crop burning and Air quality in South Asia\n\n\n\n\n\n\n\nData analysis\n\n\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\n  \n\n\n\n\nExploring Optimal Locations for Marine Aquaculture: A Comprehensive Spatial Analysis\n\n\n\n\n\n\n\nPython\n\n\nGeospatial\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nSujan\n\n\n\n\n\n\n  \n\n\n\n\nPredicting CO2 Emission in Nepal: An In-Depth Analysis\n\n\n\n\n\n\n\nData analysis\n\n\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/southAsia_crop/index.html",
    "href": "posts/southAsia_crop/index.html",
    "title": "Crop burning and Air quality in South Asia",
    "section": "",
    "text": "In South Asian cities, extreme air pollution during winter is a recurring issue, attributed significantly to crop burning in agricultural lands. Another contributing factor is the reduced wind circulation caused by fog during the winter season, preventing the dispersion of city dust.As part of this project, the focus will be on assessing the strength of the relationships between crop burning and the air quality index in South Asian nations. The analysis will address three key questions:\n\nExamine the relationships between crop burning and the air quality index, generalized for all of South Asia.\nIdentify the South Asian country with the highest incidence of crop burning and assess its contribution to air quality.\nUtilize forecasting techniques to predict the trajectory of the identified country in the coming years.\n\n\n\nThe initial chunk of the project involves loading all the required libraries, with subsequent chunks introducing specific libraries as needed, providing clarity on the functions used from each package.\n\n##Load all required libraries\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(gt)\nlibrary(tufte)\nlibrary(feasts)\nlibrary(here)\nlibrary(janitor)\nlibrary(slider)"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#background",
    "href": "posts/southAsia_crop/index.html#background",
    "title": "Crop burning and Air quality in South Asia",
    "section": "",
    "text": "In South Asian cities, extreme air pollution during winter is a recurring issue, attributed significantly to crop burning in agricultural lands. Another contributing factor is the reduced wind circulation caused by fog during the winter season, preventing the dispersion of city dust.As part of this project, the focus will be on assessing the strength of the relationships between crop burning and the air quality index in South Asian nations. The analysis will address three key questions:\n\nExamine the relationships between crop burning and the air quality index, generalized for all of South Asia.\nIdentify the South Asian country with the highest incidence of crop burning and assess its contribution to air quality.\nUtilize forecasting techniques to predict the trajectory of the identified country in the coming years.\n\n\n\nThe initial chunk of the project involves loading all the required libraries, with subsequent chunks introducing specific libraries as needed, providing clarity on the functions used from each package.\n\n##Load all required libraries\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(gt)\nlibrary(tufte)\nlibrary(feasts)\nlibrary(here)\nlibrary(janitor)\nlibrary(slider)"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#data-collection-and-wrangling",
    "href": "posts/southAsia_crop/index.html#data-collection-and-wrangling",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Data collection and wrangling",
    "text": "Data collection and wrangling\nThe required data are downloaded from World bank data catalogue, Kaggle website, and ClimateWatch. The datasets are cleaned based on requirement using stringR package, dplyr, and tidyverse. In the first part, the emission data from the kaggle is cleaned and joined with air quality data from worldbank. The data are summarized in yearly basis, ranging from 1990 to 2020. The detailed data analysis is shown in the code chunks.\n\n##read the data and clean the name \ndata_world_bank &lt;- read_csv(here(\"posts\", \"southAsia_crop\",\"data\",\n                                 \"Agrofood_co2_emission.csv\"),\n                                 show_col_types = FALSE) %&gt;%\n                   janitor::clean_names()\n\n#filter data for south asia\nsouth_asian_countries &lt;- c(\"Nepal\", \"India\", \"Bangladesh\", \"Sri Lanka\", \"Pakistan\",\n                          \"Afghanistan\", \"Maldives\")\n                          \n#just filter for south asian region\nsouth_asian_data &lt;- data_world_bank %&gt;% \n                    filter(area %in% c(south_asian_countries)) %&gt;% \n                    mutate(year = as.numeric(year))\n\n\n##read the data and transpose it to form a database\naqi_world_bank1 &lt;- read_csv(here(\"posts\", \"southAsia_crop\",\"data\", \"aqi.csv\")) %&gt;% \n                   clean_names() %&gt;% \n                   t()\n\n##Perform data cleaning \naqi_world_bank &lt;- data.frame(cbind(year1 = rownames(aqi_world_bank1), \n                                   aqi_world_bank1)) %&gt;%  \n                  mutate(year = as.numeric(sub(\".*yr(\\\\d{4}).*\", \"\\\\1\", year1)))\n\n##filter only those rows where column is not equal to NA\n\naqi_world_bank &lt;- aqi_world_bank %&gt;% \n                  filter(!is.na(year))\n\n##select only required columns and rename bad-columns \naqi_world_bank &lt;- aqi_world_bank %&gt;% \n                  select(year = year, 'aqi_value' = 'V2') %&gt;% \n                  rownames_to_column(var = \"RowID\") %&gt;% #drop the index, not required\n                  select(-RowID)\n\n##arrange the data based on numeric columns\naqi_world_bank &lt;- aqi_world_bank %&gt;% \n                  arrange(year) %&gt;% \n                  mutate(year = as.numeric(year))\n\n\n##filter columns from the first dataset \nSA_reduced_columns &lt;- south_asian_data %&gt;% \n                      select(area, year, crop_residues, average_temperature_c)\n\n##group by year and summarize for all countries\nSA_summarized  &lt;- SA_reduced_columns %&gt;% \n                      group_by(year) %&gt;% \n                      summarize(crop_residues = sum(crop_residues),\n                                temp_average  = mean(average_temperature_c))\n\n##left join with aqi world bank \nSA_emission &lt;- left_join(SA_summarized, aqi_world_bank, by = \"year\") %&gt;% \n               mutate(aqi_value = as.numeric(aqi_value))"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#data-exploration",
    "href": "posts/southAsia_crop/index.html#data-exploration",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Data Exploration",
    "text": "Data Exploration\nVisualizations have been generated for the cleaned data as part of the exploratory phase, aimed at observing the general trends and characteristics within the dataframe. This preliminary exploration is valuable for subsequent analyses, helping to identify patterns, missing data, and errors. The graph presented below illustrates the long-term trend of crop emissions in South Asian countries. A notable and robust increasing trend in crop burning is evident from the graph, indicating a substantial rise in emissions over the depicted period.\n\n# perform exploratory data analysis \nggplot(SA_emission, aes(x = year, crop_residues)) +\n  geom_line (stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Emission from Crop Residues burning in South Asian Countries\",\n       x = \"Year\",\n       y = \"Emission (kilotonnes)\") +\n  theme_minimal()\n\n\n\n\nTotal emission from crop burning in South Asia since 1990\n\n\n\n\nLikewise, the presented graph depicts the trend in the Air Quality Index values across South Asian nations. At a 1-unit interval on the x-axis scale, the graph might initially resemble white noise, though underlying trends may exist. This is where the utility of moving averages becomes apparent. Employing a 10-year moving average has been instrumental in reconstructing this graph, revealing a discernible upward trend in air quality values over time.\n\n# include temperature with aqi index, adding interaction between crop residues\nx &lt;- ggplot(SA_emission, aes(year, aqi_value)) +\n     geom_line() +\n     labs(title = \"Before\",\n       x = \"Year\",\n       y = \"AQI value\") +\n    ylim(c(0, 10))+\n    theme_minimal()+\n  geom_smooth(method = \"lm\", se = F, col = 'red', size = 0.3)\n\ny &lt;- SA_emission %&gt;% \n     mutate(aqi_avearge = slide_dbl(aqi_value, mean, .before = 5, \n                                              .complete = TRUE)) %&gt;% \n     ggplot(aes(year, aqi_avearge)) +\n     geom_line() +\n     labs(title = \"After transformation\",\n       x = \"Year\",\n       y = \"AQI value\") +\n     theme_minimal()+\n     xlim(1990, 2019)+\n    geom_smooth(method = \"lm\", se = F, col = 'red', size = 0.3)\n\n# Displaying Plots Side by Side\ngridExtra::grid.arrange(x, y, nrow = 2)\n\n\n\n\nAir quality Index value before(a) and After(B) rolling mean transformation"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#data-transformation",
    "href": "posts/southAsia_crop/index.html#data-transformation",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Data Transformation",
    "text": "Data Transformation\nThe data exploration phase has provided valuable insights, prompting the next step in the analysisâ€”data transformation. In this phase, mathematical transformations will be applied to the data to ensure adherence to the assumptions of the intended model. Given the plan to utilize a linear regression model, establishing a robust association between the predictor variable (crop emission) and the response variable (Air Quality Index) is crucial. As part of this transformation, the predictor variable has been subjected to a rolling average, a method aimed at enhancing the correlation between the variables and facilitating the fitting of the linear regression model.\n\n#correlation between these two plots, when no transformation\ncor(SA_emission$aqi_value, SA_emission$crop_residues)\n## [1] -0.1064666\n\n\n#Try transforming data, or smoother the data to detect the pattern\nlibrary(slider)\nSA_emission &lt;- SA_emission %&gt;% \n               mutate(aqi_avearge = slide_dbl(aqi_value, mean, .before = 5, \n                                              .complete = TRUE))\ncor(SA_emission$aqi_avearge, SA_emission$crop_residues, use = \"na.or.complete\")\n## [1] 0.4571707"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#fitting-a-linear-model",
    "href": "posts/southAsia_crop/index.html#fitting-a-linear-model",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Fitting a Linear Model",
    "text": "Fitting a Linear Model\nThe data is ready, I propose a statistical hypothesis for my linear regression model: - null hypothesis: no change in aqi value with crop residues - alternate hypothesis: change in aqi value with crop residues\n\nMODEL = lm(data = SA_emission, aqi_avearge ~ crop_residues)\nsummary(MODEL)\n\n\nCall:\nlm(formula = aqi_avearge ~ crop_residues, data = SA_emission)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.95601 -0.28344  0.05615  0.32891  0.83340 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.523e+00  9.750e-01   3.613  0.00139 **\ncrop_residues 9.431e-05  3.745e-05   2.518  0.01887 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5856 on 24 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.209, Adjusted R-squared:  0.176 \nF-statistic: 6.342 on 1 and 24 DF,  p-value: 0.01887\n\n\nAccording to the modelâ€™s findings, there is a statistically significant correlation between the air quality index and crop residues. The output indicates that, For each one-unit increase in the variable (crop_residues), the dependent variable (aqi_avearge) is estimated to increase by 9.431e-05 units, assuming all other factors remain constant. This outcome aligns with expectations, as air quality is directly impacted by the smoke and dust emitted from burning crop residues. The subsequent analysis will focus on identifying the country with the highest emissions."
  },
  {
    "objectID": "posts/southAsia_crop/index.html#part-b",
    "href": "posts/southAsia_crop/index.html#part-b",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Part B",
    "text": "Part B\nIn this analysis, I replicated the methodology employed in the initial phase, with a slight alteration in the data transformation process. Instead of utilizing a rolling mean, I opted to normalize the data by the total cultivable area. To elaborate, the graph presented illustrates emissions categorized by country. However, this representation is biased due to the fact that India, with its larger area and population, naturally yields a higher total emission. To address this bias, the emission values were divided by the total cultivable area within each country, resulting in emissions per unit of cultivable area. This approach is more precise and provides a more accurate assessment.\n\n#further data exploration\nggplot(south_asian_data, aes(year, crop_residues, color = area)) +\n  geom_point() +\n  facet_grid(cols = vars(area)) +\n  labs(\n    x = \"Year 1990-2020\",\n    y = \"Emission from Crop Burning(kilotonnes)\") +\n  scale_color_discrete(name = \"Country\") +\n  theme_bw()+\n  theme(\n      # Remove x-axis title\n    axis.text.x = element_blank()    # Remove x-axis labels\n  )+\n  scale_x_continuous(breaks = seq(min(south_asian_data$year), max(south_asian_data$year), by = 30))  # Adjust the breaks as needed\n\n\n\n\nTotal crop burning emission in Each south Asian Nations since 1990\n\n\n\n# Save the plot\nggsave(\"explore.png\", width = 10, height = 5)\n\nUpon normalizing the emissions, the resulting graph reveals a notable shift in findings. Contrary to the initial assumption that India had the highest emissions, the new data indicates that Bangladesh emerges as the country with the highest emissions.\n\n#create a dataframe including values for agricultural land in these countries\ntotal_agri_area &lt;- read_csv(here(\"posts\", \"southAsia_crop\",\"data\", \"area_sa.csv\"))\n\ncolnames(total_agri_area)\n##  [1] \"Country Name\"   \"Country Code\"   \"Indicator Code\" \"1960\"          \n##  [5] \"1961\"           \"1962\"           \"1963\"           \"1964\"          \n##  [9] \"1965\"           \"1966\"           \"1967\"           \"1968\"          \n## [13] \"1969\"           \"1970\"           \"1971\"           \"1972\"          \n## [17] \"1973\"           \"1974\"           \"1975\"           \"1976\"          \n## [21] \"1977\"           \"1978\"           \"1979\"           \"1980\"          \n## [25] \"1981\"           \"1982\"           \"1983\"           \"1984\"          \n## [29] \"1985\"           \"1986\"           \"1987\"           \"1988\"          \n## [33] \"1989\"           \"1990\"           \"1991\"           \"1992\"          \n## [37] \"1993\"           \"1994\"           \"1995\"           \"1996\"          \n## [41] \"1997\"           \"1998\"           \"1999\"           \"2000\"          \n## [45] \"2001\"           \"2002\"           \"2003\"           \"2004\"          \n## [49] \"2005\"           \"2006\"           \"2007\"           \"2008\"          \n## [53] \"2009\"           \"2010\"           \"2011\"           \"2012\"          \n## [57] \"2013\"           \"2014\"           \"2015\"           \"2016\"          \n## [61] \"2017\"           \"2018\"           \"2019\"           \"2020\"          \n## [65] \"2021\"           \"2022\"\n\ntotal_agri_area &lt;- total_agri_area %&gt;%  select(-c(`Country Code`, `Indicator Code`)) %&gt;%                    filter(`Country Name` %in% south_asian_countries)\n\n##Pivot longer the data\ntransposed &lt;- total_agri_area %&gt;% pivot_longer(cols = \"1960\": \"2022\") %&gt;% \n              na.omit() %&gt;% \n              filter(name  &gt;= 1990) %&gt;% \n              rename(\"area_cultivable\"= \"value\", \"year\"= \"name\", 'area' = 'Country Name') %&gt;% \n  mutate(year = as.numeric(year))\ntransposed\n## # A tibble: 224 Ã— 3\n##    area         year area_cultivable\n##    &lt;chr&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n##  1 Afghanistan  1990          380400\n##  2 Afghanistan  1991          380300\n##  3 Afghanistan  1992          380300\n##  4 Afghanistan  1993          379340\n##  5 Afghanistan  1994          378130\n##  6 Afghanistan  1995          377630\n##  7 Afghanistan  1996          377570\n##  8 Afghanistan  1997          377950\n##  9 Afghanistan  1998          378680\n## 10 Afghanistan  1999          377640\n## # â„¹ 214 more rows\n\n\n# Divide crop residues by total agricultural land \n##SA_asian_data has the required ID for the division\nnormalized_area &lt;- left_join(south_asian_data, transposed, by = c(\"area\", \"year\")) \n\nnormalized_area &lt;- normalized_area %&gt;% \n                   mutate( norm_crop_residue = crop_residues/area_cultivable * 10^6)\n\n\n## now normalize the area\nggplot(normalized_area, aes(year, norm_crop_residue, color = area))+\n  geom_point()+\n  facet_grid(cols = vars(area), scales = \"free_x\")+\n  theme_bw()+\n  theme(axis.text.x = element_blank())+\n   scale_color_discrete(name = \"Country\") + \n labs(\n    x = \"Year 1990-2020\",\n    y = \"Emission from Crop Burning per cultivable area (kilotonnes)\"\n  ) +\n  theme(\n      # Remove x-axis title\n    axis.text.x = element_blank()    # Remove x-axis labels\n  )+\n  scale_x_continuous(breaks = seq(min(south_asian_data$year), max(south_asian_data$year), by = 30))  # Adjust the breaks as needed\n\n\n\n\nTotal Emission Per country per cultivable area. Bangladesh has the highest emission per cultivable area since 1990.\n\n\n\n# Save the plot\nggsave(\"explore2.png\", width = 10, height = 5)\n\nThe subsequent phase of my analysis is centered on Bangladesh, given its distinction as the country with the highest emissions per cultivable land. In this context, I applied a linear regression model to assess the strength of the relationship between crop residues and air quality. Bangladesh stands out as having the highest emissions from crop residues in the South Asian region. The objective is to quantify the robustness of the correlation between crop residues and air quality in this particular context.\n\n#  work for bangladesh\ndata &lt;- c(52.90993831, 51.64362616,\n53.74695817, 55.06194439, 56.41649854, 60.5358739, 68.94271136, 66.42738113, 73.27082296, 68.97033719, 67.7673598, 62.75318316, 63.27546257)\n\n# Duplicate the first two values four times each\ndata &lt;- c(rep(data[1:5], each = 4), data[3:length(data)])\n\n# Create a data frame with a single column\nBangladesh &lt;- data.frame(aqi_value = data)\n\n\n#combine this with SA Data for Bangladesh only\nbangladesh_filtered &lt;- normalized_area  %&gt;%\n                       filter(area ==  \"Bangladesh\") %&gt;% \n                       cbind(Bangladesh)\n\n\n##test the model here\n#| code-fold: false\nmodel_bangladesh &lt;- lm(data = bangladesh_filtered, aqi_value ~ crop_residues)\nsummary(model_bangladesh)\n\n\nCall:\nlm(formula = aqi_value ~ crop_residues, data = bangladesh_filtered)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.502 -1.442 -0.367  1.563 11.405 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   32.049523   3.708684   8.642 1.62e-09 ***\ncrop_residues  0.008853   0.001278   6.925 1.31e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.768 on 29 degrees of freedom\nMultiple R-squared:  0.6231,    Adjusted R-squared:  0.6101 \nF-statistic: 47.95 on 1 and 29 DF,  p-value: 1.307e-07\n\n\nAs per the model output, a 1-unit increase in the crop residues in Bangladesh Air Quality Index is associated with a 0.008-ton rise in AQI. The modelâ€™s Residual Square value stands at approximately 62%. The significance of the p-values, all below 5%, and the residual squared error value of 61% suggest that the model has performed well with the chosen variables."
  },
  {
    "objectID": "posts/southAsia_crop/index.html#further-exploration",
    "href": "posts/southAsia_crop/index.html#further-exploration",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Further Exploration",
    "text": "Further Exploration\nUp to this juncture, the analysis has been based on annual average data. The next step involves a shift to a more detailed exploration using monthly time series to uncover the nuanced trends in air quality in Bangladesh. This process includes decomposing the time series and employing various forecasting models to determine the most suitable model for the dataset.\n\n##read all year data and combine them using list methods\nfiles_list &lt;- list.files(here(\"posts\", \"southAsia_crop\", \"data\"), \n                         pattern = \"^Dhaka.*\\\\.csv$\")\ndaily_aqi &lt;- data.frame()\n\n# Loop through each file name and read the data\nfor (file in files_list) {\n  file_path &lt;- here::here(\"posts\", \"southAsia_crop\", \"data\", file)  # Adjust the path accordingly\n  x &lt;- readr::read_csv(file_path)\n  daily_aqi &lt;- dplyr::bind_rows(daily_aqi, x)\n}\n  \n##filter where all air quality index values are -999\ndaily_aqi_filtered &lt;- daily_aqi  %&gt;% \n                      filter(!AQI == -999)\n\nConducting a time-series analysis on a daily basis may not yield meaningful insights, given that crop burning is typically influenced by monthly patterns rather than daily occurrences. For instance, villagers might engage in burning crop residues on days with less wind, and with numerous villagers participating, these activities may take place on different days. Unfortunately, the unavailability of online sources providing monthly data on crop emissions has led us to focus on plotting a time series for monthly air quality. This approach aims to capture the broader trends associated with air quality fluctuations in the absence of specific monthly data on crop emissions.\n\n##load feasts package, these contains mostly time series functions \nlibrary(feasts)\nlibrary(tsibble)\nlibrary(forecast)\n\n##change date format\ndaily_aqi_filtered &lt;- daily_aqi_filtered %&gt;%\n                      mutate(date_column = as.Date(daily_aqi_filtered$`Date (LT)`, \n                            format = \"%m/%d/%Y %H:%M\")) %&gt;% \n                      na.omit()\n\n\nplot(daily_aqi_filtered$AQI,  xlab = \"Year 2016 - 2022\", ylab = \"AQI value\", main = NA)\n\n\n\n\nHourly time-series data for AQI value in Bangladesh between 2016(inclusive) and 2022(inclusive)\n\n\n\n\n\n# Aggregate by month\nmonthly_aggregated &lt;- daily_aqi_filtered %&gt;%\n                      mutate(month = month(date_column),\n                             year = year(date_column)) %&gt;% \n                      group_by(year, month) %&gt;% \n                      summarize(aqi_sum = mean(AQI)) %&gt;% \n                      mutate(date_column = make_date(year, month))\n\n##convert this to tsibble\ndf &lt;- monthly_aggregated %&gt;%\n      mutate(date = yearmonth(date_column))\ndf &lt;- as_tsibble(df, index = date)\ndf &lt;- df[c('date', 'aqi_sum')]\n\n\ndf %&gt;%\n  stl(t.window = 13, s.window = \"periodic\", robust = TRUE) %&gt;%\n  autoplot() +\n  ggtitle(\"\") +\n  labs(x = \"Time\", y = \"Value\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12),\n        legend.title = element_blank(),\n        legend.text = element_text(size = 12))\n\n\n\n\nSeasonal and Trend decomposition using Loess(STL)\n\n\n\n\nContrary to my initial expectations, the time series analysis revealed the presence of seasonality, a factor I had not anticipated based on my prior knowledge. Upon decomposing the data and examining the graph, it became apparent that seasonality does exist, albeit with a relatively modest impact. This insight is discerned from the small bar displayed to the right of each graph, indicating the extent of contribution from the seasonality component to the overall data."
  },
  {
    "objectID": "posts/southAsia_crop/index.html#forecasting",
    "href": "posts/southAsia_crop/index.html#forecasting",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Forecasting",
    "text": "Forecasting\nThe given R codes in chunks below are for predicting air quality index in Bangladesh over the next five years using four different models: Exponential Smoothing State Space Models (ETS), AutoRegressive Integrated Moving Average (ARIMA), Nonlinear AutoRegressive Neural Network (NNAR), and Trigonometric seasonality, Box-Cox transformation, ARMA errors, Trend and Seasonal components (TBATS). The data for training goes up to the year 2010, and forecasts are made for 20 time points (5 years). The individual predictions from each model are combined by averaging them, creating a combined forecast. This combined forecast, along with the original data and predictions from each individual model, is then presented graphically using the autoplot function. The aim is to evaluate how well these models perform and if combining them improves the accuracy of predictions.\n\n##preapre train model and fit the forecasting model\nBangladesh_crop &lt;- ts(bangladesh_filtered$crop_residues, start=1990, frequency=1)\n\ntrain &lt;- window(Bangladesh_crop, end=2010)\nh     &lt;- 20  # Forecasting for the next 5 years (12 months per year)\nETS   &lt;- forecast(ets(train), h=h)\nARIMA &lt;- forecast(auto.arima(train, lambda=0, biasadj=TRUE), h=h)\nNNAR  &lt;- forecast(nnetar(train), h=h)\nTBATS &lt;- forecast(tbats(train, biasadj=TRUE), h=h)\n\n# Combine forecasts for the next 5 years\nCombination &lt;- (ETS[[\"mean\"]] + ARIMA[[\"mean\"]] +\n                NNAR[[\"mean\"]] + TBATS[[\"mean\"]]) / 4 \n\n\nautoplot(Bangladesh_crop) +\n  autolayer(ETS, series=\"ETS\", PI=FALSE) +\n  autolayer(ARIMA, series=\"ARIMA\", PI=FALSE) +\n  autolayer(NNAR, series=\"NNAR\", PI=FALSE) +\n  autolayer(TBATS, series=\"TBATS\", PI=FALSE) +\n  autolayer(Combination, series=\"Combination\") +\n  xlab(\"Year\") + ylab(\"Emission from Crop Residues(Kilotonnes)\")+\n  theme_bw()\n\n\n\n\nForecast of Crop Emission in Bangladesh in 2030 using four different forecasting model\n\n\n\n\nAccording to the model, the yellow line demonstrates the closest fit to the actual trend. This yellow line represents a combination model, and its predictions can be extrapolated for the year 2030. As per this output, it is anticipated that Bangladeshâ€™s emissions will surpass 4000 tons by the year 2030.\nThe end to end code for this project can be found at :Github link"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#next-steps",
    "href": "posts/southAsia_crop/index.html#next-steps",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Next Steps",
    "text": "Next Steps\n\nValuate the accuracy of each individual model employed in the combination.\nGather monthly data specifically for crop emissions to investigate whether winter months contribute significantly to the Air Quality Index (AQI) in South Asia.\nLeverage World Bank data, which includes emissions for all countries worldwide, to implement clustering techniques for identifying groups of countries emitting similar amounts of CO2."
  },
  {
    "objectID": "featured_projects/calcofi/index.html",
    "href": "featured_projects/calcofi/index.html",
    "title": "Using AI for Good: Helping non profits in predicting carbon",
    "section": "",
    "text": "XYZ, name hidden for security purpose, was established in 1910 to investigate the ecological factors contributing to the collapse of the Pacific sardine population. Its extensive time series data provides invaluable insights into the long-term impacts of environmental changes on marine ecosystems and the communities reliant on them, not only within the California Current System but also extending to the North Pacific and beyond on an international scale.\nAs a part of this project, I am supporting the XYZ in predicting the ocean carbon values, which is a crucial part of the marine ecosystem. The dataset contains 12 columns, with 1 response variable and 11 predictors. The response variable is the dissolved inorganic carbon (DIC) concentration in the ocean, which is a key component of the marine carbon cycle. The predictors include various physical and chemical properties of the ocean, such as temperature, salinity, and oxygen concentration.\nThe goal of this project is to develop two machine learning models that can accurately predict the DIC concentration in the ocean based on the given predictors. This model will help better understand the factors influencing ocean carbon levels and make data-driven decisions to protect marine ecosystems.\n\nStepwise Flow of the Project\n\nLinear Regression Model\nFine Tuned XGBoost Model\n\n\n#hide all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#### Import all necessary libraries\n# import all required libraries\nimport numpy as np\nimport xgboost as xgb\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\n\nRead the data and store ID:required during submission for validity check\n\n#-----read train and test set\ntrain_calcofi = pd.read_csv(\"data/train.csv\")\ntest_calcofi  = pd.read_csv(\"data/test.csv\")  \n\n#collect the test ids for testcalcofi, required for submission dataset\ntest_ids = test_calcofi.id\ntest_calcofi = test_calcofi.drop(columns=['id'])\n\n\nData cleaning\nA model is only as good as the data.This step ensures that columns names are standarized, and the columns with inappropriate values are removed. The data is also checked for missing values. Incase of missing values, they are not imputed, but dropped. The CALCOFI did not provide claer guidance on how value should be imputted. So the best decision is to drop the rows with missing values.\n\n#----inspect the head and take the insights of data\ntrain_calcofi.head()\n\n   id    Lat_Dec     Lon_Dec  ...  Salinity1  Temperature_degC      DIC\n0   1  34.385030 -120.665530  ...     34.198              7.82  2270.17\n1   2  31.418333 -121.998333  ...     34.074              7.15  2254.10\n2   3  34.385030 -120.665530  ...     33.537             11.68  2111.04\n3   4  33.482580 -122.533070  ...     34.048              8.36  2223.41\n4   5  31.414320 -121.997670  ...     34.117              7.57  2252.62\n\n[5 rows x 19 columns]\n\n#### Data cleaning and preprocessing\n#the column names are in snake case, change all to lowercase\ntrain_calcofi.columns = map(str.lower, train_calcofi.columns)\ntest_calcofi.columns = map(str.lower, test_calcofi.columns)\n\n\n#remove the unnamed:12 column\ntrain_calcofi = train_calcofi.drop(columns=['unnamed: 12'])\ntrain_calcofi.rename(columns={'ta1.x': 'ta1'}, inplace=True)\n\nThe data looks clean. Now, a relationships between columns must be established This helps in understanding the data, and also helps in feature selection. The next step below plots a correlation matrix. This will show correlated variables in the dataset.\nThe reason that the correlation matrix is plotted to see if linear regression can be useful. If the correlation matrix shows strong relationship between the response and predictors, then linear regression is a great algorithm. If not, then other models must be tested.\n\n#plot correlation matrix\ncorr_matrix = train_calcofi.corr()\n\n# Plot the heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt=\".1f\")\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\n\nLinear regression model\n\n# Select only the predictors columns, and change them to array\nX = train_calcofi.drop(columns=['dic', 'id'], axis=1)\n\n# Select only the response column and change it to array\ny = train_calcofi['dic']\n\n# Split the data into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Define the range of polynomial degrees to fit\ndegrees = range(1, 5)  # From 1 to 5\n\n# Initialize lists to store R^2 scores\ntrain_r2_scores = []\nval_r2_scores = []\n\n# Loop through each polynomial degree\nfor degree in degrees:\n    # Create a pipeline with PolynomialFeatures, StandardScaler, and LinearRegression\n    model_pipeline = make_pipeline(PolynomialFeatures(degree=degree), StandardScaler(), LinearRegression())\n\n    # Fit the model pipeline to the training data\n    model_pipeline.fit(X_train, y_train)\n\n    # Calculate R^2 on the training set\n    train_r2 = model_pipeline.score(X_train, y_train)\n    train_r2_scores.append(train_r2)\n\n    # Calculate R^2 on the validation set\n    val_r2 = model_pipeline.score(X_val, y_val)\n    val_r2_scores.append(val_r2)\n\n    # Print the results for each degree\n    print(f\"Degree: {degree}\")\n    print(f\"  R^2 on training set: {train_r2}\")\n    print(f\"  R^2 on validation set: {val_r2}\")\n    print(\"-\" * 40)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=4)),\n                ('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=4)),\n                ('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])PolynomialFeaturesPolynomialFeatures(degree=4)StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n# Plotting the R^2 scores for training and validation sets\nplt.figure(figsize=(10, 6))\nplt.plot(degrees, train_r2_scores, label='Training R^2', marker='o')\nplt.plot(degrees, val_r2_scores, label='Validation R^2', marker='o')\nplt.xlabel('Polynomial Degree')\nplt.ylabel('R^2 Score')\nplt.title('Polynomial Degree vs. R^2 Score')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n#from the above sample, it is clear that either degree 1 or 2 is best\n# Define the polynomial degree\ndegree = 1\n\n# Define the range of regularization parameters (alpha values) to test\nalphas = np.logspace(-3, 3,  10)  # e.g., 10^-4 to 10^4\n\n# Initialize lists to store R^2 scores\ntrain_r2_scores = []\nval_r2_scores = []\n\n# Loop through each alpha value\nfor alpha in alphas:\n    # Create a pipeline with PolynomialFeatures, StandardScaler, and Ridge regression\n    model_pipeline = make_pipeline(PolynomialFeatures(degree=degree), StandardScaler(), Ridge(alpha=alpha))\n\n    # Fit the model pipeline to the training data\n    model_pipeline.fit(X_train, y_train)\n\n    # Calculate R^2 on the training set\n    train_r2 = model_pipeline.score(X_train, y_train)\n    train_r2_scores.append(train_r2)\n\n    # Calculate R^2 on the validation set\n    val_r2 = model_pipeline.score(X_val, y_val)\n    val_r2_scores.append(val_r2)\n\n    # Print the results for each alpha\n    print(f\"Alpha: {alpha}\")\n    print(f\"  R^2 on training set: {train_r2}\")\n    print(f\"  R^2 on validation set: {val_r2}\")\n    print(\"-\" * 40)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=1000.0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=1000.0))])PolynomialFeaturesPolynomialFeatures(degree=1)StandardScalerStandardScaler()RidgeRidge(alpha=1000.0)\n\n# Plotting the R^2 scores for training and validation sets\nplt.figure(figsize=(12, 6))\nplt.plot(alphas, train_r2_scores, label='Training R^2', marker='o', linestyle='-', color='b')\nplt.plot(alphas, val_r2_scores, label='Validation R^2', marker='o', linestyle='-', color='r')\nplt.xscale('log')  # Log scale for alpha values\nplt.xlabel('Regularization Parameter (Alpha)')\nplt.ylabel('R^2 Score')\nplt.title('Regularization Parameter (Alpha) vs. R^2 Score')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n#finalize the model\n# Split data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the polynomial degree and regularization parameter (lambda)\ndegree = 1\nalpha = 50  # Regularization parameter\n\n# Create and fit the model pipeline\nmodel_pipeline = make_pipeline(\n    PolynomialFeatures(degree=degree),\n    StandardScaler(),\n    Ridge(alpha=alpha)\n)\n\n# Fit the model to the training data\nmodel_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=50))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=50))])PolynomialFeaturesPolynomialFeatures(degree=1)StandardScalerStandardScaler()RidgeRidge(alpha=50)\n\n# Evaluate the model\ntrain_r2 = model_pipeline.score(X_train, y_train)\nval_r2 = model_pipeline.score(X_val, y_val)\n\nprint(f\"Training R^2 score for Linear regression: {train_r2}\")\n\nTraining R^2 score for Linear regression: 0.9964417610035372\n\nprint(f\"Validation R^2 score for linear regression: {val_r2}\")\n\nValidation R^2 score for linear regression: 0.9964454374974874\n\n\nThe base linear regression model has worked well with 1 degree polynomial and low regularization parameters, with mean squared error of 36 on testing set, meaning ocean carbon values(DIC) was off by 36 point on average for the prediction test.\n\n\n\nCan XGBoost perform better ?\nThe next step involves using XGboost for making the prediction, Extreme Gradient Boosting, also called the queen of the ML models is one of the most robust models. Base XGBOOST model (no tuning: Out of Box model) Note:XGBoost works on its own object type, which is Dmatrix. So, datatype conversion is required.\n\n# Create regression matrices, this is requirement for xgboost model\ndtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\ndtest_reg =  xgb.DMatrix(X_test, y_test, enable_categorical=True)\n\n\n# use cross validation approach to catch the best boosting round\nn = 1000\n\nmodel_xgb = xgb.cv(\n   dtrain=dtrain_reg,\n   params = {},\n   num_boost_round= n,\n   nfold = 20, #number of folds for cross validation\n   verbose_eval=10, #record rmse every 10 interval\n   early_stopping_rounds = 5,\n   as_pandas = True#stop if there is no improvement in 5 consecutive rounds\n)\n\n[0] train-rmse:79.31111+0.22480 test-rmse:79.31059+4.50293\n[10]    train-rmse:4.32181+0.05969  test-rmse:6.83483+2.17431\n[20]    train-rmse:2.11548+0.06541  test-rmse:6.12915+2.26123\n[30]    train-rmse:1.64633+0.06258  test-rmse:6.04879+2.23034\n[40]    train-rmse:1.29777+0.05879  test-rmse:6.02162+2.23979\n[50]    train-rmse:1.02154+0.05521  test-rmse:5.99733+2.23518\n[54]    train-rmse:0.93376+0.05252  test-rmse:6.00336+2.23321\n\n\n# Extract the optimal number of boosting rounds\noptimal_boosting_rounds = model_xgb['test-rmse-mean'].idxmin()\n\n\n# #using validation sets during training\nevals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]\n\nmodel_xgb = xgb.train(\n   params={},\n   dtrain=dtrain_reg,\n   num_boost_round= optimal_boosting_rounds,\n   evals=evals,#print rmse for every iterations\n   verbose_eval=10, #record rmse every 10 interval\n   early_stopping_rounds = 5 #stop if there is no improvement in 5 consecutive rounds\n)\n\n[0] train-rmse:79.30237 validation-rmse:79.79093\n[10]    train-rmse:4.39108  validation-rmse:4.91840\n[20]    train-rmse:2.09378  validation-rmse:3.83489\n[30]    train-rmse:1.63902  validation-rmse:3.79141\n[40]    train-rmse:1.30465  validation-rmse:3.68415\n[48]    train-rmse:1.08249  validation-rmse:3.61313\n\n# #predict on the the test matrix\npreds = model_xgb.predict(dtest_reg)\n\n#check for rmse\nmse = mean_squared_error(y_test, preds, squared=False)\n\nprint(f\"MSE of the test model: {mse:.3f}\")\n\nMSE of the test model: 3.613\n\n\n**GRID TUNED XGBOOST MODEL\n\n# Define the parameter grid\ngbm_param_grid = {\n    'colsample_bytree': [0.5, 0.7, 0.9],\n    'n_estimators': [100, 200, 300, 1450],\n    'max_depth': [5, 7, 9],\n    'learning_rate': [0.001, 0.01]\n}\n\n#best hyperparameters based on running\ngbm_param_grid_set = {\n    'colsample_bytree': [0.5],\n    'n_estimators': [1450],\n    'max_depth': [5],\n    'learning_rate': [0.01]\n}\n\n# Instantiate the regressor\ngbm = xgb.XGBRegressor()\n\n# Instantiate GridSearchCV with seed\ngridsearch_mse = GridSearchCV(\n    param_grid=gbm_param_grid_set,\n    estimator=gbm,\n    scoring='neg_mean_squared_error',\n    cv=10,\n    verbose=1,\n)\n\n# Fit the gridmse\ngridsearch_mse.fit(X_train, y_train)\n\nGridSearchCV(cv=10,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None,...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'colsample_bytree': [0.5], 'learning_rate': [0.01],\n                         'max_depth': [5], 'n_estimators': [1450]},\n             scoring='neg_mean_squared_error', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=10,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None,...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'colsample_bytree': [0.5], 'learning_rate': [0.01],\n                         'max_depth': [5], 'n_estimators': [1450]},\n             scoring='neg_mean_squared_error', verbose=1)estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)\n\n# Best estimator\nbest_estimator = gridsearch_mse.best_estimator_\n\n# Use the best estimator to make predictions on the test data\ny_pred = best_estimator.predict(X_test)\n\n# Calculate mean squared error\nmse_xgboost = mean_squared_error(y_test, y_pred)\nprint(\"Training Mean Squared Error:\", mse_xgboost)\n\nTraining Mean Squared Error: 8.783662851134999\n\n# Now, use the best estimator to make predictions on the validation data\ny_val_pred = best_estimator.predict(X_val)\n\n# Calculate mean squared error on the validation set\nmse_xgboost_val = mean_squared_error(y_val, y_val_pred)\nprint(\"Validation Mean Squared Error:\", mse_xgboost_val)\n\nValidation Mean Squared Error: 28.41969736921685\n\n# Get the model score on the validation set\nprint(f\"Model score on validation set: {best_estimator.score(X_val, y_val)}\")\n\nModel score on validation set: 0.9978739833258761\n\n\n\nprint(f\"Model score on validation setfor linear regression: {val_r2}\")\n\nModel score on validation setfor linear regression: 0.9964454374974874\n\nprint(f\"Model score on validation set for XGBoost: {best_estimator.score(X_val, y_val)}\")\n\nModel score on validation set for XGBoost: 0.9978739833258761\n\n\nThe XGBoost model has lower Bias, and high accuracy compared to the linear regression model. Thus, I suggest using the XGBOOST model for any new incoming data on ocean values."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Areas of Proficiency\n  \n\n\n\n  \n     SUJAN BHATTARAI\n     Welcome to The Dataverse\n  \n  \n    \n      Areas of Proficiency\n    \n    \n      \n        Data Science\n        Enjoys analytical thinking, data-driven decisions, and content creation about Data Science\n      \n      \n        Machine Learning\n        Strong understanding of maths in ML algorithms: neural networks, tree based models, regressions\n      \n      \n        Causal Inference\n        Experience with First order effects, DID estimate, panel data, inverse weighted regression\n      \n      \n        Databases\n        Proficient at RDBMS, experience with snowflake and big data, concepts on HDFS and Hadoop \n      \n      \n        Modelling\n         Ran modelling simulations, randomize experimental designs, measure parameters uncertainties\n      \n      \n        Data Visualization\n         Knowledge of data visualization concepts, expert at ggplot and ggplot-children packages"
  }
]