[
  {
    "objectID": "featured_projects/calcofi/index.html",
    "href": "featured_projects/calcofi/index.html",
    "title": "which ML algorithm should the organization use for higher accuracy?",
    "section": "",
    "text": "XYZ, name hidden for security purpose, was established in 1910 to investigate the ecological factors contributing to the collapse of the Pacific sardine population. Its extensive time series data provides invaluable insights into the long-term impacts of environmental changes on marine ecosystems and the communities reliant on them, not only within the California Current System but also extending to the North Pacific and beyond on an international scale.\nAs a part of this project, I am supporting the XYZ in predicting the ocean carbon values, which is a crucial part of the marine ecosystem. The dataset contains 12 columns, with 1 response variable and 11 predictors. The response variable is the dissolved inorganic carbon (DIC) concentration in the ocean, which is a key component of the marine carbon cycle. The predictors include various physical and chemical properties of the ocean, such as temperature, salinity, and oxygen concentration.\nThe goal of this project is to develop two machine learning models that can accurately predict the DIC concentration in the ocean based on the given predictors. This model will help better understand the factors influencing ocean carbon levels and make data-driven decisions to protect marine ecosystems.\n\nStepwise Flow of the Project\n\nLinear Regression Model\nFine Tuned XGBoost Model\n\n\n#hide all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#### Import all necessary libraries\n# import all required libraries\nimport numpy as np\nimport xgboost as xgb\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\n\nRead the data and store ID:required during submission for validity check\n\n#-----read train and test set\ntrain_calcofi = pd.read_csv(\"data/train.csv\")\ntest_calcofi  = pd.read_csv(\"data/test.csv\")  \n\n#collect the test ids for testcalcofi, required for submission dataset\ntest_ids = test_calcofi.id\ntest_calcofi = test_calcofi.drop(columns=['id'])\n\n\nData cleaning\nA model is only as good as the data.This step ensures that columns names are standarized, and the columns with inappropriate values are removed. The data is also checked for missing values. Incase of missing values, they are not imputed, but dropped. The CALCOFI did not provide claer guidance on how value should be imputted. So the best decision is to drop the rows with missing values.\n\n#----inspect the head and take the insights of data\ntrain_calcofi.head()\n\n   id    Lat_Dec     Lon_Dec  ...  Salinity1  Temperature_degC      DIC\n0   1  34.385030 -120.665530  ...     34.198              7.82  2270.17\n1   2  31.418333 -121.998333  ...     34.074              7.15  2254.10\n2   3  34.385030 -120.665530  ...     33.537             11.68  2111.04\n3   4  33.482580 -122.533070  ...     34.048              8.36  2223.41\n4   5  31.414320 -121.997670  ...     34.117              7.57  2252.62\n\n[5 rows x 19 columns]\n\n#### Data cleaning and preprocessing\n#the column names are in snake case, change all to lowercase\ntrain_calcofi.columns = map(str.lower, train_calcofi.columns)\ntest_calcofi.columns = map(str.lower, test_calcofi.columns)\n\n\n#remove the unnamed:12 column\ntrain_calcofi = train_calcofi.drop(columns=['unnamed: 12'])\ntrain_calcofi.rename(columns={'ta1.x': 'ta1'}, inplace=True)\n\nThe data looks clean. Now, a relationships between columns must be established This helps in understanding the data, and also helps in feature selection. The next step below plots a correlation matrix. This will show correlated variables in the dataset.\nThe reason that the correlation matrix is plotted to see if linear regression can be useful. If the correlation matrix shows strong relationship between the response and predictors, then linear regression is a great algorithm. If not, then other models must be tested.\n\n#plot correlation matrix\ncorr_matrix = train_calcofi.corr()\n\n# Plot the heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt=\".1f\")\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\n\nLinear regression model\n\n# Select only the predictors columns, and change them to array\nX = train_calcofi.drop(columns=['dic', 'id'], axis=1)\n\n# Select only the response column and change it to array\ny = train_calcofi['dic']\n\n# Split the data into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Define the range of polynomial degrees to fit\ndegrees = range(1, 5)  # From 1 to 5\n\n# Initialize lists to store R^2 scores\ntrain_r2_scores = []\nval_r2_scores = []\n\n# Loop through each polynomial degree\nfor degree in degrees:\n    # Create a pipeline with PolynomialFeatures, StandardScaler, and LinearRegression\n    model_pipeline = make_pipeline(PolynomialFeatures(degree=degree), StandardScaler(), LinearRegression())\n\n    # Fit the model pipeline to the training data\n    model_pipeline.fit(X_train, y_train)\n\n    # Calculate R^2 on the training set\n    train_r2 = model_pipeline.score(X_train, y_train)\n    train_r2_scores.append(train_r2)\n\n    # Calculate R^2 on the validation set\n    val_r2 = model_pipeline.score(X_val, y_val)\n    val_r2_scores.append(val_r2)\n\n    # Print the results for each degree\n    print(f\"Degree: {degree}\")\n    print(f\"  R^2 on training set: {train_r2}\")\n    print(f\"  R^2 on validation set: {val_r2}\")\n    print(\"-\" * 40)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=4)),\n                ('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=4)),\n                ('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])PolynomialFeaturesPolynomialFeatures(degree=4)StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n# Plotting the R^2 scores for training and validation sets\nplt.figure(figsize=(10, 6))\nplt.plot(degrees, train_r2_scores, label='Training R^2', marker='o')\nplt.plot(degrees, val_r2_scores, label='Validation R^2', marker='o')\nplt.xlabel('Polynomial Degree')\nplt.ylabel('R^2 Score')\nplt.title('Polynomial Degree vs. R^2 Score')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n#from the above sample, it is clear that either degree 1 or 2 is best\n# Define the polynomial degree\ndegree = 1\n\n# Define the range of regularization parameters (alpha values) to test\nalphas = np.logspace(-3, 3,  10)  # e.g., 10^-4 to 10^4\n\n# Initialize lists to store R^2 scores\ntrain_r2_scores = []\nval_r2_scores = []\n\n# Loop through each alpha value\nfor alpha in alphas:\n    # Create a pipeline with PolynomialFeatures, StandardScaler, and Ridge regression\n    model_pipeline = make_pipeline(PolynomialFeatures(degree=degree), StandardScaler(), Ridge(alpha=alpha))\n\n    # Fit the model pipeline to the training data\n    model_pipeline.fit(X_train, y_train)\n\n    # Calculate R^2 on the training set\n    train_r2 = model_pipeline.score(X_train, y_train)\n    train_r2_scores.append(train_r2)\n\n    # Calculate R^2 on the validation set\n    val_r2 = model_pipeline.score(X_val, y_val)\n    val_r2_scores.append(val_r2)\n\n    # Print the results for each alpha\n    print(f\"Alpha: {alpha}\")\n    print(f\"  R^2 on training set: {train_r2}\")\n    print(f\"  R^2 on validation set: {val_r2}\")\n    print(\"-\" * 40)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=1000.0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=1000.0))])PolynomialFeaturesPolynomialFeatures(degree=1)StandardScalerStandardScaler()RidgeRidge(alpha=1000.0)\n\n# Plotting the R^2 scores for training and validation sets\nplt.figure(figsize=(12, 6))\nplt.plot(alphas, train_r2_scores, label='Training R^2', marker='o', linestyle='-', color='b')\nplt.plot(alphas, val_r2_scores, label='Validation R^2', marker='o', linestyle='-', color='r')\nplt.xscale('log')  # Log scale for alpha values\nplt.xlabel('Regularization Parameter (Alpha)')\nplt.ylabel('R^2 Score')\nplt.title('Regularization Parameter (Alpha) vs. R^2 Score')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n#finalize the model\n# Split data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the polynomial degree and regularization parameter (lambda)\ndegree = 1\nalpha = 50  # Regularization parameter\n\n# Create and fit the model pipeline\nmodel_pipeline = make_pipeline(\n    PolynomialFeatures(degree=degree),\n    StandardScaler(),\n    Ridge(alpha=alpha)\n)\n\n# Fit the model to the training data\nmodel_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=50))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=50))])PolynomialFeaturesPolynomialFeatures(degree=1)StandardScalerStandardScaler()RidgeRidge(alpha=50)\n\n# Evaluate the model\ntrain_r2 = model_pipeline.score(X_train, y_train)\nval_r2 = model_pipeline.score(X_val, y_val)\n\nprint(f\"Training R^2 score for Linear regression: {train_r2}\")\n\nTraining R^2 score for Linear regression: 0.9964417610035372\n\nprint(f\"Validation R^2 score for linear regression: {val_r2}\")\n\nValidation R^2 score for linear regression: 0.9964454374974874\n\n\nThe base linear regression model has worked well with 1 degree polynomial and low regularization parameters, with mean squared error of 36 on testing set, meaning ocean carbon values(DIC) was off by 36 point on average for the prediction test.\n\n\n\nCan XGBoost perform better ?\nThe next step involves using XGboost for making the prediction, Extreme Gradient Boosting, also called the queen of the ML models is one of the most robust models. Base XGBOOST model (no tuning: Out of Box model) Note:XGBoost works on its own object type, which is Dmatrix. So, datatype conversion is required.\n\n# Create regression matrices, this is requirement for xgboost model\ndtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\ndtest_reg =  xgb.DMatrix(X_test, y_test, enable_categorical=True)\n\n\n# use cross validation approach to catch the best boosting round\nn = 1000\n\nmodel_xgb = xgb.cv(\n   dtrain=dtrain_reg,\n   params = {},\n   num_boost_round= n,\n   nfold = 20, #number of folds for cross validation\n   verbose_eval=10, #record rmse every 10 interval\n   early_stopping_rounds = 5,\n   as_pandas = True#stop if there is no improvement in 5 consecutive rounds\n)\n\n[0] train-rmse:79.31111+0.22480 test-rmse:79.31059+4.50293\n[10]    train-rmse:4.32181+0.05969  test-rmse:6.83483+2.17431\n[20]    train-rmse:2.11548+0.06541  test-rmse:6.12915+2.26123\n[30]    train-rmse:1.64633+0.06258  test-rmse:6.04879+2.23034\n[40]    train-rmse:1.29777+0.05879  test-rmse:6.02162+2.23979\n[50]    train-rmse:1.02154+0.05521  test-rmse:5.99733+2.23518\n[53]    train-rmse:0.95655+0.05754  test-rmse:6.00205+2.23493\n\n\n# Extract the optimal number of boosting rounds\noptimal_boosting_rounds = model_xgb['test-rmse-mean'].idxmin()\n\n\n# #using validation sets during training\nevals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]\n\nmodel_xgb = xgb.train(\n   params={},\n   dtrain=dtrain_reg,\n   num_boost_round= optimal_boosting_rounds,\n   evals=evals,#print rmse for every iterations\n   verbose_eval=10, #record rmse every 10 interval\n   early_stopping_rounds = 5 #stop if there is no improvement in 5 consecutive rounds\n)\n\n[0] train-rmse:79.30237 validation-rmse:79.79093\n[10]    train-rmse:4.39108  validation-rmse:4.91840\n[20]    train-rmse:2.09378  validation-rmse:3.83489\n[30]    train-rmse:1.63902  validation-rmse:3.79141\n[40]    train-rmse:1.30465  validation-rmse:3.68415\n[48]    train-rmse:1.08249  validation-rmse:3.61313\n\n# #predict on the the test matrix\npreds = model_xgb.predict(dtest_reg)\n\n#check for rmse\nmse = mean_squared_error(y_test, preds, squared=False)\n\nprint(f\"MSE of the test model: {mse:.3f}\")\n\nMSE of the test model: 3.613\n\n\n**GRID TUNED XGBOOST MODEL\n\n# Define the parameter grid\ngbm_param_grid = {\n    'colsample_bytree': [0.5, 0.7, 0.9],\n    'n_estimators': [100, 200, 300, 1450],\n    'max_depth': [5, 7, 9],\n    'learning_rate': [0.001, 0.01]\n}\n\n#best hyperparameters based on running\ngbm_param_grid_set = {\n    'colsample_bytree': [0.5],\n    'n_estimators': [1450],\n    'max_depth': [5],\n    'learning_rate': [0.01]\n}\n\n# Instantiate the regressor\ngbm = xgb.XGBRegressor()\n\n# Instantiate GridSearchCV with seed\ngridsearch_mse = GridSearchCV(\n    param_grid=gbm_param_grid_set,\n    estimator=gbm,\n    scoring='neg_mean_squared_error',\n    cv=10,\n    verbose=1,\n)\n\n# Fit the gridmse\ngridsearch_mse.fit(X_train, y_train)\n\nGridSearchCV(cv=10,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None,...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'colsample_bytree': [0.5], 'learning_rate': [0.01],\n                         'max_depth': [5], 'n_estimators': [1450]},\n             scoring='neg_mean_squared_error', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=10,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None,...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'colsample_bytree': [0.5], 'learning_rate': [0.01],\n                         'max_depth': [5], 'n_estimators': [1450]},\n             scoring='neg_mean_squared_error', verbose=1)estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)\n\n# Best estimator\nbest_estimator = gridsearch_mse.best_estimator_\n\n# Use the best estimator to make predictions on the test data\ny_pred = best_estimator.predict(X_test)\n\n# Calculate mean squared error\nmse_xgboost = mean_squared_error(y_test, y_pred)\nprint(\"Training Mean Squared Error:\", mse_xgboost)\n\nTraining Mean Squared Error: 8.783662851134999\n\n# Now, use the best estimator to make predictions on the validation data\ny_val_pred = best_estimator.predict(X_val)\n\n# Calculate mean squared error on the validation set\nmse_xgboost_val = mean_squared_error(y_val, y_val_pred)\nprint(\"Validation Mean Squared Error:\", mse_xgboost_val)\n\nValidation Mean Squared Error: 28.41969736921685\n\n# Get the model score on the validation set\nprint(f\"Model score on validation set: {best_estimator.score(X_val, y_val)}\")\n\nModel score on validation set: 0.9978739833258761\n\n\n\nprint(f\"Model score on validation setfor linear regression: {val_r2}\")\n\nModel score on validation setfor linear regression: 0.9964454374974874\n\nprint(f\"Model score on validation set for XGBoost: {best_estimator.score(X_val, y_val)}\")\n\nModel score on validation set for XGBoost: 0.9978739833258761\n\n\nThe XGBoost model has lower Bias, and high accuracy compared to the linear regression model. Thus, I suggest using the XGBOOST model for any new incoming data on ocean values."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Data science for world’s most pressing issue",
    "section": "",
    "text": "Crop burning and Air quality in South Asia\n\n\n\n\n\n\n\nData analysis\n\n\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\n  \n\n\n\n\nDetection of Forest fire using false color Image analysis\n\n\n\n\n\n\n\nPython\n\n\nGeospatial\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2023\n\n\nSujan Bhattarai\n\n\n\n\n\n\n  \n\n\n\n\nExploring Optimal Locations for Marine Aquaculture: A Comprehensive Spatial Analysis\n\n\n\n\n\n\n\nPython\n\n\nGeospatial\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nSujan\n\n\n\n\n\n\n  \n\n\n\n\nPredicting CO2 Emission in Nepal: An In-Depth Analysis\n\n\n\n\n\n\n\nData analysis\n\n\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/False_color_analysis/index.html",
    "href": "posts/False_color_analysis/index.html",
    "title": "Detection of Forest fire using false color Image analysis",
    "section": "",
    "text": "A false color image involves assigning colors to represent features not easily visible in the natural spectrum. In environmental studies, false color imagery plays a crucial role in highlighting and distinguishing various environmental elements, such as the health of vegetation, changes in land cover, and levels of pollution. By associating different attributes with distinct colors, false color images offer valuable information for assessing and monitoring environmental conditions. They aid in the analysis of ecosystems, climate patterns, and overall environmental health. The provided image belwo illustrates how false color can be applied to enhance the detection of changes. The image in the right is false color that highlights spherules in rocks in Mars. further information about the image\n\n\n\nFigure 1: Stone mountain rock outcrop in true and false colour. Image credit: NASA/JPL\n\n\nIn this brief analysis, false color imagery is employed to identify the Thomas fire event in Santa Barbara County. The process involves retrieving raster data from open data sources, specifically from Microsoft Planetary Computer and California government websites. The data sources are their citations are listed in Data Citation heading.\n\n\nFor this blog, I opted to utilize the base environment of Python3 to avoid the need for downloading resource-intensive packages for the analysis, which could potentially consume significant memory resources. Additionally, I streamlined the installation process by employing the pip installer for all the required packages, including geopandas and rioxarray.\n\n\n\n\nCode\n#load all required libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport xarray as xr\nimport rioxarray as rioxr\nimport geopandas as gpd\nfrom rasterio.features import rasterize\nimport matplotlib.dates as mdates\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.widgets import Button\nfrom tabulate import tabulate"
  },
  {
    "objectID": "posts/False_color_analysis/index.html#introduction-false-color-image",
    "href": "posts/False_color_analysis/index.html#introduction-false-color-image",
    "title": "Detection of Forest fire using false color Image analysis",
    "section": "",
    "text": "A false color image involves assigning colors to represent features not easily visible in the natural spectrum. In environmental studies, false color imagery plays a crucial role in highlighting and distinguishing various environmental elements, such as the health of vegetation, changes in land cover, and levels of pollution. By associating different attributes with distinct colors, false color images offer valuable information for assessing and monitoring environmental conditions. They aid in the analysis of ecosystems, climate patterns, and overall environmental health. The provided image belwo illustrates how false color can be applied to enhance the detection of changes. The image in the right is false color that highlights spherules in rocks in Mars. further information about the image\n\n\n\nFigure 1: Stone mountain rock outcrop in true and false colour. Image credit: NASA/JPL\n\n\nIn this brief analysis, false color imagery is employed to identify the Thomas fire event in Santa Barbara County. The process involves retrieving raster data from open data sources, specifically from Microsoft Planetary Computer and California government websites. The data sources are their citations are listed in Data Citation heading.\n\n\nFor this blog, I opted to utilize the base environment of Python3 to avoid the need for downloading resource-intensive packages for the analysis, which could potentially consume significant memory resources. Additionally, I streamlined the installation process by employing the pip installer for all the required packages, including geopandas and rioxarray.\n\n\n\n\nCode\n#load all required libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport xarray as xr\nimport rioxarray as rioxr\nimport geopandas as gpd\nfrom rasterio.features import rasterize\nimport matplotlib.dates as mdates\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.widgets import Button\nfrom tabulate import tabulate"
  },
  {
    "objectID": "posts/False_color_analysis/index.html#data-citation",
    "href": "posts/False_color_analysis/index.html#data-citation",
    "title": "Detection of Forest fire using false color Image analysis",
    "section": "Data Citation",
    "text": "Data Citation\nThe downloaded raster image(in the chunk below) stores five bands of wavelengths : red, green, blue, near infrared, and shortwave. The near infrared, shortwave and red bands will be used in detection of fire plumes. Likewise, the California fire information is downloaded from California state goverment website, which stores information about fire event around california, Santa Barbara. The links to data sources are:\n\nFalse Color Raster Image:\n\nRetrieved from the Microsoft Planetary Computer catalog.\n\nCalifornia Shapefile:\n\nDownloaded from California State GIS Data.\n\nAir Quality Data:\n\n2017 Daily AQI Data\n2018 Daily AQI Data\n\n\n\n\nCode\n##set the directory for the data\ndata_path = os.path.join(os.getcwd(), \"data/landsat8-2018-01-26-sb-simplified.nc\")\nlandsat = rioxr.open_rasterio(data_path)\n# squeeze the band dimension, as it creates problem in plotting,\n# also create dublicate, original unchanged might be required\nlandsat1 = landsat.squeeze(['band'])\n\nfire = gpd.read_file(\"data/California_Fire_Perimeters_2017/California_Fire_Perimeters_2017.shp\")\nfire.describe()\n\n\n              index      OBJECTID  ...     SHAPE_Leng    SHAPE_Area\ncount    608.000000    608.000000  ...     608.000000  6.080000e+02\nmean   20160.358553  41761.753289  ...   13447.746625  1.537259e+07\nstd      261.936829    295.329361  ...   44116.066523  8.983227e+07\nmin    19836.000000  41429.000000  ...      13.386707 -2.204240e+05\n25%    19989.750000  41582.750000  ...    1193.667186  5.695604e+04\n50%    20141.500000  41738.500000  ...    2425.248003  1.836226e+05\n75%    20293.250000  41893.250000  ...    6012.417252  9.107243e+05\nmax    21906.000000  43892.000000  ...  540531.887458  1.681106e+09\n\n[8 rows x 8 columns]\n\n\nThe three bands(NIR, SWIR, RED) are filtered for areas around Thomas fire area. The raster is adjusted with shapes and extent with another shape file, since shape file is used to extract area of interest(Thomas fire area) from whole california map. The false color bands are then plotted for visualization purpose."
  },
  {
    "objectID": "posts/False_color_analysis/index.html#data-wrangling",
    "href": "posts/False_color_analysis/index.html#data-wrangling",
    "title": "Detection of Forest fire using false color Image analysis",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nThe first step in Data wrangling involves subsetting the data only for the area of the interest. Then, the CRS of both datasets is matched, or converted as per CRS of other dataset. Converting CRS is important for accurate merging of the two raste files. Next, the grid cells of netcdf data is converted to array representing list of lists(). The format of the NetCDF data-array looks as the ouput below.\n\n\nCode\n#select only thomas fire  \nthomas_fire = fire[fire['FIRE_NAME'] == 'THOMAS'] \n\n#check if value is only for Thomas fire\nfor index, name in thomas_fire['FIRE_NAME'].items():\n    if name != 'THOMAS':\n        raise('DOUBLE CHECK')\n\n\n\n\nCode\n#change the crs to match with landsat                              \nthomas_fire = thomas_fire.to_crs(landsat.rio.crs)\n\n#validate if they are equal\nthomas_fire.crs == landsat.rio.crs\n\n\nTrue\n\n\nCode\n#also save the false color image of the landsat, it will be required in plotting\nfalse_color_data = landsat1[['swir22', 'nir08', 'red']].to_array()\n\nprint(false_color_data[1])\n\n\n&lt;xarray.DataArray (y: 731, x: 870)&gt;\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\nCoordinates:\n    band         int32 1\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int32 0\n    variable     &lt;U5 'nir08'"
  },
  {
    "objectID": "posts/False_color_analysis/index.html#data-visualization",
    "href": "posts/False_color_analysis/index.html#data-visualization",
    "title": "Detection of Forest fire using false color Image analysis",
    "section": "Data Visualization",
    "text": "Data Visualization\nThe dataset is ready for visualization. Since, it’s an array object, imshow function is used. The output image is the raster image and shows different colors for areas where fire occured.\n\n\nCode\n# Data is all set to plot\nfig, ax = plt.subplots(figsize=(5, 5))\n\nfalse_color_data.plot.imshow(ax=ax, cmap='inferno', robust = True)\nak_patch = mpatches.Patch(color='lightgreen', \n                          alpha = 0.8,\n                          label='False color image for fire predictions')\n\nthomas_fire.plot(ax=ax, edgecolor = 'red', color= 'yellow', alpha = 0.3)\nkodiak_patch = mpatches.Patch(color='yellow', \n                              alpha = 0.5,\n                              label='Fire occured')\n\nax.legend(handles = [ak_patch, kodiak_patch], \n          frameon=False, \n          loc='lower left',\n          labelcolor = 'white' ) #create legend\n\nax.get_xaxis().set_visible(False) #remove xaxis label\nax.get_yaxis().set_visible(False) #remove yaxis label\n\n\n\n\n\n\nThe graph above displays the false color image of the Thomas fire. Now, examine the air quality during, before, and after the Thomas fire to spot any notable changes. You can retrieve the tabular daily air quality data for the region from the Data Citation section. To carry out this assessment, download the data for both 2017 and 2018.\n\n\nPrior to moving forward, it’s crucial to acknowledge that the new air quality data needs some cleaning. Issues like spaces in names, inconsistent column names, and the inclusion of data for all counties in California need attention. Therefore, execute the following cleaning procedures:\n\n\n\nRemove spaces in names.\n\n\nStandardize column names to lowercase.\n\n\nCombine data for 2017 and 2018.\n\n\nReplace spaces in column names with underscores.\n\n\nConvert the date column to a datetime format recognized by Python.\n\n\n\nOnce these cleaning steps are completed, you’ll have a well-prepared dataset for further analysis of air quality changes in relation to the Thomas fire event."
  },
  {
    "objectID": "posts/False_color_analysis/index.html#data-wrangling-part-b",
    "href": "posts/False_color_analysis/index.html#data-wrangling-part-b",
    "title": "Detection of Forest fire using false color Image analysis",
    "section": "Data Wrangling (Part B)",
    "text": "Data Wrangling (Part B)\n\n\nCode\n##air quality index\n##webscrape the two dataset based on the guidelines\naqi_17 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip\")\naqi_18 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip\")\n##concatnate the two datasets, this will be useful in plotting\naqi = pd.concat([aqi_17, aqi_18])\n\n#check if both 2017 and 2018 data is present\nprint(tabulate(aqi[['Date']].head(2), headers='keys', tablefmt='pretty'))\n\n\n+---+------------+\n|   |    Date    |\n+---+------------+\n| 0 | 2017-01-01 |\n| 1 | 2017-01-04 |\n+---+------------+\n\n\nCode\nprint(tabulate(aqi[['Date']].tail(2), headers='keys', tablefmt='pretty'))\n\n\n+--------+------------+\n|        |    Date    |\n+--------+------------+\n| 327535 | 2018-12-30 |\n| 327536 | 2018-12-31 |\n+--------+------------+\n\n\n\n\nCode\n# re-assign the column names - .str.lower() makes them lower case\naqi.columns = aqi.columns.str.lower()\n\n#  re-assign the column names again - .str.replace(' ','_') replaces the space for _\naqi.columns = aqi.columns.str.replace(' ','_')\n\n# Select data from Santa Barbara county\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\n\n# Remove specified columns\ncol_remove = ['state_name', 'county_name', 'state_code', 'county_code']\naqi_sb = aqi_sb.drop(columns=col_remove)\n\n##date is in object format, and not in standard datetime python object\n# Convert 'Date' column to datetime\naqi_sb['date'] = pd.to_datetime(aqi_sb['date'])  \n\n\nAfter the data is cleaned, perform the smoothing by calculating moving average for the air quality value. Rolling average with period of 20D offers general average for 20Days. Any other values can be used other than 20.\n\n\nCode\n# Set 'Date' as the index\naqi_sb.set_index('date', inplace=True)\n#  DataFrame with the Date column as the index\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('20D').mean()"
  },
  {
    "objectID": "posts/False_color_analysis/index.html#data-visualization-part-b",
    "href": "posts/False_color_analysis/index.html#data-visualization-part-b",
    "title": "Detection of Forest fire using false color Image analysis",
    "section": "Data Visualization (Part B)",
    "text": "Data Visualization (Part B)\nFinally, create a graph to show how the air quality changed over two years. You’ll notice a significant increase at a specific time corresponding to the Thomas fire. The line graph, combined with the average value over a 20-day period, provides sufficient evidence to demonstrate the impact of the Thomas fire on air quality in the Santa Barbara area.\n\n\nCode\n# set the plot style\nplt.style.use('seaborn-darkgrid')\n\n# Create a white background\nfig, ax = plt.subplots(figsize=(10, 6))\nfig.patch.set_facecolor('white')\nax.set_facecolor('white')\n\n# Plot the data\nax.plot(aqi_sb.index, aqi_sb['aqi'], label='Daily AQI avlu', color='#00AA00')\nax.plot(aqi_sb.index, aqi_sb['five_day_average'], label='5-Day Average', color='#FFFF00')\n\n# Set the title and labels\nplt.title('Daily air quality and 5-day rolling average in Santa Barbara')\nplt.xlabel('Date')\nplt.ylabel('Air Quality Index')\nplt.legend()\n\n# Add grid for better readability\nax.grid(True, linestyle='--', alpha=0.7)\n\n# Customize y-axis ticks\nax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n\n# Set the axis ticks to be outside the plot\nax.tick_params(axis='both', direction='out')\n\n# Remove top and right spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n##add rectangle zoom\n# Add a rectangular zoom box\nzoom_date = '2017-12'  # Replace with your desired zoom date\nrect = Rectangle((mdates.datestr2num(zoom_date), ax.get_ylim()[0]),\n                 20, 150, linewidth=2, edgecolor='r', facecolor='none')  # Adjust rectangle size as needed\nax.add_patch(rect)\nplt.show()"
  },
  {
    "objectID": "posts/False_color_analysis/index.html#conclusion",
    "href": "posts/False_color_analysis/index.html#conclusion",
    "title": "Detection of Forest fire using false color Image analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, this exploration into the impact of the Thomas fire on the Santa Barbara area has uncovered crucial insights through the combination of false color imagery and air quality data analysis. The false color image provided a visual representation of the fire event, while the examination of air quality data from 2017 and 2018 allowed us to discern notable changes.\nUpon downloading and cleaning the air quality data, aligning coordinate reference systems, and converting netCDF grid cells into a structured array, a comprehensive dataset emerged. The subsequent plotting of air quality changes over the two-year period, along with a discernible spike corresponding to the Thomas fire, painted a compelling narrative of the environmental impact.\nThe utilization of line graphs, coupled with a 20-day rolling average, served as effective tools to illustrate the temporal fluctuations in air quality. The identified spike at the time of the Thomas fire provides clear evidence of the event’s influence on the region’s air quality. This integrated approach, combining imagery, data analysis, and visualization, enhances our understanding of the broader environmental implications of such natural disasters and underscores the importance of comprehensive monitoring and analysis in safeguarding environmental well-being.\n\nOffline Data and Folder structure at:\nGithub"
  },
  {
    "objectID": "posts/economic_zone/index.html",
    "href": "posts/economic_zone/index.html",
    "title": "Exploring Optimal Locations for Marine Aquaculture: A Comprehensive Spatial Analysis",
    "section": "",
    "text": "Marine aquaculture stands at the forefront of sustainable solutions for meeting the growing global demand for protein. In comparison to conventional land-based meat production, marine aquaculture offers a more eco-friendly approach. Previous research, such as the work by Gentry et al. (2017), has mapped the global potential for marine aquaculture, considering various constraints. In this project, my focus narrows to the Exclusive Economic Zones (EEZ) along the West Coast of the United States, aiming to identify the most suitable regions for developing marine aquaculture, specifically for various species of oysters.\nOysters, known for their unique ecological contributions and nutritional value, thrive under specific conditions. To ensure optimal growth, factors such as sea surface temperature and ocean depth play a crucial role. My analysis leverages data from NOAA and GEBCO to characterize these conditions and pinpoint the most favorable locations for oyster aquaculture."
  },
  {
    "objectID": "posts/economic_zone/index.html#introduction",
    "href": "posts/economic_zone/index.html#introduction",
    "title": "Exploring Optimal Locations for Marine Aquaculture: A Comprehensive Spatial Analysis",
    "section": "",
    "text": "Marine aquaculture stands at the forefront of sustainable solutions for meeting the growing global demand for protein. In comparison to conventional land-based meat production, marine aquaculture offers a more eco-friendly approach. Previous research, such as the work by Gentry et al. (2017), has mapped the global potential for marine aquaculture, considering various constraints. In this project, my focus narrows to the Exclusive Economic Zones (EEZ) along the West Coast of the United States, aiming to identify the most suitable regions for developing marine aquaculture, specifically for various species of oysters.\nOysters, known for their unique ecological contributions and nutritional value, thrive under specific conditions. To ensure optimal growth, factors such as sea surface temperature and ocean depth play a crucial role. My analysis leverages data from NOAA and GEBCO to characterize these conditions and pinpoint the most favorable locations for oyster aquaculture."
  },
  {
    "objectID": "posts/economic_zone/index.html#data",
    "href": "posts/economic_zone/index.html#data",
    "title": "Exploring Optimal Locations for Marine Aquaculture: A Comprehensive Spatial Analysis",
    "section": "Data",
    "text": "Data\n\nSea Surface Temperature\nAverage annual sea surface temperature data from 2008 to 2012, derived from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1, form the basis of our analysis. Data info\n\n\nBathymetry\nGeneral Bathymetric Chart of the Oceans GEBCO provides data on ocean depth, crucial for understanding the underwater topography.\n\n\nExclusive Economic Zones\nMaritime boundaries along the West Coast are designated using EEZ data from Marineregions.org. Marineregions.org."
  },
  {
    "objectID": "posts/economic_zone/index.html#data-preparation",
    "href": "posts/economic_zone/index.html#data-preparation",
    "title": "Exploring Optimal Locations for Marine Aquaculture: A Comprehensive Spatial Analysis",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo initiate the analysis, I load necessary packages and read in the West Coast EEZ shapefile (wc_regions_clean.shp). Additionally, I acquire and combine SST rasters for the years 2008 to 2012 and the bathymetry raster (depth.tif). The data undergoes checks for consistent coordinate reference systems.\n\n#load required packages\nlibrary(stars)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(patchwork)\nlibrary(ggspatial)\nlibrary(raster)\nlibrary(here)\n\n\n##read the data \ndepth_ocean &lt;- rast(here(\"posts\", \"economic_zone\", \"data\",\"data\", \"depth.tif\"))\n\n##stack the multiple temperature rasters into on \nyear_2008 &lt;- rast(\"data/data/average_annual_sst_2008.tif\")\nyear_2009 &lt;- rast(\"data/data/average_annual_sst_2009.tif\")\nyear_2010 &lt;- rast(\"data/data/average_annual_sst_2010.tif\")\nyear_2011 &lt;- rast(\"data/data/average_annual_sst_2011.tif\")\nyear_2012 &lt;- rast(\"data/data/average_annual_sst_2012.tif\")\n\n#stack all these SST data\ntemperature &lt;- stack(c(year_2008, year_2009, year_2010, year_2011, year_2012))\n\n\n##economic region data\nwest_coast_shape &lt;- st_read(\"data/data/wc_regions_clean.shp\")\n## Reading layer `wc_regions_clean' from data source \n##   `C:\\Users\\sujan\\OneDrive\\Documents\\MEDS\\new_github_account_website\\sujanbhattarai-jr.github.io\\posts\\economic_zone\\data\\data\\wc_regions_clean.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 5 features and 5 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\n## Geodetic CRS:  WGS 84\nplot(west_coast_shape['rgn_id'], main = NA)\n\n\n\n\nWest Coast Region with Five Sub-division\n\n\n\n\n\nData Processing\nThe process involves calculating mean SST for the specified years, converting temperatures to Celsius, and aligning SST and depth data. I reclassify SST and depth data to identify areas suitable for marine aquaculture, particularly for oysters.\n\n#average SST between 2008 and 2012\nSST_temperature &lt;- mean(year_2008, year_2009, year_2010, year_2011, year_2012)\n\n#convert to Celsius\nSST_temperature &lt;- SST_temperature - 273.15\n\n##match the extent of Depth and SST(change SST to match Depth)\n#make the crs same for both of them\n\nSST_temperature &lt;- project(SST_temperature, crs(depth_ocean))\n#crs(SST_temperature)==crs(depth_ocean)\n\n#crop the Depth to match SST\ncropped_depth_ocean &lt;- crop(depth_ocean, SST_temperature)\n\n#resample to match the resolution\ncropped_depth_ocean = resample(cropped_depth_ocean, y = SST_temperature, method = \"near\")  ##Minimum and maximum values are out of range\n\n\n\nIdentify Optimal Locations\nThe process involves calculating mean SST for the specified years, converting temperatures to Celsius, and aligning SST and depth data. I reclassify SST and depth data to identify areas suitable for marine aquaculture, particularly for oysters.\n\n##reclassify SST \nreclass_matrix_sst &lt;- matrix(c(-50, 10, NA, 11, 30, 1, 31, Inf, NA), ncol = 3, byrow = TRUE)\nsst_classified     &lt;- classify(SST_temperature, rcl = reclass_matrix_sst)\n\n## reclassify Depth\ncropped_depth_ocean[is.na(cropped_depth_ocean)] &lt;- 0\nreclass_matrix   &lt;- matrix(c(-10000, -71, NA, -70, 0, 1, 1, Inf, NA), ncol = 3, byrow = TRUE)\nocean_classified &lt;- classify(cropped_depth_ocean, rcl = reclass_matrix)\n\n#stack both rasters\nocean_sst_stacked = stack(x= c(sst_classified, ocean_classified))\n\n##function for stacking\nand_logic &lt;- function(x, y){\n  ifelse(x == 1 & y == 1, 1, NA)\n}\n\n\n# use local function to operate on each cells\nsuitable_for_sucker_fish &lt;- terra::lapp(rast(ocean_sst_stacked), fun = and_logic)\nplot(suitable_for_sucker_fish)\n\n\n\n\nSuitable region for the Oyster based on temperature and depth classification\n\n\n\n\n\n\nPrioritize Marine Aquaculture Zones\nThe next step involves calculating the total suitable area within each EEZ. I rasterize the EEZ data, determine suitable cells within each zone, and calculate the total suitable area. The percentage of each zone that is suitable is then computed. The output below shows the percent of area suitable for whole region.\n\n#create dummy variable for operation\nv =  west_coast_shape  #shape file\nr = suitable_for_sucker_fish ## raster data\n\n##since lat lon does not measure, projecting it to another for length measurement\ntarget_crs &lt;- \"+proj=utm +zone=33 +datum=WGS84 +units=m +no_defs\"\nr_transformed &lt;- project(r, target_crs)\n\n#resolution on both x and y side, as multplication of them gives area\nx = terra:: res(r_transformed)[1]\ny = terra:: res(r_transformed)[2]\n\n## extract all points that fall within in the EEZ shape file, and if raster value layers has NA, drop them\neez_zone = terra::extract(r, v) %&gt;% \n           na.omit()\n\n##calcluate area under each group ID and divide it by 10**6 to convert it to Kilimoeter square.\nareas_under_each_zone &lt;- eez_zone %&gt;% \n                         group_by(ID) %&gt;% \n                         count() %&gt;% \n                         mutate(area = (n * x * y)/10^6)\n\n##percentage of each zone that is suitable for zoning \ncombined_v_areas_under &lt;- inner_join(v, areas_under_each_zone, by  = c('rgn_id'='ID')) %&gt;% \n                          group_by('rgn_id') %&gt;% \n                          mutate(percentage_of_area = area/area_km2 * 100) %&gt;% \n                          ungroup()\n\n\n# percentage of area suitable for EEZ\ncombined_v_areas_under[c('rgn', 'percentage_of_area')]\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 3\n  rgn                 percentage_of_area                                geometry\n  &lt;chr&gt;                            &lt;dbl&gt;                      &lt;MULTIPOLYGON [°]&gt;\n1 Oregon                           0.963 (((-123.4318 46.23363, -123.4321 46.23…\n2 Northern California              0.148 (((-124.2102 41.99843, -124.2099 41.99…\n3 Central California               2.86  (((-122.9928 38.28022, -122.9926 38.27…\n4 Southern California              2.32  (((-120.6505 34.97431, -120.6502 34.97…\n5 Washington                       5.91  (((-122.7675 49.00031, -122.7603 48.99…"
  },
  {
    "objectID": "posts/economic_zone/index.html#visualization",
    "href": "posts/economic_zone/index.html#visualization",
    "title": "Exploring Optimal Locations for Marine Aquaculture: A Comprehensive Spatial Analysis",
    "section": "Visualization",
    "text": "Visualization\nTo enhance understanding, I visualize the results through various plots and maps. Basemaps provide context, and these visualizations include maps of suitable regions within each EEZ and the percentage of suitable areas.\n\n#prepare the necessary class of the data\ncliped_points &lt;- as.polygons(suitable_for_sucker_fish) %&gt;% \n                     st_as_sf() \n\n##clip the area nad make it ready for maps\ncliped_points &lt;- st_intersection(cliped_points, combined_v_areas_under)\ncliped_points[\"Suitable_region\"] &lt;- \"\"\n\n\n## plot the suitable regions within each eez\neconomic_zone &lt;- ggplot(combined_v_areas_under)+\n  geom_sf(aes(fill = rgn), alpha = 0.5)+\n  labs(fill = \"Economic zone\")+\n  annotation_scale()+\n  annotation_north_arrow( location = \"tr\", \n                          height = unit(0.7, \"cm\"),\n                          width  = unit(0.7, \"cm\"),\n                          style = north_arrow_minimal())\neconomic_zone +\n  geom_sf(data = cliped_points, aes(color = Suitable_region), shape = 20, alpha = 0.5) +\n  theme_bw()+\n  theme(aspect.ratio = 1.5)\n\n\n\n\nRegion in west coast where suitable zone exists for the Oyster\n\n\n\n\n\n\nCode\n# plot the bar plot to visualize percentage by zone\nggplot(combined_v_areas_under) +\ngeom_bar(aes(x = reorder(rgn, percentage_of_area), y = percentage_of_area),\n           stat = \"identity\", fill = 'lightblue') +\n          labs(x = \"Region\", y = \"Percentage area\") +\n          theme_bw()+\n          coord_flip()\n\n\n\n\n\nPercentage area suitable for Oyster in Each region in West Coast\n\n\n\n\n\nFetch basemaps\nThere used to be stamenmap function for the Basemap, but recently, there has been a change. The package requires API verification through google. So, I used the data stored inside SF package and loaded as Basemap for the west coast region. This is not the best way, but it does provide clues on which region the each suitable EEZ falls into.\n\n\nCode\n##subset each region and overlay on the coastline\ncalifornia &lt;- us_geo %&gt;% filter(NAME == 'California')\noregon     &lt;- us_geo %&gt;% filter(NAME == \"Oregon\")\nwashington &lt;- us_geo %&gt;% filter(NAME == \"Washington\")\n\n\n\n##use tmshape to plot the data\ntm_shape(combined_v_areas_under)+\n  tm_polygons(fill = 'percentage_of_area')\n\n\n\n\nAlternative maps including basemaps for the Economic Zone\n\n\n\n  # tm_shape(california)+\n  # tm_text(\"California\", size = 0.8, col = \"black\", bg.col = \"white\")+\n  # tm_borders(col = 'black')+\n  # tm_shape(oregon)+\n  # tm_text(\"Oregon\", size = 0.8, col = \"black\", bg.col = \"white\")+\n  # tm_borders()+\n  # tm_shape(washington)+\n  # tm_text(\"Washington\", size = 0.8, col = \"black\", bg.col = \"white\")+\n  # tm_borders()\n\n\n\nCreate a Function\nThis workflow concludes by providing a function for broader applicability. The function allows external users to analyze the suitability of locations for different fish species by specifying temperature and depth requirements. We demonstrate this by running the function for the “Abra aequalis” species. SeaLifeBase\n\n##write whole function out on this \neconomic_zone_finder &lt;- function(min_temp  =  min_temp,\n                                 max_temp  =  max_temp,\n                                 min_depth =  min_depth,\n                                 max_depth =  max_depth,\n                                 fish_species = fish){\n\ndepth_ocean &lt;- rast(here(\"posts\", \"economic_zone\", \"data\",\"data\", \"depth.tif\"))\n##stack the multiple temperature rasters\nyear_1 &lt;- rast(\"data/data/average_annual_sst_2008.tif\")\nyear_2 &lt;- rast(\"data/data/average_annual_sst_2009.tif\")\nyear_3 &lt;- rast(\"data/data/average_annual_sst_2010.tif\")\nyear_4 &lt;- rast(\"data/data/average_annual_sst_2011.tif\")\nyear_5 &lt;- rast(\"data/data/average_annual_sst_2012.tif\")\n\n#perform data manipulation\ntemperature_stacked  &lt;-   stack(c(year_1, year_2, year_3, year_4, year_5))\nwest_coast           &lt;-   st_read(\"data/data/wc_regions_clean.shp\")\nsst_temperature      &lt;-   rast(mean(temperature_stacked) - 273.15)  #(find mean among year and convert to celsius)\n\n##match the extent of Depth and SST(change SST to match Depth)\n#check if the crs is same\nsst_temperature &lt;- project(sst_temperature, crs(depth_ocean))\n\n\n##match the extent and origin of surface temperature and ocen depth and resample\ncropped_depth_ocean &lt;- resample(crop(depth_ocean, sst_temperature), \n                                       y = sst_temperature, method = 'near')\n\n##reclassify surface temperatue \nreclass_matrix_sst &lt;- matrix(c(-Inf, min_temp, NA, \n                               min_temp + 1, max_temp, 1,\n                               max_temp + 1, Inf, NA),\n                               ncol = 3, \n                               byrow = TRUE)\n\nsst_classified &lt;- classify(sst_temperature, rcl = reclass_matrix_sst)\n\n## reclassify Depth\nreclass_matrix &lt;- matrix(c(-Inf, min_depth, NA,\n                           min_depth, max_depth, 1,\n                           max_depth, Inf, NA),\n                           ncol = 3,\n                           byrow = TRUE)\n\nocean_classified &lt;- classify(cropped_depth_ocean, rcl = reclass_matrix)\n\n#stack surface temperature with ocean depth\nocean_sst_stacked = stack(x= c(sst_classified, ocean_classified))\n\n##function for applying function across global layers\nlogic &lt;- function(x, y){\n  ifelse(x == 1 & y == 1, 1, NA)}\n\n# use local function to operate on each cells\nsuited_region_for_fish &lt;- terra::lapp(rast(ocean_sst_stacked), fun = and_logic)\n\n#create dummy variable for operation\nv =  west_coast  #shape file\nr = suited_region_for_fish ## raster data\n\n##since lat lon does not measure, projecting it to another for length measurement\ntarget_crs &lt;- \"+proj=utm +zone=33 +datum=WGS84 +units=m +no_defs\"\nr_transformed &lt;- project(r, target_crs)\n\n#resolution on both x and y side, as multplication of them gives area\nx = terra:: res(r_transformed)[1]\ny = terra:: res(r_transformed)[2]\n\n## extract all points that fall within in the EEZ shape file, and if raster value layers has NA, drop them\neez_zone = terra::extract(r, v) %&gt;% \n           na.omit()\n\n##calcluate area under each group ID and divide it by 10**6 to convert it to Kilimoeter square.\nareas_under_each_zone &lt;- eez_zone %&gt;% \n                         group_by(ID) %&gt;% \n                         count() %&gt;% \n                         mutate(area = (n * x * y)/10^6)\n\n## total suitable areas for zoning across all IDS\nprint(sum(areas_under_each_zone))\n\n##percentage of each zone that is suitable for zoning \ncombined_v_areas_under &lt;- inner_join(v, areas_under_each_zone, by  = c('rgn_id'='ID')) %&gt;% \n                          group_by('rgn_id') %&gt;% \n                          mutate(percentage_of_area = area/area_km2 * 100) %&gt;% \n                          ungroup()\n\n# percentage of area suitable for EEZ\ncombined_v_areas_under[c('rgn', 'percentage_of_area')] \n\n##polygons\ncliped_points &lt;- as.points(suited_region_for_fish) %&gt;% \n                     st_as_sf() \ncliped_points &lt;- st_intersection(cliped_points, combined_v_areas_under)\ncliped_points[\"Suitable_region\"] &lt;- \"\"\n\n# plot the points on vector scaled\neconomic_zone &lt;- ggplot(combined_v_areas_under)+\n  geom_sf(aes(fill = rgn))+\n  labs(fill = \"Economic zone\")+\n  ggtitle(as.character(paste(\"Suitable region for\", fish_species)))+\n  geom_sf(data = cliped_points, size = 0.5)+\n  annotation_scale()+\n  annotation_north_arrow( location = \"tr\", \n                          height = unit(0.7, \"cm\"),\n                          width  = unit(0.7, \"cm\"),\n                          style = north_arrow_minimal())\n\nsuitable_eez &lt;- economic_zone +\n                geom_sf(data = cliped_points, aes(color = Suitable_region), shape = 20, alpha = 0.5) +\n                theme_bw()+\n                theme(aspect.ratio = 1.5)\n  \n## percent suitable area by region \npercent_area &lt;- ggplot(combined_v_areas_under)+\n                geom_sf(aes(fill = percentage_of_area))+\n                scale_fill_continuous(low=\"red\", high = \"green\")+\n                ggtitle(as.character(paste(\"Suitable Percentage region for\", fish_species)))+\n                annotation_scale()+\n                annotation_north_arrow( location = \"tr\", \n                                        height = unit(0.7, \"cm\"),\n                                        width  = unit(0.7, \"cm\"),\n                                        style = north_arrow_minimal())\n\n##plot with basemap, to show where each location fall under\n# with_basemap &lt;- tm_shape(combined_v_areas_under)+\n#                 tm_polygons(fill = 'percentage_of_area')+\n#                 tm_shape(california)+\n#                 tm_text(\"California\", size = 0.8, col = \"black\", bg.col = \"white\")+\n#                 tm_borders()+\n#                 tm_shape(oregon)+\n#                 tm_text(\"Oregon\", size = 0.8, col = \"black\", bg.col = \"white\")+\n#                 tm_borders()+\n#                 tm_shape(washington)+\n#                 tm_text(\"Washington\", size = 0.8, col = \"black\", bg.col = \"white\")+\n#                 tm_borders()+\n#                 tm_layout(title = \"Percentage area with basemap\")\n\nreturn(list(suitable_eez))}\n\n\nFunction that create EEZ for a sample species\nThe function can be used for any marine species around the west coast, if their temperature and depth range is known. Depth is always taken at negative, however there is no need to insert negative values in depth, as the function call automatically makes it negative when a function is called.\n\n##calculate for abra aequalis that survives between 0 to 28 degree depth, and tempetaure up to 73 degrees                         \neconomic_zone_finder(min_temp = 0, max_temp = 73, min_depth = 0, max_depth = 28, fish_species = \"Abra aequalis\")\n## Reading layer `wc_regions_clean' from data source \n##   `C:\\Users\\sujan\\OneDrive\\Documents\\MEDS\\new_github_account_website\\sujanbhattarai-jr.github.io\\posts\\economic_zone\\data\\data\\wc_regions_clean.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 5 features and 5 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\n## Geodetic CRS:  WGS 84\n## [1] 318.8414\n## [[1]]"
  },
  {
    "objectID": "posts/economic_zone/index.html#conclusion",
    "href": "posts/economic_zone/index.html#conclusion",
    "title": "Exploring Optimal Locations for Marine Aquaculture: A Comprehensive Spatial Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn this comprehensive analysis, I have laid the foundation for identifying optimal locations for marine aquaculture along the West Coast. By considering temperature and depth requirements, I provide valuable insights for sustainable oyster farming. The reproducible workflow also enables future analyses for different fish species. This project contributes to the ongoing efforts to harness marine resources responsibly, ensuring a resilient and sustainable food supply.\n\nReferences\n\nHall, S. J., et al. (2011). Blue Frontiers: Managing the Environmental Costs of Aquaculture. Gentry, R. R., et al. (2017). “Mapping the global potential for marine aquaculture.” Nature Ecology & - Evolution, 1, 1317-1324.\nGEBCO Compilation Group (2022). GEBCO_2022 Grid (doi:10.5285/e0f0bb80-ab44-2739-e053-6c86abc0289c)."
  },
  {
    "objectID": "featured_projects/textAnalysis/index.html",
    "href": "featured_projects/textAnalysis/index.html",
    "title": "Identifying the most common word in articles on climate science",
    "section": "",
    "text": "In today’s era of burgeoning information, understanding the predominant themes and trends in vast volumes of text data is crucial for extracting meaningful insights. This project focuses on leveraging Natural Language Processing (NLP) techniques to analyze a collection of articles centered on climate science. By employing advanced computational methods, I aim to uncover the most prevalent topics discussed across these articles, shedding light on key issues and their interconnectedness within the domain of climate science."
  },
  {
    "objectID": "featured_projects/textAnalysis/index.html#objectives",
    "href": "featured_projects/textAnalysis/index.html#objectives",
    "title": "Identifying the most common word in articles on climate science",
    "section": "Objectives",
    "text": "Objectives\n\nData Collection and Preparation\n\nData Retrieval: Obtain a comprehensive dataset comprising articles related to climate science, sourced from diverse publications.\nData Cleaning: Standardize and preprocess the text data to remove noise, such as URLs, dates, and duplicates, ensuring a clean corpus for analysis.\n\nTopic Modeling and Optimization\n\nTopic Extraction: Apply Latent Dirichlet Allocation (LDA) to identify latent topics within the article corpus.\nOptimization: Determine the optimal number of topics using quantitative metrics such as CaoJuan2009 and Deveaud2014, ensuring robust and interpretable results.\n\nInsights Generation\n\nTopic Interpretation: Analyze and interpret the extracted topics to discern prevalent themes and their implications in climate science research.\nVisualization: Visualize topic distributions and top terms to facilitate intuitive understanding and exploration of thematic clusters.\n\n\n\n# Change data_list into table for analysis, extract metadata\ndata_table &lt;- data_list@meta\n\n# Create a tibble with date, headline, id, and article\ndata_tibble &lt;- tibble(Date = data_table$Date, \n                      Headline = data_table$Headline, \n                      id = data_list@articles$ID, \n                      text = data_list@articles$Article)\n\n# Clean the dataset\ndata_tibble &lt;- data_tibble %&gt;% \n  # Remove all URLs and dates from the text\n  mutate(text = str_remove_all(text, paste(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]\",\n                               \"|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")),\n  text = str_remove_all(text, \"\\\\b\\\\d{1,2}/\\\\d{1,2}/\\\\d{4}\\\\b\"),\n  # Remove words with this pattern: February 14th, 2024 \n  text = str_remove_all(text, paste0(\"\\\\b\\\\w+\\\\s\\\\d{1,2}[a-z]{2},\\\\s\\\\d{4}\\\\s\\\\\",\n                                    \"(\\\\s\\\\w+\\\\s—\\\\sDelivered\\\\sby\\\\sNewstex\\\\s\\\\)\")))\n\n# Remove duplicates in the text if they have the same words in the first 20 words\ndata_tibble &lt;- data_tibble %&gt;% distinct(Headline, .keep_all = TRUE)\n\n# Calculate the distance between duplicates and remove the duplicates\ndata_tibble &lt;- data_tibble %&gt;%\n  mutate(dist = stringdist::stringdist(text, lag(text), method = \"jaccard\")) %&gt;% \n  filter(dist &lt; 0.5 | is.na(dist)) %&gt;% select(-dist) %&gt;% \n  # Make all text lowercase\n  mutate(text = tolower(text))\n\n\n# Create a corpus\ncorpus &lt;- corpus(x = data_tibble, text_field = \"text\")\ntokens(corpus)\n\nTokens consisting of 51 documents and 3 docvars.\ntext1 :\n [1] \"february\"    \"14\"          \",\"           \"2024\"        \"release\"    \n [6] \"date\"        \"-\"           \"13022024\"    \"-\"           \"researchers\"\n[11] \"closely\"     \"observed\"   \n[ ... and 947 more ]\n\ntext2 :\n [1] \"katarzyna\" \"kudlacz\"   \"was\"       \"preparing\" \"a\"         \"breakfast\"\n [7] \"of\"        \"scrambled\" \"eggs\"      \"at\"        \"a\"         \"research\" \n[ ... and 395 more ]\n\ntext3 :\n [1] \"the\"         \"following\"   \"information\" \"was\"         \"released\"   \n [6] \"by\"          \"defenders\"   \"of\"          \"wildlife\"    \":\"          \n[11] \"ragen\"       \"davey\"      \n[ ... and 782 more ]\n\ntext4 :\n [1] \"the\"           \"albuquerque\"   \"biopark\"       \"zoo\"          \n [5] \"could\"         \"bear-ly\"       \"contain\"       \"its\"          \n [9] \"excitement\"    \"for\"           \"international\" \"polar\"        \n[ ... and 324 more ]\n\ntext5 :\n [1] \"by\"         \"associated\" \"press\"      \"for\"        \"polar\"     \n [6] \"bears\"      \",\"          \"the\"        \"climate\"    \"change\"    \n[11] \"diet\"       \"is\"        \n[ ... and 891 more ]\n\ntext6 :\n [1] \"scientists\"  \"fear\"        \"canadian\"    \"polar\"       \"bears\"      \n [6] \"may\"         \"be\"          \"threat-ened\" \"by\"          \"the\"        \n[11] \"spread\"      \"of\"         \n[ ... and 444 more ]\n\n[ reached max_ndoc ... 45 more documents ]\n\nadd_stops &lt;- stopwords(kind = quanteda_options(\"language_stopwords\"))\n\ntoks &lt;- tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE)\ntok1 &lt;- tokens_select(toks, pattern = add_stops, selection = \"remove\") # Remove stop words\n\ndfm1 &lt;- dfm(tok1, tolower = TRUE)\ndfm2 &lt;- dfm_trim(dfm1, min_docfreq = 2) # Word has to appear at least 2 times to stay in matrix\n\n# As long as sum across the row is &gt; 0, retain it\nsel_idx &lt;- slam::row_sums(dfm2) &gt; 0\ndfm &lt;- dfm2[sel_idx, ]\n\n\n# Set value of k (how many ideas/themes within articles in this data set)\n\n# Set model with k = 2\nk &lt;- 2\ntopicModel_k2 &lt;- LDA(dfm,\n                     k,\n                     method = \"Gibbs\",\n                     control = list(iter = 1000), # Number of iterations\n                     verbose = 25) # Track progress after every 25 iterations\n\nk &lt;- 3 \ntopicModel_k3 &lt;- LDA(dfm,\n                     k,\n                     method = \"Gibbs\",\n                     control = list(iter = 1000), # Number of iterations\n                     verbose = 25) # Track progress after every 25 iterations\n\n# Run another model with k = 4\nk &lt;- 4\ntopicModel_k4 &lt;- LDA(dfm,\n                     k,\n                     method = \"Gibbs\",\n                     control = list(iter = 1000), # Number of iterations\n                     verbose = 25) # Track progress after every 25 iterations\n\n# Run another model with range of values of k\nresults &lt;- FindTopicsNumber(dfm,\n                            topics = seq(from = 2,\n                                         to = 20, # Number of topics\n                                         by = 1),\n                            metrics = c(\"CaoJuan2009\", \"Deveaud2014\"),\n                            method = \"Gibbs\",\n                            control = list(iter = 1000), # Number of iterations\n                            verbose = 25) # Track progress after every 25 iterations\n\nfit models... done.\ncalculate metrics:\n  CaoJuan2009... done.\n  Deveaud2014... done.\n\nresults\n\n   topics CaoJuan2009 Deveaud2014\n1      20   0.1712109    1.320867\n2      19   0.2138635    1.275245\n3      18   0.1786250    1.347951\n4      17   0.2378534    1.363592\n5      16   0.2151895    1.429165\n6      15   0.2284857    1.434625\n7      14   0.2729804    1.437147\n8      13   0.2485207    1.459438\n9      12   0.2891788    1.467272\n10     11   0.2664270    1.525242\n11     10   0.2823625    1.574507\n12      9   0.3304659    1.551699\n13      8   0.3788804    1.562623\n14      7   0.3586914    1.547233\n15      6   0.4176206    1.527413\n16      5   0.4742997    1.546658\n17      4   0.4880627    1.551933\n18      3   0.5252982    1.584083\n19      2   0.5753212    1.579636\n\n\n\n# Plot topics and both metrics for getting the best value\nggplot(results, aes(x = topics, y = CaoJuan2009)) +\n  geom_line() +\n  theme_minimal()\n\n\n\nggplot(results, aes(x = topics, y = Deveaud2014)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\nor my sets of articles, the optimization metrics continuously resulted in Null, showing that the model is not able to find the best value of k. However, the optimization metrics for the model with k = 2 is the lowest, suggesting that the model with k = 2 is the best model for my data set. The plots show a similar result, as there is no stabilization of trend for different values of optimization metrics. So, for the dataset, I will go with the model with k = 2.\n\n# Plot the top terms in each topic\nterms(topicModel_k2, 10) # Most probable words in each topic\n\n      Topic 1  Topic 2        \n [1,] \"bears\"  \"polar\"        \n [2,] \"polar\"  \"bear\"         \n [3,] \"land\"   \"bears\"        \n [4,] \"ice\"    \"ice\"          \n [5,] \"energy\" \"arctic\"       \n [6,] \"study\"  \"international\"\n [7,] \"sea\"    \"said\"         \n [8,] \"said\"   \"change\"       \n [9,] \"time\"   \"wildlife\"     \n[10,] \"bear\"   \"day\"          \n\ntmResult &lt;- posterior(topicModel_k2)\nterms(topicModel_k2)\n\nTopic 1 Topic 2 \n\"bears\" \"polar\" \n\ntheta &lt;- tmResult$topics\nbeta &lt;- tmResult$terms\nvocab &lt;- colnames(beta)\n\n# Plot the distribution of topics across a sample of the documents from beta\ntopics &lt;- tidy(topicModel_k2, matrix = \"beta\")\n\ntop_terms &lt;- topics |&gt; \n  group_by(topic) |&gt; \n  top_n(10, beta) |&gt; \n  ungroup() |&gt; \n  arrange(topic, -beta)\n\nkableExtra::kable(top_terms)\n\n\n\n\ntopic\nterm\nbeta\n\n\n\n\n1\nbears\n0.0582435\n\n\n1\npolar\n0.0347553\n\n\n1\nland\n0.0164051\n\n\n1\nice\n0.0160381\n\n\n1\nenergy\n0.0143254\n\n\n1\nstudy\n0.0140807\n\n\n1\nsea\n0.0109000\n\n\n1\nsaid\n0.0096767\n\n\n1\ntime\n0.0090650\n\n\n1\nbear\n0.0088203\n\n\n2\npolar\n0.0593949\n\n\n2\nbear\n0.0374399\n\n\n2\nbears\n0.0257971\n\n\n2\nice\n0.0119921\n\n\n2\narctic\n0.0108278\n\n\n2\ninternational\n0.0098298\n\n\n2\nsaid\n0.0096635\n\n\n2\nchange\n0.0093309\n\n\n2\nwildlife\n0.0091645\n\n\n2\nday\n0.0089982\n\n\n\n\n# Plot the distribution of topics across a sample of the documents from theta\n\ntop_terms |&gt; \n  mutate(term = reorder_within(term, beta, topic, sep = \"\")) |&gt; \n  ggplot(aes(term, beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~topic, scales = \"free_y\") +\n  scale_x_reordered() +\n  coord_flip() +\n  # Remove digits from labels \n  scale_x_discrete(labels = function(x) gsub(\"[0-9]\", \"\", x)) +\n  theme_minimal()\n\n\n\n\nUpon analyzing the topics extracted from the articles using a two-topic classification model, it becomes evident that the primary focus of the articles revolves around polar bears, as indicated by the presence of the search item ‘polar bear’ across all topics. The specificity of the search item directly influences the thematic coherence of the topics.\nIn the first topic, discussions predominantly centers around polar bear hunting and climate change research. This topic also touches on sea ice and the Arctic, possibly suggesting a thematic exploration of the ecological and environmental challenges faced by polar bears in the context of climate change.\nLastly, the second topic likely appears to explore the intricate relationship between polar bears and the importance of World Ice Day. It highlights the interconnectedness of these elements within the Arctic ecosystem, emphasizing the ecological implications of environmental shifts on polar bear populations and the emotion of the global community.\nOverall, the topics extracted from the articles provide valuable insights that polar bears are not talked about in any other topics, other than explicitly discussing polar bears. Exploring more articles and topics could provide more insights on the themes discussed in the articles, but for my subset of articles, polar bears are not discussed in any other context other than research and when discussing international ice days."
  },
  {
    "objectID": "featured_projects/econometrics/index.html",
    "href": "featured_projects/econometrics/index.html",
    "title": "Did Smoking Mothers Have Lowered Weight Children?",
    "section": "",
    "text": "Summary and Conclusion\nThe analysis aimed to estimate the causal effect of maternal smoking during pregnancy on infant birth weight. Initial comparisons showed a significant difference in birth weights between infants of smoking and non-smoking mothers. However, confounding variables suggested that the groups were not comparable.\nPropensity score matching was applied to address these confounders. Post-matching tests indicated successful balancing of the treatment and control groups. The final analysis showed that maternal smoking causally reduces infant birth weight by an average of 13 grams, after accounting for confounding variables. This result highlights the importance of proper matching techniques in observational studies to draw valid causal inferences\n\n\nProject Begins:\nThe goal is to estimate the causal effect of maternal smoking during pregnancy on infant birth weight using the treatment ignorability assumptions. The data are taken from the National Natality Detail Files, and the extract “SMOKING_EDS241.csv”’ is a random sample of all births in Pennsylvania during 1989-1991. Each observation is a mother-infant pair. The key variables are:\nThe outcome and treatment variables are:\nbirthwgt=birth weight of infant in grams\ntobacco=indicator for maternal smoking\nThe control variables are:\nmage (mother’s age), meduc (mother’s education), mblack (=1 if mother identifies as Black), alcohol (=1 if consumed alcohol during pregnancy), first (=1 if first child), diabete (=1 if mother diabetic), anemia (=1 if mother anemic)\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(stargazer)\nlibrary(plm)\nlibrary(pglm)\nlibrary(dplyr)\nlibrary(MatchIt)\nlibrary(lmtest)\nlibrary(RItools)\nlibrary(sandwich)\nlibrary(estimatr)\nlibrary(Hmisc)\n\n\n# Load data\nsmoking_data &lt;- read_csv(here(\"featured_projects/econometrics/data/SMOKING_EDS241.csv\"))\n\n\nMean Differences, Assumptions, and Covariates\nFor conducting t-test, it is important to have population divided into treatment and control group. Since this experiment already has that information, I can segregate them into two tables and perform t-tests.\n\n## calculating difference using t.test when tobacco is 1 and 0\nsmoking_mothers = smoking_data %&gt;% filter(tobacco == 1)\nnon_smoking_mothers = smoking_data %&gt;% filter(tobacco == 0)\n\n#peform t-test to see if the difference is significant in other covariates\nt.test(smoking_mothers$birthwgt, non_smoking_mothers$birthwgt)\n\n\n    Welch Two Sample t-test\n\ndata:  smoking_mothers$birthwgt and non_smoking_mothers$birthwgt\nt = -58.932, df = 26945, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -252.6727 -236.4060\nsample estimates:\nmean of x mean of y \n 3185.747  3430.286 \n\n\nThe mean difference is 244.539 grams between children from smoker and non-smoker mothers, which is significant at 5%. The assumptions for this mean difference to hold are ignorability (no other confounding variables influencing the outcome) and common support (there is sufficient overlap between the treatment and control group). If these assumptions are satisfied, I can infer the difference is actually due to smoking and not due to random chance.\nI can test those assumptions with t-tests for numeric covariates, and proportion test for categorical covariates The following code achives that:\n\n#set options to have maximum 5 decimal\noptions(digits=5)\n## For continuous variables I can use the t-test\n#t.test()\neducation &lt;- t.test(smoking_mothers$meduc, non_smoking_mothers$meduc)\nage &lt;- t.test(smoking_mothers$mage, non_smoking_mothers$mage)\nbirthwht &lt;- t.test(smoking_mothers$birthwgt, non_smoking_mothers$birthwgt)\n\n## For binary variables I should use the proportions test\n#prop.test()\nalcohol &lt;- prop.test(table(smoking_mothers$alcohol), table(non_smoking_mothers$alcohol))\nfirst_child &lt;-prop.test(table(smoking_mothers$first), table(non_smoking_mothers$first))\ndiabetes&lt;- prop.test(table(smoking_mothers$diabete), table(non_smoking_mothers$diabete))\nanaemia &lt;- prop.test(table(smoking_mothers$anemia),  table(non_smoking_mothers$anemia))\nblack   &lt;- prop.test(table(smoking_mothers$mblack),  table(non_smoking_mothers$mblack))\n\n#-------------- Covariate Calculations and Tables\n\n# create dataframe of coefficents from above results including\n# first column should be variable name, then mean of estimate for sample 1, then\n# mean of sample 2, then p values \ntable &lt;- data.frame(\n  variable = c(\"birthweight\", \"education\", \"age\", \"alcohol\", \"first_child\", \"diabetes\", \"anaemia\", \"black\"),\n  smoking_mothers = c(birthwht$estimate[1], education$estimate[1], age$estimate[1], \n                      sum(smoking_mothers$alcohol)/length(smoking_mothers$alcohol),  \n                      sum(smoking_mothers$first)/length(smoking_mothers$first), \n                      sum(smoking_mothers$diabete)/length(smoking_mothers$diabete), \n                      sum(smoking_mothers$anemia)/length(smoking_mothers$anemia), \n                      sum(smoking_mothers$mblack)/length(smoking_mothers$mblack)),\n  \n  non_smoking_mothers = c(birthwht$estimate[2], education$estimate[2], age$estimate[2], \n                          sum(non_smoking_mothers$alcohol)/length(non_smoking_mothers$alcohol),\n                          sum(non_smoking_mothers$first)/length(non_smoking_mothers$first),\n                          sum(non_smoking_mothers$diabete)/length(non_smoking_mothers$diabete), \n                          sum(non_smoking_mothers$anemia)/length(non_smoking_mothers$anemia), \n                          sum(non_smoking_mothers$mblack)/length(non_smoking_mothers$mblack)),\n  \n  p_value = round(c(birthwht$p.value, \n                    education$p.value, \n                    age$p.value, \n                    alcohol$p.value, \n                    first_child$p.value, \n                    diabetes$p.value, \n                    anaemia$p.value, \n                    black$p.value), 6))\n\nprint(table)\n\n     variable smoking_mothers non_smoking_mothers p_value\n1 birthweight      3.1857e+03          3.4303e+03       0\n2   education      1.1921e+01          1.3239e+01       0\n3         age      2.5539e+01          2.7453e+01       0\n4     alcohol      4.4182e-02          7.1033e-03       0\n5 first_child      3.6459e-01          4.3609e-01       0\n6    diabetes      1.7519e-02          1.7364e-02       0\n7     anaemia      1.4103e-02          7.8005e-03       0\n8       black      1.3541e-01          1.0863e-01       0\n\n\nInterpretation: The result shows that the population sample on treated and control group is dissimilar. The p values are less than .05, which means alternative hypothesis shoule be accepted. Alternative hypothesis for this test is: there is difference between treated and control group.\nSo far, I know there is already a difference in the two samples. But we can still quantify how much these covariates influence these biased samples. The following chunk will do that by calculating average treatment effects with a regression model for these biased groups.\n\n# ATE Regression univariate\ntobacco_univariate &lt;- lm(birthwgt ~ tobacco, data = smoking_data)\n\n# ATE with covariates\ntobacco_covariates &lt;- lm(birthwgt ~  tobacco + mage +  meduc +\n                           mblack + alcohol + first + diabete + anemia, \n                      data = smoking_data)\n\n## create combined table\nstargazer(tobacco_univariate, tobacco_covariates, type = \"text\", \n          out.header = TRUE, \n          title = \"Regression with and without controls\",\n          notes.label = \"significance level\")\n\n\nRegression with and without controls\n===========================================================================\n                                      Dependent variable:                  \n                    -------------------------------------------------------\n                                           birthwgt                        \n                                (1)                         (2)            \n---------------------------------------------------------------------------\ntobacco                     -244.540***                 -228.070***        \n                              (4.079)                     (4.177)          \n                                                                           \nmage                                                      -0.694*          \n                                                          (0.357)          \n                                                                           \nmeduc                                                    11.688***         \n                                                          (0.860)          \n                                                                           \nmblack                                                  -240.030***        \n                                                          (5.106)          \n                                                                           \nalcohol                                                  -77.350***        \n                                                          (13.465)         \n                                                                           \nfirst                                                    -96.944***        \n                                                          (3.447)          \n                                                                           \ndiabete                                                  73.228***         \n                                                          (12.104)         \n                                                                           \nanemia                                                     -4.796          \n                                                          (16.754)         \n                                                                           \nConstant                    3,430.300***                3,362.300***       \n                              (1.791)                     (11.927)         \n                                                                           \n---------------------------------------------------------------------------\nObservations                   94,173                      94,173          \nR2                             0.037                       0.072           \nAdjusted R2                    0.037                       0.072           \nResidual Std. Error     493.750 (df = 94171)        484.730 (df = 94164)   \nF Statistic         3,594.300*** (df = 1; 94171) 909.180*** (df = 8; 94164)\n===========================================================================\nsignificance level                              *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nIf I have to see the result above, seems like all covariates are responsible for change in birthweights in the smoking and non smoking mothers. But since the treated and control groups are dissimilar, it should not be inferred that these covariates are producing change.\nI can do one more test before I process the data to make them true. I can test if any of the covariates have a similar population between the treated and control groups already. I can use chi-squared tests among all these variables to see which one truly represents a similar population being compared.\n\n# perform balance test\nx &lt;- xBalance(tobacco ~ mage +  meduc + mblack + alcohol + first + diabete + anemia + birthwgt,  data = smoking_data,\n         report=c(\"std.diffs\",\"chisquare.test\", \"p.values\"))\n\n#use staragazer to present the results\nas.data.frame(x[1]) %&gt;% \n   #round last column to 5 digit\n  mutate_if(is.numeric, round, 5) %&gt;% \n  #rename columns based on number index\n  setNames(c( \"chi-Square/standard difference test\", \"p-value\"))\n\n         chi-Square/standard difference test p-value\nmage                                -0.36194  0.0000\nmeduc                               -0.64374  0.0000\nmblack                               0.08439  0.0000\nalcohol                              0.31525  0.0000\nfirst                               -0.14500  0.0000\ndiabete                              0.00119  0.8858\nanemia                               0.06670  0.0000\nbirthwgt                            -0.49527  0.0000\n\n\nOnly the diabetic covariate is similar between sample group at this point. What this means is that the population samples between treatment and control group are similar only in regards to diabetes.\nLets also calculate propensity estimation for this biased sample:\n\n\nPropensity Score Estimation for the biased Treated and control Groups\n\n## Propensity Scores estimation with logistic regression\nmother_prospensityscore &lt;- glm(tobacco ~  mage +  meduc +\n                           mblack + alcohol + first + diabete + anemia + birthwgt, data = smoking_data,\n                              family = binomial())\n\n## create a table of coefficients\nstargazer:: stargazer(mother_prospensityscore, type = \"text\")\n\n\n=============================================\n                      Dependent variable:    \n                  ---------------------------\n                            tobacco          \n---------------------------------------------\nmage                       -0.040***         \n                            (0.002)          \n                                             \nmeduc                      -0.288***         \n                            (0.005)          \n                                             \nmblack                     -0.361***         \n                            (0.028)          \n                                             \nalcohol                    1.962***          \n                            (0.062)          \n                                             \nfirst                      -0.460***         \n                            (0.020)          \n                                             \ndiabete                    0.229***          \n                            (0.067)          \n                                             \nanemia                     0.333***          \n                            (0.081)          \n                                             \nbirthwgt                   -0.001***         \n                           (0.00002)         \n                                             \nConstant                   6.456***          \n                            (0.090)          \n                                             \n---------------------------------------------\nObservations                94,173           \nLog Likelihood            -40,841.000        \nAkaike Inf. Crit.         81,699.000         \n=============================================\nNote:             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n#create regression table dataframe based on mother_prospensityscore\n# Assuming you have a regression model object named 'model'\n# You would need to replace 'model' with the actual name of your model object\n\n\nmodel = mother_prospensityscore\n# Extract coefficients, standard errors, and p-values from the model\ncoefficients &lt;- coef(model)\nstandard_errors &lt;- sqrt(diag(vcov(model)))\np_values &lt;- summary(model)$coefficients[, 4]  # Extracting p-values\n\n# Define function to determine significance level\nget_significance_level &lt;- function(p_value) {\n  if (p_value &lt; 0.01) {\n    return('***')\n  } else if (p_value &lt; 0.05) {\n    return('**')\n  } else if (p_value &lt; 0.1) {\n    return('*')\n  } else {\n    return('')\n  }\n}\n\n# Get significance levels\nsignificance_levels &lt;- sapply(p_values, get_significance_level)\n\n# Create dataframe\ndf &lt;- data.frame(\n  Coefficient = coefficients,\n  Standard_error = round(standard_errors, 3),\n  Significance_level = significance_levels\n)\n\n# Print the dataframe\nprint(df)\n\n            Coefficient Standard_error Significance_level\n(Intercept)  6.45614093          0.090                ***\nmage        -0.04012754          0.002                ***\nmeduc       -0.28772570          0.005                ***\nmblack      -0.36100043          0.028                ***\nalcohol      1.96216177          0.062                ***\nfirst       -0.45972476          0.020                ***\ndiabete      0.22927944          0.067                ***\nanemia       0.33284394          0.081                ***\nbirthwgt    -0.00091614          0.000                ***\n\n\nThe covariates coefficients are strengths/weights of each variables that determines whether a unit will recieve the treatment. For example, the coefficient of mother’s age is 0.04, which means that for every unit increase in mother’s age, keeping all other covariates constant, the log odds of being in treatment group decreases by 0.02. This is statistically significant as evident by p value, which is much less than 0.05. All covariates in the model are significant which means they are all important in determining the treatment status for a unit X.\nAmong all covariates, alcohol has the greatest influence on whether a unit will recieve treatment, followed by first born child, which negative influence in being treatment group. These coefficients are all significant at 5% significance level.\nIn summary: what the above table is showing is that: all these covariates are responsible whether a unit falls into a treatment or control group. This is bad because an experiment should be completely random and should not influenced by anything. that means significance level for all those should have been more than 0.05.\nLook at the side by side histogram below. That is asymmetric. It should be made symmetric. and then I can run all analysis again .\n\n## use this logistic model to create a new column of prospensity scores for each observation\nsmoking_data$prospensity_scores &lt;- predict(mother_prospensityscore, type = \"response\")\n\n#round the scores to 2 decimal places\nsmoking_data$prospensity_scores &lt;- round(smoking_data$prospensity_scores, 2)\n\n## Histogram of PS before matching\nhistbackback(split(smoking_data$prospensity_scores, smoking_data$tobacco),\n             main= \"Propensity score before matching\",  \n             xlab=c(\"control\",  \"treatment\"))\n\n\n\n\nOverlap and its meaning Overlap refers to the degree of similarity or commonality between the treatment group (those who received the treatment being studied) and the control group (those who did not receive the treatment). The overlap is important because it is a necessary condition for the ignorability assumption to hold. If there is no overlap, then the treatment and control groups are so different that it is impossible to make causal inferences. The histogram above shows that there is a only some overlap between the treatment and control group, more individuals in the control group with lower propensity scores than in the treatment group. This is a violation of the Common Support assumption.\nBased on the analysis done so far. So, to resolve that I can select only those subjects that can create similarity in the samples.\nThat will involve following steps:\n-Calculate propensity score and match based on propensity values, then rerun all analysis as before. Propensity score calculates the probability that a unit will fall into treatment group.\n**Okay: lets begin the real analysis on unbiased data: The steps are:\n\nmatch the population and rerun ATE\nrerun average treatment effect on treated only\nperform more analysis matching samples by neighbour method, inverse weighted method\n\n\n\nSTEP 1. Matching the population with propensity score\nMatch treated/control mothers using your estimated propensity scores and nearest neighbor matching. Compare the balancing of pretreatment characteristics (covariates) between treated and non-treated units in the original dataset.\n\n## Nearest-neighbor Matching\nprospensity_score_matched &lt;- MatchIt::matchit(tobacco ~  mage +  meduc +\n                             mblack + alcohol + first + diabete + anemia + birthwgt, data = smoking_data, method = \"nearest\")\n\n## Covariate Imbalance post matching\nmatched_prospensity_dataset &lt;- match.data(prospensity_score_matched)\n\n# Drawing back to back histograms for propensity scores for treated and \n# non-treated after matching\nhistbackback(split(matched_prospensity_dataset$prospensity_scores,  matched_prospensity_dataset$tobacco),   main= \"Propensity\n        score   after   matching\",  xlab=c(\"control\",   \"treatment\"))\n\n\n\n\nPost matching, there is a better overlap between the treatment and control group. Units with high propensity score is matched with its counterparts and vice versa. This means the units we are comparing between the treatment and control group are similar. This helps us define counterfactuals of what would have happened to the treatment group if they were not treated.\n\n# the covariates between treated and non-treated that were used in the\n# estimation of the propensity scores\nxBalance(tobacco ~ mage +  meduc + mblack + alcohol + first + diabete + anemia + birthwgt, data = matched_prospensity_dataset,\n         report=c(\"std.diffs\",\"chisquare.test\", \"p.values\"))\n\n         strata():  unstrat    \n         stat      std.diff    \nvars                           \nmage                    0.0 ***\nmeduc                   0.0 ***\nmblack                  0.0 ** \nalcohol                 0.1 ***\nfirst                   0.0 ***\ndiabete                 0.0 .  \nanemia                  0.0    \nbirthwgt                0.0 *  \n---Overall Test---\n        chisquare df p.value\nunstrat       175  8   1e-33\n---\nSignif. codes:  0 '***' 0.001 '** ' 0.01 '*  ' 0.05 '.  ' 0.1 '   ' 1 \n\nxBalance(tobacco ~ mage +  meduc + mblack + alcohol + first + diabete + anemia + birthwgt, data = smoking_data,\n         report=c(\"std.diffs\",\"chisquare.test\", \"p.values\"))\n\n         strata():  unstrat     \n         stat      std.diff     \nvars                            \nmage                   -0.4 *** \nmeduc                  -0.6 *** \nmblack                  0.1 *** \nalcohol                 0.3 *** \nfirst                  -0.1 *** \ndiabete                 0.0     \nanemia                  0.1 *** \nbirthwgt               -0.5 *** \n---Overall Test---\n        chisquare df p.value\nunstrat     10298  8       0\n---\nSignif. codes:  0 '***' 0.001 '** ' 0.01 '*  ' 0.05 '.  ' 0.1 '   ' 1 \n\n\nAfter the matching, the nature and weight of the regression coefficients changed. Previously in unmatched data, I discussed that with increase age of mother, propensity score decreases. But post matching, the coefficients is showing that propensity score actually increases with increasing age of the mother. This is a sign that matching have accounted fixed effects in the observational dataset. Moreover, some covariates which were significant in determining the treatement status in the pre matching, are not significant anymore post matching. Like diabete, anemia, birthwgt, and mblack. Thus mother being black, having diabetes, and having anaemia, does not determine if she will receive the treatment or not.\nBut, this conclusion is not yet sufficient. What if I see this change only on treated population and not in control group ? The follwing step involves that:\n\n\nSTEP 2: Average treatment effect on treated group\nThis step is necessary because it can be more robust estimate. For example: this will compare the change before treatment and after treatment in the same units.\n\n## calculate ATE based on nearest neighbor matching\nsumdiff_data &lt;- matched_prospensity_dataset%&gt;%\n  group_by(subclass)%&gt;%\n  mutate(diff = birthwgt[tobacco==1]- birthwgt[tobacco==0])\n\ndif_in_treated &lt;- sum(sumdiff_data$diff)/2\n\nATT_weighted_count = 1/sum(matched_prospensity_dataset$tobacco) * dif_in_treated\nATT_weighted_count\n\n[1] -13.361\n\n\nFor the treated smoker mothers, the tobacco effects on their child birth weight is lower by 13 grams on average than for its counterfactual non smoking mothers. This means that the treatment has caused lower birth weight in the treated group of mothers. For any other units of population who shares similar covariates as treated group, the birth weight of their child will be lower by 13 grams on average if they were to start smoking tobacco.\nIs the conclusion final ? not yet. What if the propensity matching has drawbacks, which it does already. Propensity score matching is based on the fact that all covariates have similar influence in treatment. But, this does not always hold true. WLS matching fixes that.\n\n\nATE with WLS Matching\nThis step is necessary becuase the\n\n## Weighted least Squares (WLS) estimator Preparation\nsmoking_data &lt;- smoking_data %&gt;% \n  mutate(weights = tobacco / prospensity_scores + (1 - tobacco) / (1 - prospensity_scores))\n\n## Weighted least Squares (WLS) Estimates\nwls &lt;- lm(birthwgt ~ tobacco + mage +  meduc +\n                           mblack + alcohol + first + diabete + anemia, \n          data = smoking_data, weights = weights)\n\nsummary(wls)\n\n\nCall:\nlm(formula = birthwgt ~ tobacco + mage + meduc + mblack + alcohol + \n    first + diabete + anemia, data = smoking_data, weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n -7283   -374     32    396  12243 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3122.734     11.584  269.58  &lt; 2e-16 ***\ntobacco        1.426      3.241    0.44     0.66    \nmage           0.219      0.358    0.61     0.54    \nmeduc         23.497      0.881   26.69  &lt; 2e-16 ***\nmblack      -215.695      4.978  -43.33  &lt; 2e-16 ***\nalcohol     -174.499     13.803  -12.64  &lt; 2e-16 ***\nfirst        -65.326      3.509  -18.61  &lt; 2e-16 ***\ndiabete       72.223     12.213    5.91  3.4e-09 ***\nanemia       -23.746     16.766   -1.42     0.16    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 698 on 94163 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.0369,    Adjusted R-squared:  0.0368 \nF-statistic:  451 on 8 and 94163 DF,  p-value: &lt;2e-16\n\n## Present results\n\nThe WLS matching is weighted based on propensity scores, meaning unit with higher similarity in covariates gets more weights. Based on this matching, the average birth weight of children for non smoker mother or control group is 3122.7338, keeping all other variables unchanged. This is statistically significant at 5% signficance. Of all the covariates, mblack covariates has greatest influence in outcome. i.e If the women identifies as black, the birth weight of the child is lower by 241 grams, which is significant. Besides, other covariates like drinking alcohol, first born child, and diabetes in covariates also significantly influence birthweight. However, tobacco appears insignificant at 5 percent signficance level. The model only explains 3 percent of variation given by R squared in birth weight of children. This means that the model is not a good fit for the data.\nThere are certain factors that influences the output more than other covariates. This was also shown by the coefficients in Balance estimates in previous step. When using ATT with propensity score matching, the model assumed that all covariates has equal influence in determining the treatment status. But in reality, this is not always true. The WLS matching takes weights of each covariates into account, thus providing different output than ATT. if all the covariates had same weights, we would have got same estimate using both ATT and WLS matching.\nIs the conclusion final ?\n-Statistically YES !"
  },
  {
    "objectID": "featured_projects/data_viz/index.html",
    "href": "featured_projects/data_viz/index.html",
    "title": "Soccer players attributes comparison: Conclusion from Data visualization",
    "section": "",
    "text": "Introduction about the project\nThis project aims to create infographics comparing attributes of soccer players, sourced from a European soccer database available on Kaggle. The database contains data on over 10,000 players. Using R exclusively, the visualization focuses on attributes such as shooting, finishing, strength, age, etc. All tasks, from fetching the data to adding text and aligning elements, are performed using R packages. Special credit is given to the UFO alien template, accessible at UFO link\nImportant Note: The ggplot package version used in this project is 3.4.4, and ggtern version 3.0.0 is utilized. It’s crucial to consider that modern versions of ggplot might conflict when used in combination with other packages employed here.\n\n\nData Wrangling\nThe datasets available on Kaggle are stored in SQLite and require joins to build a complete dataset. However, for this project, data from a single table is sufficient. Nevertheless, for future use, datasets will be combined if necessary.\n\n#read the csv file\nplayer_attr &lt;- read_csv(here(\"featured_projects/data_viz/player_attr.csv\"))\n\nThe dataset includes a column named “overall ratings,” representing the FIFA average rating from 2008 to 2016. This column will be utilized to categorize players into Advanced, Intermediate, and Novice. It’s important to note that the term “Novice” is used for classification purposes and does not undermine any players; it’s simply a classification based on personal preference.\n\n#---classify the players based on their overall rating\nplayer_attr &lt;- player_attr %&gt;% \n  mutate(player_class = ifelse(overall_rating &gt;= 85, \"Advanced\", \n                                 ifelse(overall_rating &gt;= 70 & overall_rating &lt; 85, \"Intermediate\", \"Novice\")))\n\nThe project will compare the following attributes of players. The hypothesis is whether there exists a difference in attributes between the world’s renowned top performers and players who do not enjoy the same level of popularity.\n\n# filter required columns from the dataset\nplayer_data &lt;- player_attr %&gt;% \n        select(player_name, player_class, crossing, agility, \n               dribbling, finishing, aggression, balance, strength, stamina)\n\nFrom the database containing over 10,000 players, only a subset is required. A random sample of 1000 players will be drawn from the intermediate and novice categories, while all players from the advanced category will be included, given that there are already fewer than 1000 advanced category players.\n\n#---filter only good players from the player_data datase\ngood_players &lt;- player_data %&gt;% filter(player_class == \"Advanced\")\n\n#---filter average and bad players from the player_data dataset\nset.seed(123)\naverage_bad_players &lt;- player_data %&gt;% filter(player_class != \"Advanced\") %&gt;% \n  #sample only 1000 from average and bad players\n  group_by(player_class) %&gt;% \n  #randomly select 1000 players from each class and always include the player Van dijk in the sample\n  sample_n(1000)\n\n#good defender van dijk\nvan_dijk &lt;- player_data %&gt;% filter(player_name == \"Virgil van Dijk\")\n\n#---combine the good_players and average_bad_players datasets\nclean_player_data &lt;- bind_rows(good_players, average_bad_players, van_dijk)\n\n\n\nPLot1: Radar plot for common attributes comparison\n\n#summarize the data and plot the summary output with radar chart\nradar_data &lt;- clean_player_data %&gt;% \n  group_by(player_class) %&gt;% \n  select(-player_name,  player_class) %&gt;%\n  summarise_all(mean, na.rm = TRUE) %&gt;% \n  arrange(ifelse(player_class == \"Advanced\", 1, ifelse(player_class == \"Intermediate\", 2, 3))) %&gt;% \n  #arrange so that corssing, finishing, driblling and agility are away from each other\n  rename(\"finish\"= \"finishing\") %&gt;% \n  select(player_class, crossing, aggression, agility, balance, dribbling, stamina, finish, strength)\n\n#create custom color paletteA\ncolors &lt;- c(\"#7f58AF\", \"#64C5EB\", \"#E84D8A\", \"#FEB326\", \"lightblue\")\n\nradar_plot &lt;- ggradar::ggradar(radar_data,\n        grid.min = 0,\n        grid.max =  100,\n        grid.mid = 50,\n        axis.label.size = 22,\n        label.centre.y = FALSE,\n        group.line.width = 1,\n        gridline.mid.colour = \"#27593d\",\n        group.point.size  = 2,\n        grid.label.size = 20,\n        group.colours = c(\"#7f58AF\", \"#64C5EB\", \"#E84D8A\"),\n        background.circle.colour = \"white\",\n        legend.title  = \"Players proficiency\",\n        legend.text.size = 24,\n        font.radar = \"serif\") +\n  \n  #make the background theme white\n  theme_void() +\n  #remove y axis label\n  theme(axis.text.y = element_blank())+\n  #remove x axis label\n  theme(axis.text.x = element_blank())+\n  #remove all grid lines \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+\n  #remove legend\n  theme(legend.position = \"none\")\n\n#save this as png\nggsave(plot = radar_plot, filename = \"image/radar_plot.png\", height = 6, width = 6, scale = 1.1, dpi= 300)\n\n#Include the radar plot in the rmd\n#knitr::include_graphics(\"image/radar_plot.png\")\nknitr::include_graphics(\"static_image/radar.png\")\n\n\n\n\n\n\nPlot2: Stat-halfeye plot for age distribution\n\nlibrary(ggdist)\n##eye plot\n#plot lefeye plot using stat_halfeye\n\ndata &lt;-player_attr %&gt;% \n  mutate(birthday = as.Date(birthday)) %&gt;% \n  mutate(birthday = as.numeric(format(birthday, \"%Y\"))) %&gt;%  #calculate the age\n  mutate(age = 2016 - birthday)\n\n#plot the half eye\neye_plot &lt;- ggplot(data, aes(x = age, y = player_class,  fill = player_class))+\n  stat_halfeye(alpha = 0.7, color = \"black\") +\n  theme(panel.background = element_blank(),\n               panel.grid = element_blank(),\n               legend.position = \"none\",\n               plot.background = element_blank(),\n               panel.border = element_blank(),\n               axis.title.x = element_blank(),\n               axis.line.y = element_line())+\n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 60),\n        axis.title = element_text(size = 90),\n        text = element_text(family = \"serif\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  scale_fill_manual(values = colors) +\n  theme(axis.text.x = element_blank()) +\n  coord_flip() +\n  xlab(\"\") +\n  ylab(\"\")+\n  theme(panel.background = element_blank())\n  \n#save as ggplot\nggplot2::ggsave(plot = eye_plot, filename = \"image/eye_plot.png\", height = 3, width = 5, dpi = 450)\n\n#include the age plot in rmd\n#knitr::include_graphics(\"image/eye_plot.png\")\nknitr::include_graphics(\"static_image/eye_plot.png\")\n\n\n\n\n\n\nPlot3: Ternary plot for 3 major attributes comparison\n\n#standarize the clean_player_data for all numeric columns based on min and max value\nplayer_standarized &lt;- clean_player_data %&gt;% \n  mutate(across(where(is.numeric), ~scales::rescale(.x, to = c(0, 1))))\n\n# Plot the ternary plot for three attributes: strength, aggression, and balance\n#Plot the ternary plot for three attributes: strength, aggression, and balance\n\ntern_plot &lt;- ggtern::ggtern(player_standarized, aes(x = finishing, \n                                                    y = dribbling, \n                                                    z = strength, \n                                                    color = player_class)) +\n  geom_point(alpha = 0.3)+\n  theme_bw()+\n  scale_color_manual(values = colors)+\n  # Highlight all points with advanced players\n  geom_point(data = player_standarized %&gt;% \n      filter(player_class == \"Advanced\"), \n              aes(x = finishing, y = dribbling, z = agility))+\n  # Add labels for popular players of presnt data\n  geom_text(data = player_standarized %&gt;% \n            filter(player_name %in% c(\"Lionel Messi\", \"Cristiano Ronaldo\", \"Virgil van Dijk\")), \n              aes(label = player_name),alpha = 1, hjust = -0.1, \n              hjust = 0.5, size = 12, color = \"black\")+\n  # Color the points for Messi and Ronaldo\n  geom_point(data = player_standarized %&gt;% \n            filter(player_name %in% c(\"Lionel Messi\", \"Cristiano Ronaldo\", \"Virgil van Dijk\")), \n                   aes(x = finishing, y = dribbling, z = strength, fill = player_class), \n                   color = 'black', shape = 21, size = 3)+\n  #manual fill color for the highlighted players\n  scale_fill_manual(values = c(\"Advanced\" = \"#7f58AF\", \"Intermediate\" = \"#64C5EB\", \"Novice\" = \"#E84D8A\"))+\n  #remove the legend produced from scale fill manual\n  guides(fill = \"none\")+\n  #increase axis text size\n  theme(axis.text = element_text(size = 36))+\n  theme(axis.title = element_text(size = 52))+\n  theme(axis.title = element_text(family = \"serif\"))+\n  #remove the legend\n  theme(legend.position = \"none\")+\n  geom_confidence_tern(breaks = 0.95)+\n  theme(tern.axis.title.L = element_text(hjust = -.1, vjust = 2),\n        tern.axis.title.R = element_text(hjust = 1, vjust = 2))\n\n\n#save this as jpg using ggsave with name tern_plot\nggsave(plot = tern_plot, filename = \"image/tern_plot.png\", height = 6 , width = 6, dpi = 300)\n\n#Include the ternary plot in the rmd\n#knitr::include_graphics(\"image/tern_plot.png\")\nknitr::include_graphics(\"static_image/tern_plot.png\")\n\n\n\n\n\n\nInfographics:aesthetics buildup\nThis segment and beyond involves, creating text, plots, base plots, etc aesthetics for the final infographics. Most of these are derived from UFO plot, as mentioned in the introduction of this document.\n\n#specify text size and fonts\nalien &lt;- c('#47fcea', '#3c6478', '#548687', '#17bd52', '#679d76', '#3e6f50', '#27593d')\ntxt &lt;- alien[7]\nbg &lt;- 'black' # '#010101'\naccent &lt;- txt\n\nsysfonts::font_add(\"fa-brands\", regular = \"data/font/fa-brands-400.ttf\")\nfont_add(\"fa-solid\",  regular = \"data/font/fa-solid-900.ttf\")\nfont_add_google(\"Orbitron\", \"orb\")\nfont_add_google(\"Barlow\", \"bar\")\nshowtext_auto()\nft &lt;- \"orb\"\nft1 &lt;- \"bar\"\n\n# 🔡 text --------------------------------------------------------------------\n\nmastodon &lt;- glue(\"&lt;span style='font-family:fa-brands; color:{accent}'&gt;&#xf4f6;&lt;/span&gt;\")\ntwitter &lt;- glue(\"&lt;span style='font-family:fa-brands; color:{accent}'&gt;&#xf099;&lt;/span&gt;\")\ngithub &lt;- glue(\"&lt;span style='font-family:fa-brands; color:{accent}'&gt;&#xf09b;&lt;/span&gt;\")\nfloppy &lt;- glue(\"&lt;span style='font-family:fa-solid; color:{accent}'&gt;&#xf0c7;&lt;/span&gt;\")\nspace &lt;- glue(\"&lt;span style='color:{bg};font-size:1px'&gt;'&lt;/span&gt;\")\nspace2 &lt;- glue(\"&lt;span style='color:{bg}'&gt;--&lt;/span&gt;\")\ncaption &lt;- glue(\"{mastodon}{space2}@sujan@{space}sujandon.org{space2}{twitter}{space2}@sujan{space2}{github}{space2}sbgithubhm/tidytues{space2}{floppy}{space2}European players attributes comparison\")\n\n\n# Define subtitle with HTML formatting for colors\nsubtitle &lt;- \"&lt;span style='color:#7f58AF'&gt;Advanced players[&gt;85 ratings]&lt;/span&gt;, &lt;span style='color:#64C5EB'&gt;Intermediate players[70-85]&lt;/span&gt;, &lt;span style='color:#E84D8A'&gt;Novice players[&lt;70]&lt;/span&gt;\"\n\n#---------------copy of UFO plot\ng_base &lt;- ggplot() +\n  labs(\n    title = \"European Soccer Players \\n attributes comparison based \\n on Fifa ratings, 2016\",\n    subtitle = subtitle,\n    caption = caption\n    ) +\n  theme_void() +\n  theme(\n    text = element_text(family = ft, size = 36, lineheight = 0.3, colour = txt),\n    plot.background = element_rect(fill = \"white\", colour = bg),\n    plot.title = element_text(size = 128, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.subtitle = element_text(family = ft1, hjust = 0.5, margin = margin(b = 20), color = \"#27593d\", size = 60),\n    plot.caption = element_markdown(family = ft1, colour = colorspace::darken(txt, 0.5), hjust = 0.5,\n                                    margin = margin(t = 20)),\n    plot.margin = margin(b = 20, t = 50, r = 50, l = 50),\n    axis.text.x = element_text())+\n    theme(\n    plot.subtitle = element_markdown()\n    )\n# # quote 1 for the distribution\n\nquote1 &lt;- ggplot() +\n  annotate(\"text\", x = 0, y = 1, label = \"The 'half-eye' plot in the left, illustrates the age \\n distribution of players from the 2016 European soccer \\n database. Advanced players exhibit a relatively \\n concentrated age range, with the lower limit set at \\n 24 years. On the other hand, intermediate and novice  \\n players display flatter distributions. Novice  \\n players, in particular, show a broad age  \\n range, evenly spread from 17 to 45 years. Despite \\n these variations the median age remains similar \\n across all categories\",\n           family = ft1, colour = 'black', size = 16, hjust = 0, fontface = \"italic\", lineheight = 0.4) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  theme_void() +\n  coord_cartesian(clip = \"off\")\n\n\nquote2 &lt;- ggplot() +\n  annotate(\"text\", x = 0, y = 1, label =\" In comparing overall playing attributes, advanced players \\n show slightly higher finish (ability to score goal),\\n crossing (skill to pass the ball to team), \\n and dribbling (ability to pass through opponent).\\n Aggression, strenth, and balance are similar \\n among all players. However, the difference is subtle,\\n off by only few percentage. This indicate difference \\n           in                    attributes is not huge between \\n players                categories\",\n           family = ft1, colour = 'black', size = 16, hjust = 0, fontface = \"italic\", lineheight = 0.4) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  theme_void() +\n  coord_cartesian(clip = \"off\")\n\n#quote 3\nquote3 &lt;- ggplot() +\n  annotate(\"text\", x = 0, y = 1, label = \"The comparison of three most critical attributes \\n     finishing, strength, and dribbling—a high \\n         similarity is observed between intermediate and \\n            novice players. However, advanced players exhibit \\n                a narrower overlap. This suggests distinctions \\n                      between players of varying FIFA ratings are subtle.\",\n                    family = ft1, colour = 'black', size = 16, hjust = 0, fontface = \"italic\", lineheight = 0.4) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  theme_void() +\n  coord_cartesian(clip = \"off\")\n                                                  \nquote4 &lt;- ggplot()+\nannotate(\"text\", x= 0, y = 1, label = \"Lionel Messi, the world's best player,\\n     does not stand out, other players demonstrate \\n          similar capabilities. Given that the outcome in \\n                sports game relies heavily on overall team \\n                    performance, individual attributes \\n                         measures can be wrong metrics for\\n                               game winning\",\n  \n           family = ft1, colour = \"black\", size = 16, hjust = 0, fontface = \"italic\", lineheight = 0.4) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  theme_void() +\n  coord_cartesian(clip = \"off\")\n\nImages cannot be loaded into ggplot object directly. it should be convereted to raster object and then it can be overlaid over other ggplot object. The following codes achieves the same thing for different images that are used in the final infographics.\n\n#load the tern image\nimage &lt;- readPNG(\"image/tern_plot.png\")\ntern_image &lt;- as.raster(image)\n\n#radar plot\nradar &lt;- readPNG(\"image/radar_plot.png\")\nradar_image &lt;- as.raster(radar)\n\n#tern plot image\nimage &lt;- readPNG(\"image/tern_plot.png\")\nimage &lt;- as.raster(image)\n\n#lionel messi image\nmessi &lt;- readJPEG(\"data/images/messi.jpg\")\nmessi &lt;- as.raster(messi)\n\n#fifa logo image\nfifa &lt;- readPNG(\"data/images/FIFA.png\")\nfifa &lt;- as.raster(fifa)\n\n#goalpost as image\nsoccer_field &lt;- readJPEG(\"data/images/soccer-field.jpg\")\nsoccer_field &lt;- as.raster(soccer_field)\n\nThe final graph is produced from the following line. This chunk adds all previously made plots and inset them into appropriate locations, based on preference. This process takes really long time, as finding the exact location from just point values is significantly tedious task. I would suggest using any other graphical interface software to add the quotes and paragraphs, which takes way less time.\n\n# Combine the plots into a single infographic\ng_final &lt;- g_base +\n  inset_element(fifa, left = 0.9, right = 1.08, top = 0.25, bottom = -0.2)+\n  inset_element(eye_plot, left = -0.12, right = 0.57, top = 1, bottom = 0.66) +\n  #insert radar plot\n  inset_element(tern_image, left = -0.10, right = 0.80, top = 0.5, bottom = -.1) +\n  inset_element(messi, left = -0.08, right = 0.15, top = 0.50, bottom = 0.15) +\n  inset_element(radar_plot, left = 0.4, right = 1.1, top = 0.81, bottom = 0.31) +\n  #insert quote 1\n  inset_element(quote1, left = 0.5, right = 1, top = 0.88, bottom = 0.72) +\n  #Insert quote 2\n  inset_element(quote2, left = -0.07, right = 0.5, top = 0.55, bottom = 0.5) +\n  #insert quote 3\n  inset_element(quote3, left = 0.41, right = 1, top = 0.31, bottom = 0) +\n  #insert quote 4\n  inset_element(quote4, left = 0.54, right = 1, top = 0.13, bottom = -0.10)+\n  plot_annotation(theme = theme(\n      plot.background = element_rect(fill = \"white\", colour = 'white')))\n\nggsave(plot = g_final, filename = \"infographics_draft.png\", height = 16, width = 10)\n\n\n\n\n\n\n\n\n\n\nFor further information and the Initial motivation behind project, please follow:\nIn all sports domain, some players always stands out in popularity. For example, Lebron James in Basketball, Khabib Nurmagomedov in UFC, Serena Williams in Tennis, and Lionel Messi and Cristiano Ronaldo in Soccer. But why do some players always stand out ? Does it relates to their ability to perform better than other players ? This is the central question of this project, which will explore how attributes between advanced and novice category differs.\nThe project utilizes the soccer dataset from European soccer database, which has data about more than 10000 players and currently open source at Kaggle website. The dataset contains all information about every games played, home goals wins for teams, aways goals wins or losses, individual players attributes like shooting, strength, skill, balance, kicking, crossing, volleying and all other attributes that are described in soccer playing. This project only utilizes the player attributes data and not the team data, and explores the players attributes. The players will first be categorized into advanced, intermediate, and novice based on fifa ranking. Fifa ranking is available for each of those players, which is a average ratings of the overall performance of the player since 2008 season to 2016. All players with average ratings higher than 85 are classified as advanced player, 70-85 as intermediate, and &lt;70 as novice players.\nFor the infographics design, all information and codes help is archived from UFO infographics template. All designed are done only inside R and no other graphical editing softwares will be used. Three of the graphics: eyeplot, radar plot, and tern plot is used for attributes comparison. The base template will be created with ggplot which will have all other plots overlaid over it. The title and subtitle will be highlighted at the top. The subtitle will explains players and their category. For example: all advanced players in the infographics is represented as purple color, and that thing is shown from how subtitle is highlighted. The colors are chosen neutral in appearance with excellent contrast for quick differentiation. These colors are supposed to be colorblind friendly. The text sizes, themes, background, and spacing are all first defined based on the base template of the infographics. To avoid information overloading, none of the legend and axis are kept, since quick explanation at the paragraph should be sufficient. Some top performers like Lionel messi, Cristiano Ronaldo, and Virgil van Djik, who are some of the most popular players, are highlighted and annotated to contextualize the data. This should help where the worlds famous players stand. All my plots also centers the message about attributes comparison among different category soccer players. For example, the ternary plot will have the circled grouped data based on 95% confidence interval for all categories. This should help where categories overlap. This approach was able to show the high overlapped categories.\n\nThe DEI approach is not satisfied for this plot, because in order to understand what player attributes are, an audience should be knowledged about the soccer games. So, this infographics is designed only for soccer fans, and none else. This data-viz project concludes that the difference between advanced and novice category is subtle."
  },
  {
    "objectID": "featured_projects/ml_spotify/index.html",
    "href": "featured_projects/ml_spotify/index.html",
    "title": "Comparing competing Machine Learning models in classifying Spotify songs",
    "section": "",
    "text": "In this project, I explored four popular machine learning models (K-Nearest Neighbors, Decision Tree, Bagged Tree, and Random Forest) to classify Spotify songs into two genres: Underground Rap and Drum and Bass (dnb). I used a dataset from the Spotify API, containing 18 audio features of each song. The goal was to determine the best model for this classification task based on accuracy, sensitivity, and specificity.\n\n\nModel Comparison\n\nImplemented and tuned each model using grid search and cross-validation\nEvaluated model performance using accuracy, sensitivity, specificity, and F1 score\nVisualized model performance using confusion matrices and ROC-AUC curves\n\n\n\nResults\n\nRandom Forest model achieved the highest accuracy (92.1%) and F1 score (0.921)\nBagged Tree model showed the highest sensitivity (0.943)\nDecision Tree model had the highest specificity (0.933)\n\n\n#load required libaries\nlibrary(spotifyr) #API interaction\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(caret)\n\n\nData Preparation\n\n# read the online spotify data\ngenre &lt;- read_csv(\"genres_v2.csv\")\nplaylist &lt;- read_csv(\"playlists.csv\")\n\n\n#jon the data with id from genre and playlist\nspotify_data &lt;- genre %&gt;% \n                left_join(playlist, by =c(\"id\"= \"Playlist\"))\n\n# filter the data to include only the tracks of the two genres you'd like to use for the classification task\nspotify_data &lt;- spotify_data %&gt;% \n                filter(genre %in% c(\"Underground Rap\", \"dnb\")) %&gt;% \n                #rename underground rap to rap\n                mutate(genre = ifelse(genre == \"Underground Rap\", \"URap\", genre))\n\n\n\nData Exploration\n\n# remove columns that you won't be feeding model\nspotify &lt;- spotify_data  %&gt;%  select(song_name, genre, danceability: tempo) %&gt;% \n            #change genre to factor\n            mutate(genre = as.factor(genre))\n\n#find the top 10 most danaceable tracks in the dataset\ntop_20_songs &lt;- spotify %&gt;% \n     arrange(desc(danceability)) %&gt;% \n     head(20) %&gt;% \n     select(song_name, genre, danceability)\n\n# output table with interactivity features\nkableExtra:: kable(top_20_songs,\n                   caption = \"Top 20 most danceable tracks in the dataset\")\n\n\nTop 20 most danceable tracks in the dataset\n\n\nsong_name\ngenre\ndanceability\n\n\n\n\nPOP, LOCK & DROPDEAD\nURap\n0.985\n\n\nLoyaltyRunsDeepInDaLongRun\nURap\n0.985\n\n\nTwo Left Feet Flow\nURap\n0.984\n\n\nHate Your Guts\nURap\n0.983\n\n\nMugen Woe\nURap\n0.982\n\n\nThe 3\nURap\n0.980\n\n\nMavericks\nURap\n0.979\n\n\nTechnicolor\nURap\n0.977\n\n\nKillmonger\nURap\n0.977\n\n\nFunky Friday\nURap\n0.975\n\n\nNervous\nURap\n0.975\n\n\nGo to the Sto\nURap\n0.975\n\n\nBad Bad Bad (feat. Lil Baby)\nURap\n0.974\n\n\nAbraham Lincoln\nURap\n0.974\n\n\nPsycho Pass\nURap\n0.973\n\n\nWho the Fuck Is You\nURap\n0.972\n\n\nDottin Up\nURap\n0.971\n\n\nWorst Day of My Life\nURap\n0.971\n\n\nExcalibur\nURap\n0.970\n\n\nSexy\nURap\n0.970\n\n\n\n\n\n\n#difference between genres for some of the audio features\n#---drop song name, not required for models\nspotify &lt;- spotify %&gt;%  select(-song_name)\n\n#setup manual colors\ncolors &lt;- c(\"#7f58AF\", \"#64C5EB\", \"#E84D8A\", \"#FEB326\", \"lightblue\")\n\n#plot the data\nspotify %&gt;% \n  group_by(genre) %&gt;% \n  summarise(across(danceability:tempo, mean)) %&gt;% \n  pivot_longer(cols = danceability:tempo, names_to = \"audio_feature\", values_to = \"mean\") %&gt;% \n  ggplot(aes(x = genre, y = mean, fill = genre)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~audio_feature, scales = \"free\") +\n  theme_classic() +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = colors)+\n  #add caption\n  labs(caption = paste0(title = \"Source: Spotify API\")) + \n  #add title at the top center \n  ggtitle(\"Difference between genres for some of the audio features\")\n\n\n\n\n\n\n\n1.K-nearest neighbor\nThe k-nearest neighbors (KNN) method is a non-parametric classification algorithm used for recommendation systems. In the context of a Spotify dataset, the KNN algorithm can be employed to recommend songs or artists to a user based on the preferences of other users with similar listening histories. The algorithm calculates the distances between the target user and other users in the dataset based on their features, such as the artists or genres they have listened to. It then selects the k nearest neighbors, which are the users with the shortest distances to the target user, and recommends songs or artists that are popular among these neighbors.\n\n#load the libraries specific for this model\nlibrary(dplyr)    \nlibrary(ggplot2) #great plots\nlibrary(rsample)  #data splitting \nlibrary(recipes) #data preprocessing\nlibrary(skimr) #data exploration\nlibrary(tidymodels) #re-entering tidymodel mode\nlibrary(kknn) #knn modeling\nlibrary(caTools) #for splitting the data\n\n\n#split the data into training and testing set\nset.seed(123)\nsplit = sample.split(spotify$genre, SplitRatio = 0.7)\nspotify_split = initial_split(spotify)\ntrain = subset(spotify, split == TRUE)\ntest =  subset(spotify, split == FALSE)\n\n\n#specify the recipe\nknn_rec &lt;- recipe(genre ~., data = train) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\n## knn spec\nknn_spec &lt;- nearest_neighbor(neighbors = 5) %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\n#bake the data\n#baked_train &lt;- bake(knn_rec, train)\n\n\n#apply to testing set\n#baked_test &lt;- bake(knn_rec, test)\n\nTrain the model with 5 folds validation . The model is then tuned with grid search to find the best value of k. The model is then fit to the testing set and the performance is evaluated with confusion matrix, precision, recall and f1 score.\n\n# cross validation on the dataset\ncv_folds &lt;- vfold_cv(train, v = 5)\n\n# put all together in workflow\nknn_workflow &lt;- workflow() %&gt;%\n                add_recipe(knn_rec) %&gt;% \n                add_model(knn_spec)\n                \n\n#fit the resamples and carry out validation\nknn_res &lt;- knn_workflow %&gt;%\n           fit_resamples(resamples = cv_folds, \n           control = control_resamples(save_pred = TRUE))\n\n# find the best value of k\nknn_spec_tune &lt;- nearest_neighbor(neighbors = tune()) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"kknn\")\n\n# define a new workflow\nwf_knn_tune  &lt;- workflow() %&gt;%\n                add_model(knn_spec_tune) %&gt;% \n                add_recipe(knn_rec)\n  \n# tune the best model with grid search\nfit_knn_cv &lt;-  wf_knn_tune %&gt;%\n  tune_grid(cv_folds, grid = data.frame(neighbors = c(1, 5, seq(10, 100, 10))))\n\n#plot the fit knn cv with changing value of neighbours\nfit_knn_cv %&gt;% collect_metrics() %&gt;% \n  filter(.metric == \"roc_auc\") %&gt;% \n  ggplot(aes(x = neighbors, y = mean)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Accuracy of KNN model with different values of k\",\n       x = \"Number of neighbors\",\n       y = \"Accuracy\") +\n  theme_minimal()\n\n\n\n# check the performance with collect_metrics\nfit_knn_cv %&gt;% collect_metrics()\n\n# A tibble: 36 × 7\n   neighbors .metric     .estimator   mean     n  std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                \n 1         1 accuracy    binary     0.965      5 0.00170  Preprocessor1_Model01\n 2         1 brier_class binary     0.0351     5 0.00170  Preprocessor1_Model01\n 3         1 roc_auc     binary     0.965      5 0.00309  Preprocessor1_Model01\n 4         5 accuracy    binary     0.968      5 0.00176  Preprocessor1_Model02\n 5         5 brier_class binary     0.0258     5 0.00118  Preprocessor1_Model02\n 6         5 roc_auc     binary     0.989      5 0.00111  Preprocessor1_Model02\n 7        10 accuracy    binary     0.968      5 0.000903 Preprocessor1_Model03\n 8        10 brier_class binary     0.0247     5 0.000804 Preprocessor1_Model03\n 9        10 roc_auc     binary     0.993      5 0.000952 Preprocessor1_Model03\n10        20 accuracy    binary     0.967      5 0.00105  Preprocessor1_Model04\n# ℹ 26 more rows\n\n# create a final workflow\nfinal_wf &lt;- wf_knn_tune %&gt;%\n  finalize_workflow(select_best(fit_knn_cv, metric= \"accuracy\"))\n\n# fit the final model\nfinal_fit &lt;- final_wf %&gt;% fit(data = train)\n\n#predict to testing set\nspotify_pred &lt;- final_fit %&gt;% predict(new_data = test)\n\n# Write over 'final_fit' with this last_fit() approach\nfinal_fit &lt;- final_wf %&gt;% last_fit(spotify_split)\n\n# Collect metrics on the test data!\n\n\nfinal_fit %&gt;% collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.968  Preprocessor1_Model1\n2 roc_auc     binary        0.992  Preprocessor1_Model1\n3 brier_class binary        0.0237 Preprocessor1_Model1\n\n#bind genre from test data to the predicted data\nbind_test_pred &lt;- spotify_pred %&gt;% bind_cols(test)\n\n#plot the confusion matrix\nbind_test_pred %&gt;%\n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()+\n  #remove legend\n  theme(legend.position = \"none\")\n\n\n\n#calculate precision, recall and f1 score\nknn_estimate &lt;- bind_test_pred %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to knn estimate\n                rename(\"knn estimate\" = .estimate)\n\n\n\n2.Decision tree\nDecision trees uses CART algorithm to split the data into two or more homogeneous sets. The algorithm uses the Gini index to create the splits. The model is then tuned with grid search to find the best hyperparameters. The model is then fit to the testing set and the performance is evaluated with confusion matrix, precision, recall and f1 score, as for knn model.\n\n# load the packages for decision trees\nlibrary(MASS)\nlibrary(doParallel)\nlibrary(vip)\n\n# methods using in class\ngenre_split &lt;-  initial_split(spotify)\ngenre_train &lt;-  training(genre_split)\ngenre_test  &lt;-  testing(genre_split)\n\n\n##Preprocess the data\ngenre_rec &lt;- recipe(genre ~., data = genre_train) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\n#new spec, tell the model that we are tuning hypermeter\ntree_spec_tune &lt;- decision_tree(\n  cost_complexity = tune(),\n  tree_depth = tune(),\n  min_n = tune()) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nUse grid search to find the best hyperparameters and train on folded training set.\n\n# grid search the hyperparameters tuning \ntree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)\n\n#set up k-fold cv and use the model on this data\ngenre_cv = genre_train %&gt;% vfold_cv(v = 10)\ndoParallel::registerDoParallel() #build trees in parallel\n\n# setup the grid search\ntree_rs &lt;- tune_grid(tree_spec_tune, genre ~., \n                     resamples = genre_cv,\n                     grid = tree_grid,\n                     metrics = metric_set(accuracy))\n\nUse the workflow to finalize the model and also fit on the training set, and predicting on the test set.\n\n#finalize the model that is cross validate and hyperparameter is tuned\nfinal_tree &lt;- finalize_model(tree_spec_tune, select_best(tree_rs))\n\n#similar functions here.\nfinal_tree_fit &lt;- fit(final_tree, genre~., data = genre_train)\n\n#last_fit() fits on training data (like fit()), but then also evaluates on the test data.\nfinal_tree_result &lt;- last_fit(final_tree, genre~., genre_split)\nfinal_tree_result$.predictions\n\n[[1]]\n# A tibble: 2,211 × 6\n   .pred_dnb .pred_URap  .row .pred_class genre .config             \n       &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt; &lt;chr&gt;               \n 1   0            1         2 URap        URap  Preprocessor1_Model1\n 2   0            1         3 URap        URap  Preprocessor1_Model1\n 3   0            1         4 URap        URap  Preprocessor1_Model1\n 4   0.00157      0.998     6 URap        URap  Preprocessor1_Model1\n 5   0            1         7 URap        URap  Preprocessor1_Model1\n 6   0.00157      0.998     8 URap        URap  Preprocessor1_Model1\n 7   0            1        14 URap        URap  Preprocessor1_Model1\n 8   0.00157      0.998    16 URap        URap  Preprocessor1_Model1\n 9   0.00157      0.998    17 URap        URap  Preprocessor1_Model1\n10   0.00157      0.998    20 URap        URap  Preprocessor1_Model1\n# ℹ 2,201 more rows\n\n\n\n#visualize the model\nfinal_tree_fit %&gt;%\n  vip(geom = \"col\", aesthetics = list(fill = \"midnightblue\", alpha = 0.8)) +\n  scale_y_continuous(expand = c(0,0))\n\n\n\n# how accurate is this model on the test data?\nfinal_tree_result %&gt;% collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.983  Preprocessor1_Model1\n2 roc_auc     binary        0.982  Preprocessor1_Model1\n3 brier_class binary        0.0166 Preprocessor1_Model1\n\n#plot the confusion matrix\nfinal_tree_result %&gt;% \n  collect_predictions() %&gt;% \n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()+\n  theme(legend.position = \"none\")\n\n\n\n#plot precision, recall and f1 score\ndt_estimate &lt;- final_tree_result %&gt;% \n                collect_predictions() %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to deicsion tree estimate\n                rename(\"decision tree estimate\" = .estimate)\n\n\n\n3.Bagged tree\nBagging is a method of ensemble learning that combines the predictions of multiple models to improve the overall performance. It is different from two previous model in the sense that it is collection of ML models. It works by training multiple models on different subsets of the training data and then combining their predictions to make a final prediction. The bagged tree uses decision trees as the base model. The model is then tuned with grid search to find the best hyperparameters. The model is then fit to the testing set and the performance is evaluated with confusion matrix, precision, recall and f1 score, as for knn model and the decision tree model.\n\n# Helper packages\nlibrary(doParallel)  # for parallel backend to foreach\nlibrary(foreach)     # for parallel processing with for loops\nlibrary(caret)       # for general model fitting\nlibrary(rpart)       # for fitting decision trees\nlibrary(ipred)       # for bagging\nlibrary(baguette)    # for bagging\nlibrary(tidymodels)  # Assuming you have tidymodels installed\n\n# Methods using in class\ngenre_split &lt;- initial_split(spotify)\ngenre_train &lt;- training(genre_split)\ngenre_test  &lt;- testing(genre_split)\n\n## Preprocess the data\ngenre_rec &lt;- recipe(genre ~ ., data = genre_train) %&gt;%\n         step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %&gt;%\n         step_normalize(all_numeric(), -all_outcomes(),)\n\n# instatntiate bag model\nbag_model &lt;- bag_tree() %&gt;%\n     set_engine(\"rpart\", times = 100) %&gt;%\n     set_mode(\"classification\")\n\n# create a workflow\nbag_workflow &lt;- workflow() %&gt;%\n    add_model(bag_model) %&gt;%\n    add_recipe(genre_rec)\n\n\n##folds the data for validation set\ngenre_cv = genre_train %&gt;% vfold_cv(v = 10)\n\n# use tune grid to tune the model\nbag_tune &lt;- tune_grid(bag_workflow,\n                      resamples = genre_cv,\n                      grid = 10)\n\n# collect metrices\nbag_tune %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric     .estimator    mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.991      10 0.00159  Preprocessor1_Model1\n2 brier_class binary     0.00833    10 0.00112  Preprocessor1_Model1\n3 roc_auc     binary     0.998      10 0.000355 Preprocessor1_Model1\n\n\n\n#finalize the workflow\nbag_best = show_best(bag_tune, n = 1,  metric = \"roc_auc\")\n\n#fit the model\nfinalize_bag &lt;- finalize_workflow(bag_workflow, select_best(bag_tune, metric = \"roc_auc\" ))\n\n# fit the finalized model \nbag_fit &lt;- finalize_bag %&gt;% fit(genre_train)\n\n# predict the model on test\nbag_pred &lt;- bag_fit %&gt;% predict(genre_test) %&gt;% \n            bind_cols(genre_test)\n\n#predict the model with probaility values\nbag_pred_prob &lt;- bag_fit %&gt;% predict(genre_test, type = \"prob\") %&gt;% \n            bind_cols(genre_test)\n\n#model metrics and evaluation\naccuracy(bag_pred, truth = genre, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.992\n\n#visualize the model\nbag_pred %&gt;%\n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()\n\n\n\n  #remove legend\n\n\ntheme(legend.position = \"none\")\n\nList of 1\n $ legend.position: chr \"none\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n#plot precision, recall and f1 score\nbag_estimate &lt;- bag_pred %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to bagged tree estimate\n                rename(\"bagged tree estimate\" = .estimate)\n\n\n\n4.Random Forest\nRandom forest is a popular ensemble learning method that combines the predictions of multiple decision trees to improve the overall performance. It is different from bagged tree in the sense that it uses a collection of decision trees as the base model, stochastically chosing number of columns too in training. That is not seen in general bagging model. The model is then tuned with grid search to find the best hyperparameters.\n\n# random forest with R\nlibrary(ranger) # for random forest\n\n# methods using in class\ngenre_split &lt;- initial_split(spotify)\ngenre_train &lt;- training(genre_split)\ngenre_test &lt;-  testing(genre_split)\n\n# fold the data for validation set\ncv_folds = vfold_cv(genre_train, v = 5)\n\n# create a previous recipe\ngenre_recipe &lt;- recipe(genre ~., data = genre_train) %&gt;%\n          step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %&gt;%\n          step_normalize(all_numeric(), -all_outcomes())\n\n# instantiate model\nrf_model &lt;- rand_forest(mtry = tune(), trees = tune()) %&gt;%\n         set_engine(\"ranger\") %&gt;%\n         set_mode(\"classification\")\n\n# create a workflow\nrf_workflow &lt;- workflow() %&gt;%\n         add_model(rf_model) %&gt;%\n         add_recipe(genre_recipe)\n\n# use tune grid to tune the model\nrf_tune &lt;- tune_grid(\n        rf_workflow,\n        resamples = cv_folds,\n        grid = 10)\n\n# collect metrices\nrf_tune %&gt;% collect_metrics()\n\n# A tibble: 30 × 8\n    mtry trees .metric     .estimator    mean     n   std_err .config           \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;             \n 1    10  1748 accuracy    binary     0.990       5 0.00121   Preprocessor1_Mod…\n 2    10  1748 brier_class binary     0.00755     5 0.000711  Preprocessor1_Mod…\n 3    10  1748 roc_auc     binary     0.999       5 0.000150  Preprocessor1_Mod…\n 4     7    69 accuracy    binary     0.993       5 0.00103   Preprocessor1_Mod…\n 5     7    69 brier_class binary     0.00666     5 0.000590  Preprocessor1_Mod…\n 6     7    69 roc_auc     binary     1.00        5 0.000167  Preprocessor1_Mod…\n 7     5  1242 accuracy    binary     0.993       5 0.000942  Preprocessor1_Mod…\n 8     5  1242 brier_class binary     0.00664     5 0.000575  Preprocessor1_Mod…\n 9     5  1242 roc_auc     binary     1.00        5 0.0000860 Preprocessor1_Mod…\n10     6   464 accuracy    binary     0.993       5 0.000840  Preprocessor1_Mod…\n# ℹ 20 more rows\n\n\n\n#finalize workflow\nrf_best = show_best(rf_tune, n = 1,  metric = \"roc_auc\")\n\n# finalize the model \nfinal_rand_forest &lt;- finalize_workflow(rf_workflow, select_best(rf_tune, metric = \"roc_auc\" ))\n\n#use this to predict the on test set\nfinal_rf_fit &lt;- final_rand_forest %&gt;% fit(genre_train)\n  \n# predict the model on test\nfinal_rf_pred &lt;- final_rf_fit %&gt;% predict(genre_test) %&gt;% \n            bind_cols(genre_test)\n\n#model metrics and evaluation\naccuracy(final_rf_pred, truth = genre, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.993\n\n#visualize the model\nfinal_rf_pred %&gt;%\n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()+\n  #remove legend\n  theme(legend.position = \"none\")\n\n\n\n#plot precision, recall and f1 score\nrf_estimate &lt;- final_rf_pred %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to random forest estimate\n                rename(\"random forest estimate\" = .estimate)\n\n\n\nModel Comparison\n\n#Compare the accuracy of all models and create a dataframe\n\n#bind all the estimates using left join\nestimate_table &lt;- knn_estimate %&gt;% \n                left_join(dt_estimate) %&gt;% \n                left_join(bag_estimate) %&gt;% \n                left_join(rf_estimate)\n\n\nestimate_table\n\n# A tibble: 4 × 6\n  .metric  .estimator `knn estimate` `decision tree estimate`\n  &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;                    &lt;dbl&gt;\n1 accuracy binary              0.970                    0.983\n2 kap      binary              0.933                    0.963\n3 sens     binary              0.970                    0.976\n4 spec     binary              0.970                    0.987\n# ℹ 2 more variables: `bagged tree estimate` &lt;dbl&gt;,\n#   `random forest estimate` &lt;dbl&gt;\n\n\n\n## plot the histogram of the accuracy of all models\nestimate_table %&gt;% \n  pivot_longer(cols = c(\"knn estimate\", \"decision tree estimate\", \"bagged tree estimate\", \"random forest estimate\"), names_to = \"model\", values_to = \"accuracy\") %&gt;% \n  ggplot(aes(x = model, y = accuracy, fill = model)) +\n  geom_col(position = \"dodge\") +\n  theme_minimal() +\n  xlab(\"\") +\n  ylab(\"accuracy\") +\n  theme(legend.position = \"none\") +\n  labs(caption = paste0(title = \"Source: Spotify API\")) + \n  ggtitle(\"Accuracy of all models\") + \n  #transform y axis to percentage multiplying by 100\n  scale_y_continuous(labels = scales::percent)+\n  #fill manual colors\n  scale_fill_manual(values = colors)\n\n\n\n\n\n\nConclusion\nThis project demonstrated the effectiveness of ensemble learning methods (Bagged Tree and Random Forest) in classifying Spotify songs into two genres. The results can be used to inform music recommendation systems or genre classification tasks. I showcased my skills in data preprocessing, model implementation, hyperparameter tuning, and performance evaluation."
  },
  {
    "objectID": "featured_projects.html",
    "href": "featured_projects.html",
    "title": "Data science projects",
    "section": "",
    "text": "which ML algorithm should the organization use for higher accuracy?\n\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\n  \n\n\n\n\nDid Smoking Mothers Have Lowered Weight Children?\n\n\n\n\n\n\n\nStatistics\n\n\nEconometrics\n\n\nCausal Inference\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\n  \n\n\n\n\nIdentifying the most common word in articles on climate science\n\n\n\n\n\n\n\nNLP\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\n  \n\n\n\n\nSoccer players attributes comparison: Conclusion from Data visualization\n\n\n\n\n\n\n\nData Viz\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nSujan Bhattarai\n\n\n\n\n\n\n  \n\n\n\n\nComparing competing Machine Learning models in classifying Spotify songs\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nSujan Bhattarai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/energy/index.html",
    "href": "posts/energy/index.html",
    "title": "Predicting CO2 Emission in Nepal: An In-Depth Analysis",
    "section": "",
    "text": "Sustainable energy is a critical component in addressing global challenges such as climate change and achieving sustainable development goals. In this data analysis, we delve into the global dataset on sustainable energy spanning from 2000 to 2020, with a specific focus on Nepal. Our aim is to understand energy trends, explore potential use cases, and employ predictive modeling to forecast CO2 emissions in Nepal."
  },
  {
    "objectID": "posts/energy/index.html#data-source",
    "href": "posts/energy/index.html#data-source",
    "title": "Predicting CO2 Emission in Nepal: An In-Depth Analysis",
    "section": "Data Source",
    "text": "Data Source\nThe dataset used in this analysis is sourced from Kaggle. You can find the dataset here\n\n##load all required libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(rnaturalearth)\nlibrary(sf)\nlibrary(kableExtra)\n\n\nData Cleaning\nData cleaning is an essential step in data analysis. We begin by reading the “energy_data.csv” file and calculating the total count of missing values for each column in the dataset. We then check for and report any duplicated rows in the dataset. The data types of each column are inspected to ensure consistency.\n\n##read the energy data\nenergy_data &lt;- read_csv(\"energy_data.csv\")\ndata.frame(total_NA_count = colSums(is.na(energy_data)))\n##                                                                  total_NA_count\n## Entity                                                                        0\n## Year                                                                          0\n## Access to electricity (% of population)                                      10\n## Access to clean fuels for cooking                                           169\n## Renewable-electricity-generating-capacity-per-capita                        931\n## Financial flows to developing countries (US $)                             2089\n## Renewable energy share in the total final energy consumption (%)            194\n## Electricity from fossil fuels (TWh)                                          21\n## Electricity from nuclear (TWh)                                              126\n## Electricity from renewables (TWh)                                            21\n## Low-carbon electricity (% electricity)                                       42\n## Primary energy consumption per capita (kWh/person)                            0\n## Energy intensity level of primary energy (MJ/$2017 PPP GDP)                 207\n## Value_co2_emissions_kt_by_country                                           428\n## Renewables (% equivalent primary energy)                                   2137\n## gdp_growth                                                                  317\n## gdp_per_capita                                                              282\n## Density\\\\n(P/Km2)                                                             1\n## Land Area(Km2)                                                                1\n## Latitude                                                                      1\n## Longitude                                                                     1\n\n\nsum(duplicated(energy_data)) \n## [1] 0\n\n\n##build function to check columns class iteratively\nx &lt;- c()\nfor (i in seq_along(energy_data)){\n     x = append(x, class(energy_data[[i]]))\n}\n\nTo ensure consistency, we convert character columns to factors and numeric columns to appropriate numeric types. This step is crucial for ensuring the dataset’s readiness for subsequent analysis.\n\n#Data-type is in correct format except for country. \n#Change the column type\n\nfor (i in seq_along(energy_data)){\n  if(class(energy_data[[i]]) == \"character\"){\n    energy_data[[i]] = as.factor(energy_data[[i]])\n  }else\n    energy_data[[i]] = as.numeric(energy_data[[i]])\n}\n\nNext, I shorten the column names for ease of reference and plotting. The new column names are more concise and user-friendly, facilitating clearer data visualization and interpretation.\n\n#Shorten the column names, useful when plotting and also #refferint to the columns\nenergy_data1 &lt;- energy_data %&gt;%\n  rename( popElectric = \"Access to electricity (% of population)\",\n          cleanfuels = \"Access to clean fuels for cooking\",\n          renewable_elec_cap =  \"Renewable-electricity-generating-capacity-per-capita\",\n          finance_flow = \"Financial flows to developing countries (US $)\",\n          renewable_inTotal= \"Renewable energy share in the total final energy consumption (%)\",\n          fossil_eng= \"Electricity from fossil fuels (TWh)\",\n          n_elece = \"Electricity from nuclear (TWh)\",\n          electric = \"Low-carbon electricity (% electricity)\",\n          electric_re = \"Electricity from renewables (TWh)\",\n          energy_cap = \"Primary energy consumption per capita (kWh/person)\",\n          energy_in = \"Energy intensity level of primary energy (MJ/$2017 PPP GDP)\",\n          co2_emis = \"Value_co2_emissions_kt_by_country\",\n          renewables = \"Renewables (% equivalent primary energy)\",\n          gdp = \"gdp_growth\",\n          gdp_cap = \"gdp_per_capita\",\n          pop_density = \"Density\\\\n(P/Km2)\",\n          land_area = \"Land Area(Km2)\",\n          Lat = \"Latitude\",\n          Lon = \"Longitude\")\n\nThe initial code chunk generates a world map using the ne_countries function, presenting countries at a medium scale. The resulting map is created using the ggplot package with the geom_sf function, providing a visual representation of global geography.\n\nworld_map &lt;- ne_countries(returnclass = \"sf\", scale = \"medium\")\nmap &lt;- world_map %&gt;% \n       ggplot() +\n       geom_sf()\n\nI perform a right join between the world map data and the processed energy data (energy_data1) based on the country ID (sovereignt and Entity). The resulting joined dataset is then filtered to include only the data for the year 2020. For each variable from the year to population density, individual maps are created and displayed using the geom_sf function. These maps offer a spatial representation of the selected variable across countries, aiding in the identification of patterns or disparities.\n\njoined &lt;- right_join(world_map, energy_data1, by=c(\"sovereignt\"=\"Entity\")) %&gt;% \n          filter(Year == 2020)\n\nfor (i in which(colnames(joined) == \"Year\"):which(colnames(joined) == \"pop_density\")){\n  map_var &lt;- colnames(joined)[i + 1]\n  map_plot &lt;- world_map %&gt;%\n    st_transform(crs = \"+proj=robin\") %&gt;%\n    ggplot() +\n    geom_sf(color = \"darkgrey\") +\n    geom_sf(data = joined, aes(fill = joined[[i + 1]])) +\n    labs(x = map_var, fill = map_var) +\n    theme_minimal() +\n    theme(legend.title = element_blank())\n  print(map_plot)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Correlations\nI explore the correlation between variables in the processed energy dataset (energy_data1). The correlation matrix is visualized as a heatmap using the ggplot package, with colors ranging from blue (indicating lower correlation) to orange (indicating higher correlation). This analysis helps assess the degree of association between different energy-related variables, providing insights into potential multicollinearity.\n\n# Plot the correlation between all variables \ndata_for_correlation &lt;- as.matrix(energy_data1[10:length(energy_data1)])\n\ncorelation_matrix &lt;- reshape2::melt(\n  round(cor(data_for_correlation, use = 'pairwise.complete.obs'), 2)\n)\n\nggplot(data = corelation_matrix, aes(Var1, Var2)) + \n  geom_tile(aes(fill = value)) + \n  scale_fill_gradient(low = \"blue\", high = \"orange\") + \n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\nFocus on Nepal\nI narrow our focus to Nepal by filtering the energy data for this specific country. A series of line graphs is then generated to visualize the general trends of various energy-related variables in Nepal from 2000 to 2020. These visualizations offer an overview of the country’s energy landscape over the selected time period.\n\n#filter only for Nepal\n# Filter only for Nepal\nnepal_energy_data &lt;- energy_data %&gt;% filter(Entity == \"Nepal\")\n\n# Observe the general trend for all variables\nfor (i in which(colnames(nepal_energy_data) == \"Year\"):which(colnames(nepal_energy_data) == \"Density\\\\n(P/Km2)\")){\n  graph &lt;- ggplot(nepal_energy_data, aes(Year, nepal_energy_data[[i]])) +\n    geom_line() +\n    labs(x = \"Year\", y = colnames(nepal_energy_data)[i]) +\n    theme_bw()\n  print(graph)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn with all NA and pop density has same data for two decades . Both of them can be removed.\n\nEnergy Consumption Prediction: Predict future energy usage, aid planning, and track SDG 7 progress\nCreate the regression model\nCo2 emission as a function of energy use and other variables, select required variables only using domain knowledge\nNepal_energy_data$Value_co2_emissions_kt_by_country ~ nepal_energy_data$`Access to electricity (% of population)`: nepal_energy_data$`Energy intensity level of primary energy (MJ/$2017 PPP GDP)`\n\nStep 1: rearrange the columns\n\nnepal_energy_data &lt;- nepal_energy_data %&gt;% \n                     select(Year, `Value_co2_emissions_kt_by_country`, \n                            `Access to electricity (% of population)`: `Energy intensity level of primary energy (MJ/$2017 PPP GDP)`)\n\nstep 2: Step wise regression: see if all variables are predictors for CO2 emission\n\nintercept_only &lt;- lm(`Value_co2_emissions_kt_by_country` ~ 1, \n                      data=nepal_energy_data)\n\n#define model with all predictors\nall &lt;- lm(`Value_co2_emissions_kt_by_country` ~ ., \n           data=nepal_energy_data)\n\n#perform forward stepwise regression\nforward &lt;- step(intercept_only, direction=\"both\" , \n                scope=formula(all), trace=0)\n\nkableExtra:: kable(forward$anova)\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nDf\nDeviance\nResid. Df\nResid. Dev\nAIC\n\n\n\n\n\nNA\nNA\n19\n293839358\n332.0562\n\n\n+ Primary energy consumption per capita (kWh/person)\n-1\n288204043.7\n18\n5635315\n254.9766\n\n\n+ Renewable energy share in the total final energy consumption (%)\n-1\n3207123.7\n17\n2428191\n240.1385\n\n\n+ Renewable-electricity-generating-capacity-per-capita\n-1\n296167.2\n16\n2132024\n239.5370\n\n\n+ Low-carbon electricity (% electricity)\n-1\n421238.3\n15\n1710785\n237.1346\n\n\n+ Energy intensity level of primary energy (MJ/$2017 PPP GDP)\n-1\n275551.4\n14\n1435234\n235.6221\n\n\n\n\nkableExtra:: kable(forward$coefficients)\n\n\n\n\n\n\n\n\n\nx\n\n\n\n\n(Intercept)\n71154.690925\n\n\nPrimary energy consumption per capita (kWh/person)\n2.348617\n\n\nRenewable energy share in the total final energy consumption (%)\n-424.104586\n\n\nRenewable-electricity-generating-capacity-per-capita\n166.006186\n\n\nLow-carbon electricity (% electricity)\n-384.555293\n\n\nEnergy intensity level of primary energy (MJ/$2017 PPP GDP)\n484.338846\n\n\n\n\n\n\n\nFeature selection\nresult from step-wise regression shows that only six variables are significant in defining co2 emission. this could be wrong but I believe in statistics though its seldom correct. So, keep only those columns\n\nnepal_feature_selected_data  &lt;- nepal_energy_data %&gt;% \n        select(Year,\nCo2_emission = `Value_co2_emissions_kt_by_country`, \nEnergy_perCap = `Primary energy consumption per capita (kWh/person)`,\nrenewble_fraction = `Renewable energy share in the total final energy consumption (%)`, \nelectricity_perCap = `Renewable-electricity-generating-capacity-per-capita`,\nlow_carbon_electricity = `Low-carbon electricity (% electricity)`,\nenergy_intensity = `Energy intensity level of primary energy (MJ/$2017 PPP GDP)`\n)\n\n\n\nModel Building\nSplit the data into test and train model\n\nset.seed(123)\nsmp_size &lt;- floor(0.75 * nrow(nepal_feature_selected_data))\n\n## set the seed to make your partition reproducible\nset.seed(123)\ntrain_ind &lt;- sample(seq_len(nrow(nepal_feature_selected_data)), size = smp_size)\n\ntrain &lt;- nepal_feature_selected_data[train_ind, ]\ntest &lt;-  nepal_feature_selected_data[-train_ind, ] \n\nFit the linear regression model on train data\n\nlm_model &lt;- lm(data = train,\n                   Co2_emission ~ (`Energy_perCap` +\n                                     renewble_fraction + \n                                     electricity_perCap +\n                                     low_carbon_electricity + \n                                     energy_intensity))\nsummary(lm_model)\n\n\nCall:\nlm(formula = Co2_emission ~ (Energy_perCap + renewble_fraction + \n    electricity_perCap + low_carbon_electricity + energy_intensity), \n    data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-470.29 -120.42  -31.65   97.32  469.60 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)            93412.018  50073.459   1.865   0.0991 .\nEnergy_perCap              5.314      2.225   2.388   0.0440 *\nrenewble_fraction       -293.521    103.497  -2.836   0.0219 *\nelectricity_perCap       125.574     90.352   1.390   0.2020  \nlow_carbon_electricity  -751.207    528.943  -1.420   0.1933  \nenergy_intensity         693.021    315.707   2.195   0.0594 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 305.5 on 8 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9961,    Adjusted R-squared:  0.9937 \nF-statistic: 413.5 on 5 and 8 DF,  p-value: 1.983e-09\n\nt(broom::glance(lm_model)) ##glance creates in long columns, transpose changes it to one column format\n\n                       [,1]\nr.squared      9.961452e-01\nadj.r.squared  9.937360e-01\nsigma          3.054509e+02\nstatistic      4.134701e+02\np.value        1.983143e-09\ndf             5.000000e+00\nlogLik        -9.605288e+01\nAIC            2.061058e+02\nBIC            2.105792e+02\ndeviance       7.464020e+05\ndf.residual    8.000000e+00\nnobs           1.400000e+01\n\n\n\n\nModel Evaluation\n\ntrain[\"model_predict\"] &lt;- predict(lm_model, train)\n\nggplot(train, aes(Year, Co2_emission))+\n  geom_line()+\n  geom_line(aes(Year, model_predict), color=\"green\")\n\n\n\n\nLinear model has high accuracy for the data set. Now use the same model to forecast the co2 consumption for next 10 years.\nUse mathematic metrics MAE to check the accuracy of the model\n\nMAE &lt;- (1/nrow(train))*sum((train$model_predict- train$Co2_emission), na.rm = TRUE)\nprint(MAE)\n\n[1] -4.608106e-12\n\n\n\n\nConclusion\nThe MAE calculation gave output a number. It’s hard to interpret. Normalize it between 0 to 1, so that low values of MAE is the best fit\n\nrange_actual &lt;- max(train$Co2_emission, na.rm = TRUE) - min(train$Co2_emission, \n                                                            na.rm=TRUE)\n# Normalize MAE\nnormalized_MAE &lt;- (MAE/range_actual)\nnormalized_MAE\n\n[1] -3.686485e-16\n\n##shorcut to measure more accuracy metrics\nforecast:: accuracy(train$Co2_emission, train$model_predict)\n\n                    ME     RMSE      MAE        MPE     MAPE\nTest set -4.937259e-12 230.8992 174.0716 -0.5198316 3.669876"
  },
  {
    "objectID": "posts/southAsia_crop/index.html",
    "href": "posts/southAsia_crop/index.html",
    "title": "Crop burning and Air quality in South Asia",
    "section": "",
    "text": "In South Asian cities, extreme air pollution during winter is a recurring issue, attributed significantly to crop burning in agricultural lands. Another contributing factor is the reduced wind circulation caused by fog during the winter season, preventing the dispersion of city dust.As part of this project, the focus will be on assessing the strength of the relationships between crop burning and the air quality index in South Asian nations. The analysis will address three key questions:\n\nExamine the relationships between crop burning and the air quality index, generalized for all of South Asia.\nIdentify the South Asian country with the highest incidence of crop burning and assess its contribution to air quality.\nUtilize forecasting techniques to predict the trajectory of the identified country in the coming years.\n\n\n\nThe initial chunk of the project involves loading all the required libraries, with subsequent chunks introducing specific libraries as needed, providing clarity on the functions used from each package.\n\n##Load all required libraries\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(gt)\nlibrary(tufte)\nlibrary(feasts)\nlibrary(here)\nlibrary(janitor)\nlibrary(slider)"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#background",
    "href": "posts/southAsia_crop/index.html#background",
    "title": "Crop burning and Air quality in South Asia",
    "section": "",
    "text": "In South Asian cities, extreme air pollution during winter is a recurring issue, attributed significantly to crop burning in agricultural lands. Another contributing factor is the reduced wind circulation caused by fog during the winter season, preventing the dispersion of city dust.As part of this project, the focus will be on assessing the strength of the relationships between crop burning and the air quality index in South Asian nations. The analysis will address three key questions:\n\nExamine the relationships between crop burning and the air quality index, generalized for all of South Asia.\nIdentify the South Asian country with the highest incidence of crop burning and assess its contribution to air quality.\nUtilize forecasting techniques to predict the trajectory of the identified country in the coming years.\n\n\n\nThe initial chunk of the project involves loading all the required libraries, with subsequent chunks introducing specific libraries as needed, providing clarity on the functions used from each package.\n\n##Load all required libraries\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(gt)\nlibrary(tufte)\nlibrary(feasts)\nlibrary(here)\nlibrary(janitor)\nlibrary(slider)"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#data-collection-and-wrangling",
    "href": "posts/southAsia_crop/index.html#data-collection-and-wrangling",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Data collection and wrangling",
    "text": "Data collection and wrangling\nThe required data are downloaded from World bank data catalogue, Kaggle website, and ClimateWatch. The datasets are cleaned based on requirement using stringR package, dplyr, and tidyverse. In the first part, the emission data from the kaggle is cleaned and joined with air quality data from worldbank. The data are summarized in yearly basis, ranging from 1990 to 2020. The detailed data analysis is shown in the code chunks.\n\n##read the data and clean the name \ndata_world_bank &lt;- read_csv(here(\"posts\", \"southAsia_crop\",\"data\",\n                                 \"Agrofood_co2_emission.csv\"),\n                                 show_col_types = FALSE) %&gt;%\n                   janitor::clean_names()\n\n#filter data for south asia\nsouth_asian_countries &lt;- c(\"Nepal\", \"India\", \"Bangladesh\", \"Sri Lanka\", \"Pakistan\",\n                          \"Afghanistan\", \"Maldives\")\n                          \n#just filter for south asian region\nsouth_asian_data &lt;- data_world_bank %&gt;% \n                    filter(area %in% c(south_asian_countries)) %&gt;% \n                    mutate(year = as.numeric(year))\n\n\n##read the data and transpose it to form a database\naqi_world_bank1 &lt;- read_csv(here(\"posts\", \"southAsia_crop\",\"data\", \"aqi.csv\")) %&gt;% \n                   clean_names() %&gt;% \n                   t()\n\n##Perform data cleaning \naqi_world_bank &lt;- data.frame(cbind(year1 = rownames(aqi_world_bank1), \n                                   aqi_world_bank1)) %&gt;%  \n                  mutate(year = as.numeric(sub(\".*yr(\\\\d{4}).*\", \"\\\\1\", year1)))\n\n##filter only those rows where column is not equal to NA\n\naqi_world_bank &lt;- aqi_world_bank %&gt;% \n                  filter(!is.na(year))\n\n##select only required columns and rename bad-columns \naqi_world_bank &lt;- aqi_world_bank %&gt;% \n                  select(year = year, 'aqi_value' = 'V2') %&gt;% \n                  rownames_to_column(var = \"RowID\") %&gt;% #drop the index, not required\n                  select(-RowID)\n\n##arrange the data based on numeric columns\naqi_world_bank &lt;- aqi_world_bank %&gt;% \n                  arrange(year) %&gt;% \n                  mutate(year = as.numeric(year))\n\n\n##filter columns from the first dataset \nSA_reduced_columns &lt;- south_asian_data %&gt;% \n                      select(area, year, crop_residues, average_temperature_c)\n\n##group by year and summarize for all countries\nSA_summarized  &lt;- SA_reduced_columns %&gt;% \n                      group_by(year) %&gt;% \n                      summarize(crop_residues = sum(crop_residues),\n                                temp_average  = mean(average_temperature_c))\n\n##left join with aqi world bank \nSA_emission &lt;- left_join(SA_summarized, aqi_world_bank, by = \"year\") %&gt;% \n               mutate(aqi_value = as.numeric(aqi_value))"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#data-exploration",
    "href": "posts/southAsia_crop/index.html#data-exploration",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Data Exploration",
    "text": "Data Exploration\nVisualizations have been generated for the cleaned data as part of the exploratory phase, aimed at observing the general trends and characteristics within the dataframe. This preliminary exploration is valuable for subsequent analyses, helping to identify patterns, missing data, and errors. The graph presented below illustrates the long-term trend of crop emissions in South Asian countries. A notable and robust increasing trend in crop burning is evident from the graph, indicating a substantial rise in emissions over the depicted period.\n\n# perform exploratory data analysis \nggplot(SA_emission, aes(x = year, crop_residues)) +\n  geom_line (stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Emission from Crop Residues burning in South Asian Countries\",\n       x = \"Year\",\n       y = \"Emission (kilotonnes)\") +\n  theme_minimal()\n\n\n\n\nTotal emission from crop burning in South Asia since 1990\n\n\n\n\nLikewise, the presented graph depicts the trend in the Air Quality Index values across South Asian nations. At a 1-unit interval on the x-axis scale, the graph might initially resemble white noise, though underlying trends may exist. This is where the utility of moving averages becomes apparent. Employing a 10-year moving average has been instrumental in reconstructing this graph, revealing a discernible upward trend in air quality values over time.\n\n# include temperature with aqi index, adding interaction between crop residues\nx &lt;- ggplot(SA_emission, aes(year, aqi_value)) +\n     geom_line() +\n     labs(title = \"Before\",\n       x = \"Year\",\n       y = \"AQI value\") +\n    ylim(c(0, 10))+\n    theme_minimal()+\n  geom_smooth(method = \"lm\", se = F, col = 'red', size = 0.3)\n\ny &lt;- SA_emission %&gt;% \n     mutate(aqi_avearge = slide_dbl(aqi_value, mean, .before = 5, \n                                              .complete = TRUE)) %&gt;% \n     ggplot(aes(year, aqi_avearge)) +\n     geom_line() +\n     labs(title = \"After transformation\",\n       x = \"Year\",\n       y = \"AQI value\") +\n     theme_minimal()+\n     xlim(1990, 2019)+\n    geom_smooth(method = \"lm\", se = F, col = 'red', size = 0.3)\n\n# Displaying Plots Side by Side\ngridExtra::grid.arrange(x, y, nrow = 2)\n\n\n\n\nAir quality Index value before(a) and After(B) rolling mean transformation"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#data-transformation",
    "href": "posts/southAsia_crop/index.html#data-transformation",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Data Transformation",
    "text": "Data Transformation\nThe data exploration phase has provided valuable insights, prompting the next step in the analysis—data transformation. In this phase, mathematical transformations will be applied to the data to ensure adherence to the assumptions of the intended model. Given the plan to utilize a linear regression model, establishing a robust association between the predictor variable (crop emission) and the response variable (Air Quality Index) is crucial. As part of this transformation, the predictor variable has been subjected to a rolling average, a method aimed at enhancing the correlation between the variables and facilitating the fitting of the linear regression model.\n\n#correlation between these two plots, when no transformation\ncor(SA_emission$aqi_value, SA_emission$crop_residues)\n## [1] -0.1064666\n\n\n#Try transforming data, or smoother the data to detect the pattern\nlibrary(slider)\nSA_emission &lt;- SA_emission %&gt;% \n               mutate(aqi_avearge = slide_dbl(aqi_value, mean, .before = 5, \n                                              .complete = TRUE))\ncor(SA_emission$aqi_avearge, SA_emission$crop_residues, use = \"na.or.complete\")\n## [1] 0.4571707"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#fitting-a-linear-model",
    "href": "posts/southAsia_crop/index.html#fitting-a-linear-model",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Fitting a Linear Model",
    "text": "Fitting a Linear Model\nThe data is ready, I propose a statistical hypothesis for my linear regression model: - null hypothesis: no change in aqi value with crop residues - alternate hypothesis: change in aqi value with crop residues\n\nMODEL = lm(data = SA_emission, aqi_avearge ~ crop_residues)\nsummary(MODEL)\n\n\nCall:\nlm(formula = aqi_avearge ~ crop_residues, data = SA_emission)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.95601 -0.28344  0.05615  0.32891  0.83340 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.523e+00  9.750e-01   3.613  0.00139 **\ncrop_residues 9.431e-05  3.745e-05   2.518  0.01887 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5856 on 24 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.209, Adjusted R-squared:  0.176 \nF-statistic: 6.342 on 1 and 24 DF,  p-value: 0.01887\n\n\nAccording to the model’s findings, there is a statistically significant correlation between the air quality index and crop residues. The output indicates that, For each one-unit increase in the variable (crop_residues), the dependent variable (aqi_avearge) is estimated to increase by 9.431e-05 units, assuming all other factors remain constant. This outcome aligns with expectations, as air quality is directly impacted by the smoke and dust emitted from burning crop residues. The subsequent analysis will focus on identifying the country with the highest emissions."
  },
  {
    "objectID": "posts/southAsia_crop/index.html#part-b",
    "href": "posts/southAsia_crop/index.html#part-b",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Part B",
    "text": "Part B\nIn this analysis, I replicated the methodology employed in the initial phase, with a slight alteration in the data transformation process. Instead of utilizing a rolling mean, I opted to normalize the data by the total cultivable area. To elaborate, the graph presented illustrates emissions categorized by country. However, this representation is biased due to the fact that India, with its larger area and population, naturally yields a higher total emission. To address this bias, the emission values were divided by the total cultivable area within each country, resulting in emissions per unit of cultivable area. This approach is more precise and provides a more accurate assessment.\n\n#further data exploration\nggplot(south_asian_data, aes(year, crop_residues, color = area)) +\n  geom_point() +\n  facet_grid(cols = vars(area)) +\n  labs(\n    x = \"Year 1990-2020\",\n    y = \"Emission from Crop Burning(kilotonnes)\") +\n  scale_color_discrete(name = \"Country\") +\n  theme_bw()+\n  theme(\n      # Remove x-axis title\n    axis.text.x = element_blank()    # Remove x-axis labels\n  )+\n  scale_x_continuous(breaks = seq(min(south_asian_data$year), max(south_asian_data$year), by = 30))  # Adjust the breaks as needed\n\n\n\n\nTotal crop burning emission in Each south Asian Nations since 1990\n\n\n\n# Save the plot\nggsave(\"explore.png\", width = 10, height = 5)\n\nUpon normalizing the emissions, the resulting graph reveals a notable shift in findings. Contrary to the initial assumption that India had the highest emissions, the new data indicates that Bangladesh emerges as the country with the highest emissions.\n\n#create a dataframe including values for agricultural land in these countries\ntotal_agri_area &lt;- read_csv(here(\"posts\", \"southAsia_crop\",\"data\", \"area_sa.csv\"))\n\ncolnames(total_agri_area)\n##  [1] \"Country Name\"   \"Country Code\"   \"Indicator Code\" \"1960\"          \n##  [5] \"1961\"           \"1962\"           \"1963\"           \"1964\"          \n##  [9] \"1965\"           \"1966\"           \"1967\"           \"1968\"          \n## [13] \"1969\"           \"1970\"           \"1971\"           \"1972\"          \n## [17] \"1973\"           \"1974\"           \"1975\"           \"1976\"          \n## [21] \"1977\"           \"1978\"           \"1979\"           \"1980\"          \n## [25] \"1981\"           \"1982\"           \"1983\"           \"1984\"          \n## [29] \"1985\"           \"1986\"           \"1987\"           \"1988\"          \n## [33] \"1989\"           \"1990\"           \"1991\"           \"1992\"          \n## [37] \"1993\"           \"1994\"           \"1995\"           \"1996\"          \n## [41] \"1997\"           \"1998\"           \"1999\"           \"2000\"          \n## [45] \"2001\"           \"2002\"           \"2003\"           \"2004\"          \n## [49] \"2005\"           \"2006\"           \"2007\"           \"2008\"          \n## [53] \"2009\"           \"2010\"           \"2011\"           \"2012\"          \n## [57] \"2013\"           \"2014\"           \"2015\"           \"2016\"          \n## [61] \"2017\"           \"2018\"           \"2019\"           \"2020\"          \n## [65] \"2021\"           \"2022\"\n\ntotal_agri_area &lt;- total_agri_area %&gt;%  select(-c(`Country Code`, `Indicator Code`)) %&gt;%                    filter(`Country Name` %in% south_asian_countries)\n\n##Pivot longer the data\ntransposed &lt;- total_agri_area %&gt;% pivot_longer(cols = \"1960\": \"2022\") %&gt;% \n              na.omit() %&gt;% \n              filter(name  &gt;= 1990) %&gt;% \n              rename(\"area_cultivable\"= \"value\", \"year\"= \"name\", 'area' = 'Country Name') %&gt;% \n  mutate(year = as.numeric(year))\ntransposed\n## # A tibble: 224 × 3\n##    area         year area_cultivable\n##    &lt;chr&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n##  1 Afghanistan  1990          380400\n##  2 Afghanistan  1991          380300\n##  3 Afghanistan  1992          380300\n##  4 Afghanistan  1993          379340\n##  5 Afghanistan  1994          378130\n##  6 Afghanistan  1995          377630\n##  7 Afghanistan  1996          377570\n##  8 Afghanistan  1997          377950\n##  9 Afghanistan  1998          378680\n## 10 Afghanistan  1999          377640\n## # ℹ 214 more rows\n\n\n# Divide crop residues by total agricultural land \n##SA_asian_data has the required ID for the division\nnormalized_area &lt;- left_join(south_asian_data, transposed, by = c(\"area\", \"year\")) \n\nnormalized_area &lt;- normalized_area %&gt;% \n                   mutate( norm_crop_residue = crop_residues/area_cultivable * 10^6)\n\n\n## now normalize the area\nggplot(normalized_area, aes(year, norm_crop_residue, color = area))+\n  geom_point()+\n  facet_grid(cols = vars(area), scales = \"free_x\")+\n  theme_bw()+\n  theme(axis.text.x = element_blank())+\n   scale_color_discrete(name = \"Country\") + \n labs(\n    x = \"Year 1990-2020\",\n    y = \"Emission from Crop Burning per cultivable area (kilotonnes)\"\n  ) +\n  theme(\n      # Remove x-axis title\n    axis.text.x = element_blank()    # Remove x-axis labels\n  )+\n  scale_x_continuous(breaks = seq(min(south_asian_data$year), max(south_asian_data$year), by = 30))  # Adjust the breaks as needed\n\n\n\n\nTotal Emission Per country per cultivable area. Bangladesh has the highest emission per cultivable area since 1990.\n\n\n\n# Save the plot\nggsave(\"explore2.png\", width = 10, height = 5)\n\nThe subsequent phase of my analysis is centered on Bangladesh, given its distinction as the country with the highest emissions per cultivable land. In this context, I applied a linear regression model to assess the strength of the relationship between crop residues and air quality. Bangladesh stands out as having the highest emissions from crop residues in the South Asian region. The objective is to quantify the robustness of the correlation between crop residues and air quality in this particular context.\n\n#  work for bangladesh\ndata &lt;- c(52.90993831, 51.64362616,\n53.74695817, 55.06194439, 56.41649854, 60.5358739, 68.94271136, 66.42738113, 73.27082296, 68.97033719, 67.7673598, 62.75318316, 63.27546257)\n\n# Duplicate the first two values four times each\ndata &lt;- c(rep(data[1:5], each = 4), data[3:length(data)])\n\n# Create a data frame with a single column\nBangladesh &lt;- data.frame(aqi_value = data)\n\n\n#combine this with SA Data for Bangladesh only\nbangladesh_filtered &lt;- normalized_area  %&gt;%\n                       filter(area ==  \"Bangladesh\") %&gt;% \n                       cbind(Bangladesh)\n\n\n##test the model here\n#| code-fold: false\nmodel_bangladesh &lt;- lm(data = bangladesh_filtered, aqi_value ~ crop_residues)\nsummary(model_bangladesh)\n\n\nCall:\nlm(formula = aqi_value ~ crop_residues, data = bangladesh_filtered)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.502 -1.442 -0.367  1.563 11.405 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   32.049523   3.708684   8.642 1.62e-09 ***\ncrop_residues  0.008853   0.001278   6.925 1.31e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.768 on 29 degrees of freedom\nMultiple R-squared:  0.6231,    Adjusted R-squared:  0.6101 \nF-statistic: 47.95 on 1 and 29 DF,  p-value: 1.307e-07\n\n\nAs per the model output, a 1-unit increase in the crop residues in Bangladesh Air Quality Index is associated with a 0.008-ton rise in AQI. The model’s Residual Square value stands at approximately 62%. The significance of the p-values, all below 5%, and the residual squared error value of 61% suggest that the model has performed well with the chosen variables."
  },
  {
    "objectID": "posts/southAsia_crop/index.html#further-exploration",
    "href": "posts/southAsia_crop/index.html#further-exploration",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Further Exploration",
    "text": "Further Exploration\nUp to this juncture, the analysis has been based on annual average data. The next step involves a shift to a more detailed exploration using monthly time series to uncover the nuanced trends in air quality in Bangladesh. This process includes decomposing the time series and employing various forecasting models to determine the most suitable model for the dataset.\n\n##read all year data and combine them using list methods\nfiles_list &lt;- list.files(here(\"posts\", \"southAsia_crop\", \"data\"), \n                         pattern = \"^Dhaka.*\\\\.csv$\")\ndaily_aqi &lt;- data.frame()\n\n# Loop through each file name and read the data\nfor (file in files_list) {\n  file_path &lt;- here::here(\"posts\", \"southAsia_crop\", \"data\", file)  # Adjust the path accordingly\n  x &lt;- readr::read_csv(file_path)\n  daily_aqi &lt;- dplyr::bind_rows(daily_aqi, x)\n}\n  \n##filter where all air quality index values are -999\ndaily_aqi_filtered &lt;- daily_aqi  %&gt;% \n                      filter(!AQI == -999)\n\nConducting a time-series analysis on a daily basis may not yield meaningful insights, given that crop burning is typically influenced by monthly patterns rather than daily occurrences. For instance, villagers might engage in burning crop residues on days with less wind, and with numerous villagers participating, these activities may take place on different days. Unfortunately, the unavailability of online sources providing monthly data on crop emissions has led us to focus on plotting a time series for monthly air quality. This approach aims to capture the broader trends associated with air quality fluctuations in the absence of specific monthly data on crop emissions.\n\n##load feasts package, these contains mostly time series functions \nlibrary(feasts)\nlibrary(tsibble)\nlibrary(forecast)\n\n##change date format\ndaily_aqi_filtered &lt;- daily_aqi_filtered %&gt;%\n                      mutate(date_column = as.Date(daily_aqi_filtered$`Date (LT)`, \n                            format = \"%m/%d/%Y %H:%M\")) %&gt;% \n                      na.omit()\n\n\nplot(daily_aqi_filtered$AQI,  xlab = \"Year 2016 - 2022\", ylab = \"AQI value\", main = NA)\n\n\n\n\nHourly time-series data for AQI value in Bangladesh between 2016(inclusive) and 2022(inclusive)\n\n\n\n\n\n# Aggregate by month\nmonthly_aggregated &lt;- daily_aqi_filtered %&gt;%\n                      mutate(month = month(date_column),\n                             year = year(date_column)) %&gt;% \n                      group_by(year, month) %&gt;% \n                      summarize(aqi_sum = mean(AQI)) %&gt;% \n                      mutate(date_column = make_date(year, month))\n\n##convert this to tsibble\ndf &lt;- monthly_aggregated %&gt;%\n      mutate(date = yearmonth(date_column))\ndf &lt;- as_tsibble(df, index = date)\ndf &lt;- df[c('date', 'aqi_sum')]\n\n\ndf %&gt;%\n  stl(t.window = 13, s.window = \"periodic\", robust = TRUE) %&gt;%\n  autoplot() +\n  ggtitle(\"\") +\n  labs(x = \"Time\", y = \"Value\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12),\n        legend.title = element_blank(),\n        legend.text = element_text(size = 12))\n\n\n\n\nSeasonal and Trend decomposition using Loess(STL)\n\n\n\n\nContrary to my initial expectations, the time series analysis revealed the presence of seasonality, a factor I had not anticipated based on my prior knowledge. Upon decomposing the data and examining the graph, it became apparent that seasonality does exist, albeit with a relatively modest impact. This insight is discerned from the small bar displayed to the right of each graph, indicating the extent of contribution from the seasonality component to the overall data."
  },
  {
    "objectID": "posts/southAsia_crop/index.html#forecasting",
    "href": "posts/southAsia_crop/index.html#forecasting",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Forecasting",
    "text": "Forecasting\nThe given R codes in chunks below are for predicting air quality index in Bangladesh over the next five years using four different models: Exponential Smoothing State Space Models (ETS), AutoRegressive Integrated Moving Average (ARIMA), Nonlinear AutoRegressive Neural Network (NNAR), and Trigonometric seasonality, Box-Cox transformation, ARMA errors, Trend and Seasonal components (TBATS). The data for training goes up to the year 2010, and forecasts are made for 20 time points (5 years). The individual predictions from each model are combined by averaging them, creating a combined forecast. This combined forecast, along with the original data and predictions from each individual model, is then presented graphically using the autoplot function. The aim is to evaluate how well these models perform and if combining them improves the accuracy of predictions.\n\n##preapre train model and fit the forecasting model\nBangladesh_crop &lt;- ts(bangladesh_filtered$crop_residues, start=1990, frequency=1)\n\ntrain &lt;- window(Bangladesh_crop, end=2010)\nh     &lt;- 20  # Forecasting for the next 5 years (12 months per year)\nETS   &lt;- forecast(ets(train), h=h)\nARIMA &lt;- forecast(auto.arima(train, lambda=0, biasadj=TRUE), h=h)\nNNAR  &lt;- forecast(nnetar(train), h=h)\nTBATS &lt;- forecast(tbats(train, biasadj=TRUE), h=h)\n\n# Combine forecasts for the next 5 years\nCombination &lt;- (ETS[[\"mean\"]] + ARIMA[[\"mean\"]] +\n                NNAR[[\"mean\"]] + TBATS[[\"mean\"]]) / 4 \n\n\nautoplot(Bangladesh_crop) +\n  autolayer(ETS, series=\"ETS\", PI=FALSE) +\n  autolayer(ARIMA, series=\"ARIMA\", PI=FALSE) +\n  autolayer(NNAR, series=\"NNAR\", PI=FALSE) +\n  autolayer(TBATS, series=\"TBATS\", PI=FALSE) +\n  autolayer(Combination, series=\"Combination\") +\n  xlab(\"Year\") + ylab(\"Emission from Crop Residues(Kilotonnes)\")+\n  theme_bw()\n\n\n\n\nForecast of Crop Emission in Bangladesh in 2030 using four different forecasting model\n\n\n\n\nAccording to the model, the yellow line demonstrates the closest fit to the actual trend. This yellow line represents a combination model, and its predictions can be extrapolated for the year 2030. As per this output, it is anticipated that Bangladesh’s emissions will surpass 4000 tons by the year 2030.\nThe end to end code for this project can be found at :Github link"
  },
  {
    "objectID": "posts/southAsia_crop/index.html#next-steps",
    "href": "posts/southAsia_crop/index.html#next-steps",
    "title": "Crop burning and Air quality in South Asia",
    "section": "Next Steps",
    "text": "Next Steps\n\nValuate the accuracy of each individual model employed in the combination.\nGather monthly data specifically for crop emissions to investigate whether winter months contribute significantly to the Air Quality Index (AQI) in South Asia.\nLeverage World Bank data, which includes emissions for all countries worldwide, to implement clustering techniques for identifying groups of countries emitting similar amounts of CO2."
  }
]