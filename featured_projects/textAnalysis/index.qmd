---
title: "Identifying the most common word in articles on climate science"
image: "/featured_projects/textanalysis/news.jpg"
Date: "2023-12-15"
format:
  html:
    toc: true
    html-math-method: katex
    css: /featured_style_projects.css
author:
  - name: Sujan Bhattarai
categories: [NLP, R] # self-defined categories
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
editor: 
  markdown: 
    wrap: 72
engine: knitr
---

In today's era of burgeoning information, understanding the predominant themes and trends in vast volumes of text data is crucial for extracting meaningful insights. This project focuses on leveraging Natural Language Processing (NLP) techniques to analyze a collection of articles centered on climate science. By employing advanced computational methods, we aim to uncover the most prevalent topics discussed across these articles, shedding light on key issues and their interconnectedness within the domain of climate science.

Objectives
1. Data Collection and Preparation
Data Retrieval: Obtain a comprehensive dataset comprising articles related to climate science, sourced from diverse publications.
Data Cleaning: Standardize and preprocess the text data to remove noise, such as URLs, dates, and duplicates, ensuring a clean corpus for analysis.
2. Topic Modeling and Optimization
Topic Extraction: Apply Latent Dirichlet Allocation (LDA) to identify latent topics within the article corpus.
Optimization: Determine the optimal number of topics using quantitative metrics such as CaoJuan2009 and Deveaud2014, ensuring robust and interpretable results.
3. Insights Generation
Topic Interpretation: Analyze and interpret the extracted topics to discern prevalent themes and their implications in climate science research.
Visualization: Visualize topic distributions and top terms to facilitate intuitive understanding and exploration of thematic clusters.


```{r include=FALSE}
# Hide all warnings and messages
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Load the necessary libraries
library(quanteda)
library(tm)
library(topicmodels)
library(ldatuning)
library(tidyverse)
library(tidytext)
library(reshape2)
library(here)
library(dplyr)
library(LexisNexisTools)
library(readr)
library(stringr)
library(tidyr)
library(ggplot2)
```


```{r include=FALSE}
# Read the files
post_files <- list.files(pattern = ".docx", path = here("featured_projects/textanalysis/data"),
                         full.names = TRUE, 
                         recursive = TRUE, 
                         ignore.case = TRUE)

data_list <- lnt_read(post_files, convert_date = FALSE,
                      remove_cover = FALSE)
```


```{r}
# Change data_list into table for analysis, extract metadata
data_table <- data_list@meta

# Create a tibble with date, headline, id, and article
data_tibble <- tibble(Date = data_table$Date, 
                      Headline = data_table$Headline, 
                      id = data_list@articles$ID, 
                      text = data_list@articles$Article)

# Clean the dataset
data_tibble <- data_tibble %>% 
  # Remove all URLs and dates from the text
  mutate(text = str_remove_all(text, paste("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]",
                               "|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+")),
  text = str_remove_all(text, "\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b"),
  # Remove words with this pattern: February 14th, 2024 
  text = str_remove_all(text, paste0("\\b\\w+\\s\\d{1,2}[a-z]{2},\\s\\d{4}\\s\\",
                                    "(\\s\\w+\\sâ€”\\sDelivered\\sby\\sNewstex\\s\\)")))

# Remove duplicates in the text if they have the same words in the first 20 words
data_tibble <- data_tibble %>% distinct(Headline, .keep_all = TRUE)

# Calculate the distance between duplicates and remove the duplicates
data_tibble <- data_tibble %>%
  mutate(dist = stringdist::stringdist(text, lag(text), method = "jaccard")) %>% 
  filter(dist < 0.5 | is.na(dist)) %>% select(-dist) %>% 
  # Make all text lowercase
  mutate(text = tolower(text))


```

```{r}
# Create a corpus
corpus <- corpus(x = data_tibble, text_field = "text")
tokens(corpus)
add_stops <- stopwords(kind = quanteda_options("language_stopwords"))

toks <- tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE)
tok1 <- tokens_select(toks, pattern = add_stops, selection = "remove") # Remove stop words

dfm1 <- dfm(tok1, tolower = TRUE)
dfm2 <- dfm_trim(dfm1, min_docfreq = 2) # Word has to appear at least 2 times to stay in matrix

# As long as sum across the row is > 0, retain it
sel_idx <- slam::row_sums(dfm2) > 0
dfm <- dfm2[sel_idx, ]

```


```{r}
# Set value of k (how many ideas/themes within articles in this data set)

# Set model with k = 2
k <- 2
topicModel_k2 <- LDA(dfm,
                     k,
                     method = "Gibbs",
                     control = list(iter = 1000), # Number of iterations
                     verbose = 25) # Track progress after every 25 iterations

k <- 3 
topicModel_k3 <- LDA(dfm,
                     k,
                     method = "Gibbs",
                     control = list(iter = 1000), # Number of iterations
                     verbose = 25) # Track progress after every 25 iterations

# Run another model with k = 4
k <- 4
topicModel_k4 <- LDA(dfm,
                     k,
                     method = "Gibbs",
                     control = list(iter = 1000), # Number of iterations
                     verbose = 25) # Track progress after every 25 iterations

# Run another model with range of values of k
results <- FindTopicsNumber(dfm,
                            topics = seq(from = 2,
                                         to = 20, # Number of topics
                                         by = 1),
                            metrics = c("CaoJuan2009", "Deveaud2014"),
                            method = "Gibbs",
                            control = list(iter = 1000), # Number of iterations
                            verbose = 25) # Track progress after every 25 iterations

results

```


```{r}
# Plot topics and both metrics for getting the best value
ggplot(results, aes(x = topics, y = CaoJuan2009)) +
  geom_line() +
  theme_minimal()

ggplot(results, aes(x = topics, y = Deveaud2014)) +
  geom_line() +
  theme_minimal()

```
or my sets of articles, the optimization metrics continuously resulted in Null, showing that the model is not able to find the best value of k. However, the optimization metrics for the model with k = 2 is the lowest, suggesting that the model with k = 2 is the best model for my data set. The plots show a similar result, as there is no stabilization of trend for different values of optimization metrics. So, for the dataset, I will go with the model with k = 2.

```{r}
# Plot the top terms in each topic
terms(topicModel_k2, 10) # Most probable words in each topic

tmResult <- posterior(topicModel_k2)
terms(topicModel_k2)

theta <- tmResult$topics
beta <- tmResult$terms
vocab <- colnames(beta)

# Plot the distribution of topics across a sample of the documents from beta
topics <- tidy(topicModel_k2, matrix = "beta")

top_terms <- topics |> 
  group_by(topic) |> 
  top_n(10, beta) |> 
  ungroup() |> 
  arrange(topic, -beta)

kableExtra::kable(top_terms)

# Plot the distribution of topics across a sample of the documents from theta

top_terms |> 
  mutate(term = reorder_within(term, beta, topic, sep = "")) |> 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free_y") +
  scale_x_reordered() +
  coord_flip() +
  # Remove digits from labels 
  scale_x_discrete(labels = function(x) gsub("[0-9]", "", x)) +
  theme_minimal()
```

Upon analyzing the topics extracted from the articles using a two-topic classification model, it becomes evident that the primary focus of the articles revolves around polar bears, as indicated by the presence of the search item 'polar bear' across all topics. The specificity of the search item directly influences the thematic coherence of the topics.

In the first topic, discussions predominantly center around polar bear hunting and climate change research. This topic also touches on sea ice and the Arctic, possibly suggesting a thematic exploration of the ecological and environmental challenges faced by polar bears in the context of climate change.

Lastly, the second topic likely appears to explore the intricate relationship between polar bears and the importance of World Ice Day. It highlights the interconnectedness of these elements within the Arctic ecosystem, emphasizing the ecological implications of environmental shifts on polar bear populations and the emotion of the global community.

Overall, the topics extracted from the articles provide valuable insights that polar bears are not talked about in any other topics, other than explicitly discussing polar bears. Exploring more articles and topics could provide more insights on the themes discussed in the articles, but for my subset of articles, polar bears are not discussed in any other context other than research and when discussing international ice days.


