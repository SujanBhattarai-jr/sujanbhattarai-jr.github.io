{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Ocean Carbon prediction:serving Calcofi dataset to make data driven Decision\"\n",
        "Date: \"2023-12-15\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    html-math-method: katex\n",
        "    css: /featured_style_projects.css\n",
        "author:\n",
        "  - name: Sujan Bhattarai\n",
        "categories: [Machine Learning, Python] # self-defined categories\n",
        "draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n",
        "editor: \n",
        "  markdown: \n",
        "    wrap: 72\n",
        "---"
      ],
      "id": "8ffa48a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CalCOFI, the California Cooperative Oceanic Fisheries Investigations, was established in 1949 to investigate the ecological factors contributing to the collapse of the Pacific sardine population off California. Over time, its scope expanded to encompass the entire ecosystem of the southern California Current System, utilizing cutting-edge observation techniques. Its extensive time series data provides invaluable insights into the long-term impacts of environmental changes on marine ecosystems and the communities reliant on them, not only within the California Current System but also extending to the North Pacific and beyond on an international scale.\n"
      ],
      "id": "99d6d0bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#hide all warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#### Import all necessary libraries\n",
        "# import all required libraries\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import Ridge"
      ],
      "id": "4ed2e784",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Read the data and store ID, required for submission to kaggle"
      ],
      "id": "e001a0bb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#-----read train and test set\n",
        "train_calcofi = pd.read_csv(\"data/train.csv\")\n",
        "test_calcofi  = pd.read_csv(\"data/test.csv\")  \n",
        "\n",
        "#collect the test ids for testcalcofi, required for submission dataset\n",
        "test_ids = test_calcofi.id\n",
        "test_calcofi = test_calcofi.drop(columns=['id'])"
      ],
      "id": "6d0c58da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#----inspect the head and take the insights of data\n",
        "train_calcofi.head()\n",
        "#### Data cleaning and preprocessing\n",
        "#the column names are in snake case, change all to lowercase\n",
        "train_calcofi.columns = map(str.lower, train_calcofi.columns)\n",
        "test_calcofi.columns = map(str.lower, test_calcofi.columns)"
      ],
      "id": "674af290",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#remove the unnamed:12 column\n",
        "train_calcofi = train_calcofi.drop(columns=['unnamed: 12'])\n",
        "train_calcofi.rename(columns={'ta1.x': 'ta1'}, inplace=True)\n",
        "\n",
        "train_calcofi.head()\n",
        "\n",
        "#--test the range and distribution of values\n",
        "train_calcofi.describe()\n",
        "\n",
        "# test for NA values\n",
        "train_calcofi.info()"
      ],
      "id": "52175f67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#plot the correlation between variables to investigate linear linear relationshiop\n",
        "#most values shows either strong positive or negative associatoin\n",
        "#Linear regression can be still be better\n",
        "#Calculate the correlation matrix\n",
        "corr_matrix = train_calcofi.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt=\".1f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "id": "54860e68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Machine Learning\n",
        "For this project, I will delve into using different machine learning models, from the simplest also called the Linear regression, then gradually moving up to the best model, called XGboost, which is also called the Queen of the Machine Learning models.\n"
      ],
      "id": "5e096733"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#select only the predictors columns, and change them to array\n",
        "X = train_calcofi.drop(columns = ['dic', 'id'], axis=1)\n",
        "\n",
        "#select only the response column and change it to array\n",
        "y = train_calcofi['dic']\n",
        "\n",
        "#split the predictors and response variables from training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42)\n",
        "\n",
        "#### Linear regression\n",
        "# Create a pipeline with StandardScaler and LinearRegression\n",
        "model_pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
        "\n",
        "# Fit the model pipeline to the training data\n",
        "model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Calculate R^2 (coefficient of determination) on the training set\n",
        "r_sq = model_pipeline.score(X_train, y_train)\n",
        "\n",
        "# Print the coefficient of determination\n",
        "print(f\"Coefficient of determination (measure of variability in training set): {r_sq}\")\n",
        "\n",
        "# predict on test set from model\n",
        "y_pred = model_pipeline.predict(X_test)\n",
        "\n",
        "# Assuming y_test contains the actual labels/targets for the test set\n",
        "squared_error = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean squared error: {squared_error}\")"
      ],
      "id": "1d895fa0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The base linear regression model has worked well, with mean squared error of 31, which means the ocean carbon values(DIC) was off by 31 point on average for the prediction test. But, the model can be tuned such that the error can be minimized. The most popular method to do so is the stochastic gradient descent. This approach finds the best local minima, that has the capacity to reduce error but without redcing model Bias for new unseen data.\n",
        "\n",
        "Stochastic Gradient Descent\n",
        "This approach uses the learning rate (also called eta), to find the minimum cost function. The different between base linear regression and this stochastic Gradient descent is that the base model calculates cost based on optimum values, while stochastic gradient descent calculates based on learning rate on the cost function.\n"
      ],
      "id": "045d0dcb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a pipeline with feature scaling and SGDRegressor\n",
        "pipeline = make_pipeline(StandardScaler(), SGDRegressor(max_iter=2000, learning_rate='constant', eta0=0.0009))\n",
        "\n",
        "# Fit the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "#print the model score\n",
        "print(f\"Model score: {pipeline.score(X_test, y_test)}\")\n",
        "\n",
        "# Calculate mean squared error\n",
        "squared_error = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean squared error: {squared_error}\")"
      ],
      "id": "36412fb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Doing stochastic gradient descent did not improve anything in the model. First thing to note is that, model is already 99% accurate. Using SGD appraoch increased accuracy by 0.0001, which is not a massive improvement. But when model imporoved by that factor, the MAE value increased. This means bias increased, which we don't want.\n",
        "\n",
        "Add cross validation to Linear regression\n"
      ],
      "id": "07971fcb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Instantiate the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "test_calcofi_scaled = scaler.transform(test_calcofi)\n",
        "\n",
        "#instantiate the model\n",
        "model_cv = LinearRegression()\n",
        "\n",
        "# Define KFold cross-validation strategy\n",
        "kf = KFold(n_splits = 20, shuffle = True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(model_cv, X_train_scaled, y_train, cv = kf, scoring='neg_mean_squared_error')\n",
        "cv_scores"
      ],
      "id": "8243cf2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cross validation output does not show specific pattern. Some blocks of rows obviously don't correlate with rest of the other. See the fourth value, and the 17th, whose scores are away off. There must be something in the data that we should be able to capture. Maybe the temperature or any natural variable on that period has produced different sets of values.\n",
        "\n",
        "The best number of splits is just 2 for this Dataset. Is it suprising ?? Yes, it is. The way I identified that after testing with higher number of splits. There is somthing odd in the data, which contains off data rows. Maybe temperature or any other variable is off. If I increase the splits, there will always be at least two blocks that CV score will be much higher at least 10 times higher than all other.\n"
      ],
      "id": "39da5050"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate mean squared error using cross-validation\n",
        "mean_squared_error_cv = -cv_scores.mean()\n",
        "print(f\"Mean squared error using 2-fold cross-validation: {mean_squared_error_cv}\")\n",
        "\n",
        "# Fit the model on the entire training data\n",
        "model_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model_cv.predict(X_test_scaled)\n",
        "\n",
        "# Calculate mean squared error on the test set\n",
        "mse_lm = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean squared error on the test set: {mse_lm}\") \n",
        "#array of ids. copied from top cell\n",
        "test_ids = test_ids\n",
        "\n",
        "#predict based on the model\n",
        "prediction_on_test_calcofi = model_cv.predict(test_calcofi_scaled)\n",
        "\n",
        "# Create a DataFrame with predictions\n",
        "submission_df = pd.DataFrame({'id': test_ids, 'DIC': prediction_on_test_calcofi})\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "submission_df.to_csv('linear_cv.csv', index=False)"
      ],
      "id": "d1b9d3ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next step involves using **XGboost** for making the prediction, Extreme Gradient Boosting, also called the queen of the ML models is one of the most robust models. The next steps involves fitting a xgboost model for the same sets of data and prediciton sets. In the end, the best model among all test ML models will be submitted.\n",
        "\n",
        "**Tagline of XGBoost: I am a queen of Machine Learning model because I correct every mistakes**\n",
        "\n",
        "#### 1. Base XGBOOST model (no tuning: Out of Box model)\n",
        "Notes:XGBoost works on its own object type, which is Dmatrix. So, convert all sets to Dmatrix form\n"
      ],
      "id": "9eacb786"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create regression matrices, this is requirement for xgboost model\n",
        "dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
        "dtest_reg =  xgb.DMatrix(X_test, y_test, enable_categorical=True)"
      ],
      "id": "8f2cc845",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# use cross validation approach to catch the best boosting round\n",
        "n = 1000\n",
        "\n",
        "model_xgb = xgb.cv(\n",
        "   dtrain=dtrain_reg,\n",
        "   params = {},\n",
        "   num_boost_round= n,\n",
        "   nfold = 20, #number of folds for cross validation\n",
        "   verbose_eval=10, #record rmse every 10 interval\n",
        "   early_stopping_rounds = 5,\n",
        "   as_pandas = True#stop if there is no improvement in 5 consecutive rounds\n",
        ")\n",
        "\n",
        "# Extract the optimal number of boosting rounds\n",
        "optimal_boosting_rounds = model_xgb['test-rmse-mean'].idxmin()"
      ],
      "id": "5ffe85c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# #using validation sets during training\n",
        "evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]\n",
        "\n",
        "model_xgb = xgb.train(\n",
        "   params={},\n",
        "   dtrain=dtrain_reg,\n",
        "   num_boost_round= optimal_boosting_rounds,\n",
        "   evals=evals,#print rmse for every iterations\n",
        "   verbose_eval=10, #record rmse every 10 interval\n",
        "   early_stopping_rounds = 5 #stop if there is no improvement in 5 consecutive rounds\n",
        ")\n",
        "\n",
        "# #predict on the the test matrix\n",
        "preds = model_xgb.predict(dtest_reg)\n",
        "\n",
        "#check for rmse\n",
        "mse = mean_squared_error(y_test, preds, squared=False)\n",
        "\n",
        "print(f\"MSE of the test model: {mse:.3f}\")"
      ],
      "id": "5548e2ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#change test_calcofi to dmatrix object\n",
        "dmatrix_calcofi = xgb.DMatrix(test_calcofi)\n",
        "\n",
        "#prediction_on_separate_set = model.predict(dmatrix_calcofi)\n",
        "prediction_on_test_calcofi = model_xgb.predict(dmatrix_calcofi)\n",
        "\n",
        "# Create a DataFrame with predictions\n",
        "submission_df = pd.DataFrame({'id': test_ids, 'DIC': prediction_on_test_calcofi})\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "submission_df.to_csv('xgb_base.csv', index=False)"
      ],
      "id": "a12a6ca6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Tune XGBoost Model\n"
      ],
      "id": "c665f5bb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the parameter grid\n",
        "gbm_param_grid = {\n",
        "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
        "    'n_estimators': [100, 200, 300, 1450],\n",
        "    'max_depth': [5, 7, 9],\n",
        "    'learning_rate': [0.001, 0.01]\n",
        "}\n",
        "\n",
        "#best hyperparameters based on running\n",
        "gbm_param_grid_set = {\n",
        "    'colsample_bytree': [0.5],\n",
        "    'n_estimators': [1450],\n",
        "    'max_depth': [5],\n",
        "    'learning_rate': [0.01]\n",
        "}\n",
        "\n",
        "# Instantiate the regressor\n",
        "gbm = xgb.XGBRegressor()\n",
        "\n",
        "# Instantiate GridSearchCV with seed\n",
        "gridsearch_mse = GridSearchCV(\n",
        "    param_grid=gbm_param_grid_set,\n",
        "    estimator=gbm,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=10,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# Fit the gridmse\n",
        "gridsearch_mse.fit(X_train, y_train)\n",
        "\n",
        "# Best estimator\n",
        "best_estimator = gridsearch_mse.best_estimator_\n",
        "\n",
        "# Use the best estimator to make predictions on the test data\n",
        "y_pred = best_estimator.predict(X_test)\n",
        "\n",
        "# Calculate mean squared error\n",
        "mse_xgboost = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse_xgboost)\n",
        "\n",
        "#get the model score\n",
        "print(f\"Model score: {best_estimator.score(X_test, y_test)}\")"
      ],
      "id": "3da82dcc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#look at the hyperparameters selected by the model\n",
        "gridsearch_mse.best_estimator_"
      ],
      "id": "fb27980d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#create a table bring just mae score from linear regression and xgboost\n",
        "mse_lm, mse_xgboost \n",
        "\n",
        "#create a table bring just mae score from linear regression and xgboost\n",
        "mse_lm, mse_xgboost"
      ],
      "id": "babb2bcc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The XGboost model has lower Bias, and high accuracy. The model wins the linear regression model in overall performance. Upon this data science project, I suggest the CALCOFI to use the XGBOOST model that is already trained, to be ued when new data are brought up in the database."
      ],
      "id": "2ecc72db"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "eds217_2023",
      "language": "python",
      "display_name": "Python (eds217_2023)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}